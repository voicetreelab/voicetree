Okay, git checkout main
And now let's see
If we have...
Sure verse.
Okay, here we have it
okay so now we want to make sure that this
Scripts, we found it.
um
Um...
Let's also load the latest
Um...
So let's go
Build.
And then let's go.
Um, okay.
So, I think, did we just run films?
160
Reload juggle.
So we have now this
tool.
Um...
graph dependency traversal and now let's say find where we use it
Okay.
Okay, let's restart my computer.
I'll see you next time.
You
Thank you.
Okay, let's end this.
Hallihallo!
Hey, I'm doing, um,
Recordings.
Let's go.
What do you want to do though?
No, I was um, I just stopped my recording, but I'm doing a cool recording like this
Oh, cool.
Can I have a look?
That's what it looks like.
Well, it keeps on flickering. Can you turn my phone on?
I think it's out of battery.
It does not go on? No, it's okay.
Mm-hmm.
I'll see you next time.
Hmm
Okay, and then let's have the logs there, so.
All righty. Okay, so let's see.
if it can now understand that there's a tree loading integration.
can that we just built
So we want to understand if that works.
So
Let's see, creating a new node.
Hmm.
All right, so we've just
Um,
So we just added functionality to the load in existing tree.
We want to see if that works now.
So tree loading integration
access that node
Hmm, okay, let's look at...
Voice tree.log
Alright, did we just...
Mark down.
3 volt
What the fuck is going on?
let's see ready to listen
I have you loaded
loading tree.
Loaded 18 months.
I love you.
Okay, so we should be able to just add that there.
Okay, so we want both a logging and a print statement.
Okay, that should be sufficient.
Okay, cool. Now it's working
So if we're talking about the tree loading integrator.
can it add a new node there?
Um...
Let's see
Yeah, awesome. Okay, that's pretty cool.
Alright, so that's first feature done.
Let's just check the video.
All righty, that is done. Okay, so cool now we can actually work on voice stream. So we had
Um...
What did we have? We were going on a couple of things.
Oh, yeah, the two problems with voice.
So we want to work on the terminal default height issue.
And the, um,
graph flickering issue.
Um...
So...
Let's work on the Terminal Default Height issue.
because that's going to be easy.
Thank you.
Okay, so if we open
A terminal here now.
Cool.
Oh, actually.
Before we do that, we.
Um...
We wanted to look at if we could.
Let's hide some of that
All right
Hide.
All right, so we wanted to see
Um...
So we did that
And now we wanted to see.
Okay, so that worked, cool.
We can...
Clear that node
Okay, so we have demo complete.
Um...
Okay, so...
What do I actually want to do right now?
Um...
Oh, yeah, okay, so we were working on...
the ability to do a graph traversal search.
So that's this graph dependency traversal tool.
So if we open that up...
And we can run Claude.
Okay, and then...
let's make that a little bit bigger
Um...
Um, okay, so...
We have a tool
Voice
Tree
Oh, okay
So graph.
Get a tool for this.
Um...
What do you want to do now?
So we want.
So what do we want actually to happen?
Whenever you open
from a node.
We want to do a graph.
On that node to its parent
So up through its dependencies.
One second.
Hello?
Okay, I hope it's time to go to the side.
Where are you at?
All right, we just had a packet come.
um so yeah what do we actually want to do regarding the
dependency traversal tool.
Um...
So...
Um, we want...
Returning only seven nodes from select-
Why would that be?
Oh, because...
Right. Nothing was relevant.
Hello?
links
Sorry, we're still Orbin or...
Oben oder unten?
Unten.
Robin, okay, I'm going to go to the right side.
Thank you.
Oh man, fucking door.
Um,
Right, so...
What do we want to do we want
Um...
when
So instead of just getting the parents' content...
we want to give the full graph dependency traversal.
So, currently...
when we open Claude.
Oh, come on.
When we...
Okay, so in...
markdown tree vault, add new node.
No, here.
Demo prompt dot wait
Ah, okay, so.
Um...
So we have currently in.
We just load
So we have pump is.
Doom.
Wait.
If you want.
Injected in this prompt to be the full.
I propose a way to do this.
Okay, so I'm running an agent
right now on
So that's running.
Okay, and now I realize that
Um...
when the graph moves
Sometimes the terminal isn't moving.
Do that. That seems to work
But I guess if it happens while it's moving.
Um...
Okay.
So let's see what's decided to do
Okay, so now let's
open
A Gemini instant.
Um...
So
We are getting lost in the tree a bit too easily.
That is one problem because things just sort of flicker around too much, which is...
The second problem we need to...
Okay, so...
Let's um
delete that
So we have here and here
And now we want to go Gemini.
Okay.
And let's get it to review that.
so this is
forward dependency graph traversal into cloud SH.
Let me see, review this.
proposed
navigate to
All right, let's get more.
Okay.
And now let's go.
Opus to work on that.
and say,
review this
this
Also look at the neighbors.
So this does make me think that we're all
have to be able to see
Oh my god.
So annoying zoom.
Um...
So let's see
Um, okay, so...
There's a problem here with the markdown.
Let's, let's get, uh, do it. Okay.
Thanks.
Make sure they're known.
problems with this.
And then exit.
So this guy's saying that the clod is
the build is failing.
So when you figure out why the builds
And what does he actually change?
No, we don't want that.
So let's say no
Shut the man.
So we can't reference files easily
Um...
Yeah, it is a problem. Voice trademarked entry vault.
I'm going to guess. Oh well.
Okay, so let's test if that works.
um so we can
So we can open up a terminal here.
Claude.
Let me see, do the same for Gemini.
And then we want to see.
Um.
There you go.
Enhanced.
So we want to loop.
Okay.
You
I'll see you next time.
Okay, let's see this guy.
Okay, let's see what this guy did
Don't want that action.
Do you need to be able to specify a...
folder for it to work in.
Um...
So maybe
We could do like...
I don't know.
Claude sh
Folder like that
It's not a bad idea
Okay, cool. So now we want to work on a new task.
Um, actually, let me just see if I can open it.
Okay, that did actually work.
Okay.
The flickering is really annoying.
Okay, cool.
So today we're working on
And there's a few different things we want to work on.
um, mostly related to getting, um,
A live demo, looking really good.
Um...
And so some of the improvements we want to do for VoiceTree right now.
Uh...
Um...
So one thing is when a second node appears on voice tree
on the juggle UI.
a
It flickers, the whole graph, the location of the graph.
changes and you no longer see it in the notes.
So when the second note appears, about three seconds later,
Um, there's a large, uh,
um flickering issue so we want to
fix fix that so there it just happened you just saw
Okay, the other thing we were working on.
Um...
was for the demo was improving the graph traversal to the LLM.
Um...
So what do we want? So right now we send a full graph traversal.
But we also want to.
inverse document search
algorithm to find other potentially related nodes.
to the user's query.
Um...
So what is that going to involve?
Um,
So that's going to involve, um,
So when you run
dot sh.
we're going to
So we currently give it, we run the Python.
tool to do a graph traversal.
and we give it the full traversal of the current node.
Other functionality we want
So on that current node,
we want to do a inverse document search.
Against that current node
um
to all other nodes
in the folder.
Um...
Bet that note is in
And also send back, like, for example, the...
10% most related other nodes.
Alright, so what are some things
agent will need to know to work on this
So...
Um, the Python script is in our, um,
root repose directory.
And then there's some related
Files.
in VoiceTree for doing inverse document search.
Um...
And...
Um...
That will be useful to know as well
So let's see here.
Let's go, let's change the link here.
Um...
So let's make this.
Define inverse type in switch scope.
So can we edit a node here?
Okay, so there we edited the voice tree slightly
Um...
And then we have some...
Okay, so I'm going to also add to inverse document search invoice tree.
Um,
Other useful uTools
All right
There
So
Okay, so yeah, so you have access
to the tree functions file.
get most relevant nodes.
But for now, to keep it simple, we don't want to load the whole market.
into a
decision into a tree data structure. We just want to replicate the inverse document search just without loading it in.
tree
Um,
Cool. Does that make sense?
Okay.
So we have our dependency graph now.
Um, okay.
at relevant node retrieval.
I'm now going to launch an agent.
And let me just see if it has all the context.
um
So we'll open an agent here
And we'll go claw.sh.
And let's just see what context.
Um.
do you understand how to perform this task
What other context do you need?
Okay, so that guy is exploring.
I'm going to add the the the the the the the the the the the the the
Let's do it.
Okay, now I'm going to get...
Gemini to review his work.
review Claude's work
Okay, and then another thing I'm seeing
now is that when an agent
Adds a node to our graph
Um, the colors are not that.
Nice.
um sorry the it makes it as a
as a header.
But we don't want that.
Um...
So we want to improve that so that when an agent adds a new node to the graph,
um the formatting is better
Okay, so let's see.
You
Okay, it looks like these people are done.
Okay, now I do want to work on the...
Add New Node Tool.
and the Node Formatting Pro.
um so let's just go there
Uh, open Claude.
Okay, I'm going to do a quick fix-up toss.
which is to combine Gemini
into one script so they don't have to keep on editing them both.
Okay, so cool, we've done that task.
We can just...
Um...
I think it's we can
Collapse selection.
Cool that guy worked
So let's do.
collapse on him as well.
let's actually get um
Gemini to work on.
Cool. Right, we've done that feature.
Okay, let's just end that
Commit.
Thank you.
All righty, cool. So we're working on voice.
We're working on making the live demos right now.
And we're just making...
some small improvements to the VoiceTree system to make the demos better.
Um...
Okay, cool. So one thing we just fixed is that when you run in a
You now get
a
So when you run an agent
an existing node.
You get.
um, a graph traversal.
And it also does a TF-IDF search for the most relevant other nodes in the tree.
Think let's make our voice settings
slightly longer
So we want in our...
Voice to text config we want a bit more of a pause threshold
0.9
Cool, so that's what we've been doing and now we want to fix a few things with the UI.
that have been really annoying for VoiceTree.
Um, so one thing is, is.
Um, one annoying.
UI element.
of VoiceTree is that when a new
the graph restarts its layout.
And that can mean that things sort of fly around.
much too much.
And so we need to think of a general solution to that so that things stay put in place.
Um...
So let's think about what possible solutions could be.
for the VoiceTree Graph Layout Instability.
Um...
Oh, okay, before I start about that, I just want to talk about another VoiceTree UI issue.
which is that newly appended nodes
Um,
get an animation for 15 seconds.
but for some reason the timeout isn't working.
And the animation appears for longer than 15 minutes.
So let's figure out how to fix that as well, it's another task.
I think...
But it was working for the, when we added a timeout.
create animation it was working working I know at some point we had the time
but one of our recent changes must have regressed it.
Okay, let's keep on thinking about solutions for the...
Voice tree graph layout instability.
So one thing is, is that we could, one option is to completely disable
the layout change and juggle.
And I think that's something called the cola layer.
Animation.
And if we do that, I think it's actually fine.
Um,
We just need to make sure that our new notifications
are generally correct.
So we'll sort of have to replicate the cola layout.
but only for new nodes.
So where are we putting you?
the decision for where we put them.
Um...
needs to sort of replicate color logic so that our graph gets built in a nice way.
Or, I'm thinking now,
Cola layout.
only run the layout on the new node.
can choose some nodes and but then I think that gets too
Um...
So
Okay, and now we're talking about another VoiceTree UI fix.
which would be a really nice long-term solution for the current problem we have.
um, with terminals on the canvas.
Um...
behaving really badly when they
move or when the graph zooms.
Um...
And...
what we want there
Um,
is potentially want to represent the terminal hover editor.
itself
as a
Node on the graph with an edge connected to it that way you can always connect
Terminal.
to the node it was opened from.
So currently we have sort of like a
A very, um.
Oh, okay. Actually, I think maybe we're pretty close to having that working.
So actually a related fix so how we could fix that is if we're disabling voice tree layout changes
What that means is
is that new nodes
I'm sorry, that moving node
Um, it would just, whatever, wherever position.
put the new node in manually, it will just stay there permanently.
And what that means is there's no unexpected node movements other than the ones we're doing manually.
And what that means...
that then
if the terminal, if we put it over
then it's only going to move.
if the terminal node moves.
which is the current behavior that works well.
So I think.
we actually have a fairly good solution for that.
All right, cool. So now we want to see if
Um,
if both the new node animation
working now that we've rebuilt.
Um, and also the other thing we want to.
is if
Actually, what did we just work out? Oh yeah, if adding a new node changes the...
animation at all.
And I just saw that it was still happening.
So we can open and develop our console.
Um,
Okay, so we want to see first if we've opened the right.
Plug-in
1.59, okay.
Let's unload that
That is pretty annoying.
Okay, so...
Let's start running our agents to give it the feedback that it didn't work
Okay, cool. Let's rebuild.
juggle.
And let's check if the animation is working.
Um,
and
Look at what.
So we can.
Reload juggle.
So let's make sure we're on the latest version.
Oh, okay.
I see, I see, I see.
We have a problem.
after we've renamed the repo. So let's go voice...
So I think the problem was that we weren't building.
which is why we weren't seeing the animation.
Okay, let's see now it should work
Okay, cool.
let's open up a new graph
and let's try the dragging functionality
Um...
So it works somewhat well.
And now let's try create a new node.
What we're gonna do what we're gonna eat for dinner today. I'm not sure yet exactly what we're gonna eat for dinner
So I want to figure that out.
Um...
And now we want to...
Cool, so it looks like we fixed it. The animation stops.
After 10 seconds or so?
Awesome.
Um...
And it also looks like new nodes don't really force much of a layout.
Which is.
Pretty awesome.
Um...
so that we can close.
Um...
All right, let's get while we're reading that another agent to get to work
So that's, we're going to have.
um, here a
So we have another UI problem.
Um...
And...
That UI problem
that when you open
terminal
the initial size of the terminal.
is a bit too small.
Um, and so we want
Um, the terminal.
when it's converted into a hover editor.
to be slightly bigger.
So we'll get an agent to work on that as well.
Um, so that
problem of the initial size of the terminal.
is when converted to a Hover editor is too...
we want that to become a new node.
Ah, so there we just saw that our position still flicked.
so we need to definitely get that too
Get an agent working on that.
When and you know.
Our viewport can still flick.
to a completely different...
Okay, we just want to check right now if we append to new content.
Does that result in...
Um, um,
Uh, the...
animation not being stopped by a hover.
So that's what we want to figure out right now.
Um,
Okay, so it doesn't stop on hover.
So we want to disable that.
So append animation.
Stop is now done.
Um...
Cool, so the agent thinks they found the root cause for the viewpoint. Flicking.
You
Okay, so now I remember that we have a lot of tech
that in terminal hover editor positioning.
So this is related to terminal size problem.
we want to make a
Um,
to clean up that code and get Gemini to inspect it as well because it just has so many bugs.
All right, so in that existing tech for terminal hover editing positioning
we want to make a new node about
Improving the
Uh,
have our edited positioning.
code because it has currently lots of bugs.
That I'm recording here
Yes, please. Yeah, thank you. But you know that phone
Now the phone.
It's recording me.
The phone.
Well, if you get in the...
in the way. Yeah, thank you.
Thank you
I'm just bringing you a drink.
I'm realizing right now there'd be a really nice
for voice tree if
Um,
If the agents can also modify.
existing nodes.
which they can actually do, I can just tell them that.
Or I guess the other solution would be to have.
Voice tree nodes.
Actually,
That to include the agent node
in our tree, so where we can append to them.
the nodes that the agents themselves have made.
which we currently can't do.
You're here.
You didn't tell me I could come out now?
Hey, wait, I'm recording!
Okay, just give me five minutes and I'll stop
Oh
Done
Okay, cool. So today we're working on VoiceTree, and there's a few different things we want to work on.
mostly related to getting a live demo, looking really good.
Um...
And so some of the improvements you want to do for VoiceTree, right?
Okay, okay, okay.
not completely. I'm going to make it something quickly to eat and then I'm going to film after. Can we see what you've done?
Yeah, when?
Now, okay, we're just going to shake it.
Are you ready to eat something Manu?
Are you ready to eat?
Thank you.
I, uh...
Do you want, I want to make pasta and I have...
Fake meat.
Is that okay?
Was that too boring?
Yeah
What are you liking?
Very good.
Perfect.
Hand from hand from.
It's really keen-shaven and with a little Italian beer.
by the time we were just
I like an Italian beer, but you had a very easy...
Right, did we just load up?
It's not?
But it's the same people. It's the same brand, yeah.
Yes, you can see so much what the other one said.
Tristan's reading this.
And I don't see something else, don't they?
do this.
Follow Beagle
Yeah, ask you to go.
Do you want um
Let me set the table here.
Oh, yeah.
We'll just take everything up.
We have been wondering.
I've watched it a long time.
We're in there.
What's beautiful up there?
There's nothing in the washing machine. Are you sure? Oh yeah, no there is.
There's stuff on the floor. What is that?
Should we do it now?
I'll do it, I'll do it. I have to add my white chalk.
Why don't I put this in the dryer?
It's just going to make noise.
When he's finished the recording...
Yeah, I'm finished recording for now.
soon.
3 load
Thank you.
Roll that in the back.
Yeah
The front stuff on the floor everyone.
Honey, but how's it going? What are you doing?
I was filming live.
use cases of voice stream.
Yeah, you gotta work down.
It's great. Yeah, it's really good.
so
So with that, not enough.
edit that into with your voiceover.
I mean, I don't think Elon's going to do anything.
He's at a party right now.
I don't, I'm not counting on Ilana Dooney.
but I need something. Okay.
I need to add the thing in
going for yoga.
You can get it if you want.
Where is it?
Fake news.
Right top.
Yeah, and foot wash, you need to...
I'm going to put it on my full wash mittel.
white.
And one of those calcium...
What do you want me to use?
I want you to use a full wash method, two tablespoons.
Not that the length of my day.
have good pressure.
Peruvol
Careful with this one.
I bought two new boxes
I'm going to do a little bit of a bowl of water.
the white and the other
Okay.
For the wash metal, two tablespoons.
Yeah, yeah.
One tablespoon.
Two tablespoons, and what else do I put in?
The thing that says we've used the power systems.
Today's LLMs don't.
memory. They reprocess the entire chat history for every single turn, which is inefficient and expensive.
VoiceTree solves this at the input layer. We convert any unstructured text into a graph and then ask the LLM what nodes it would like to have in context.
On the GSM Infinite Benchmark,
This results in 70% fewer tokens and 15% greater reasoning.
But then what we discovered is that this graph itself
can be more than just an optimization.
It's an ideal foundation for an interface for human AI collaboration, a shared memory.
I'll show you what I mean.
So right now we are actually running
I can add an agent.
and tell it to do something like draw a mermaid die.
So this agent that I added starts adding its own nodes to the graph.
I didn't have to...
rewrite any of my context in the prompt
and its progress is visible to me.
in real time. I could even, if I wanted to,
Send its output to another AI.
This workspace is the first product built on our core API, a new structured primitive for building with AI.
I've been using the VoiceTree workspace every day to work on VoiceTree. Here are some recordings of me voice
Today, the LLMs don't have a...
They reprocess the entire chat history for every single turn, which is inefficient and expensive.
Voice Tree solves this at the input layer. We convert any unstructured text into a graph and then ask LLM what nodes it would like to have in common.
On the GSM Infinite benchmark, this results in 70% fewer tokens and 15% greater reasoning
What we then discovered is that this graph itself can be more than just an optimization for LLMs.
Today's LOMs
Today's LLMs don't have true
They reprocessed the entire chat history for every single...
inefficient and expensive.
Voice tree solves this at the
We convert any unstructured text into a graph and then ask the LLM what nodes would
on the GSM Infinite Benchmark.
This resulted in 70% fewer tokens.
Input tokens and 15% greater reasoning.
What we then discovered is that this graph itself can be more than just an algorithm.
for LLMs. It's an ideal foundation for an interface for human AI collaboration, a shared memory.
I'll show you what I mean.
So right now, we're actually running VoiceTreat live on what I'm saying. You can see the transcription in the bottom.
I'll now add an agent.
into our shared workspace.
Thank you.
Maybe we can keep it okay.
Thank you.
What do you think?
All right, so today's LLMs have a pretty terrible architecture.
They just chuck the whole conversation history back in every time you send a prompt.
Boistree solves this at the input layer. We create a graph.
for any content.
and ask the LLM from that graph what nodes it would like.
We've been testing this on the GSM Infinite benchmark, and we're getting 60% fewer input tokens and 15% more accurate reasoning, which is huge.
All right, so today's large language models have a pretty terrible...
memory. They reprocess the entire chat history every time you want to send the large language.
VoiceTree solves that at the input layer we convert any type of text into a graph and then ask the LLM from that graph
What nodes would it like to specifically read?
We've been testing on the GSM Infinite Bench.
and we're seeing 60% fewer input tokens sent and up to 10% better accuracy and reasoning.
But the cool thing is is that this is actually much more than an optimization. It's a whole
new foundation.
for human AI collaboration. It's because it can become a shared memory. So I'll show you what I mean. Right now, I'm running VoiceTree live.
It's transcribing my voice in the bottom left.
And I can now launch an agent.
So it gets sent all the relevant context from the graph
And now I'm going to tell it to draw a simple moment.
All right, so today the LMs have a pretty terrible architecture primary. They just chuck the whole conversation history back in every time you send a prompt. Boistry solves this at the input layer. We create a graph for any content.
And ask the LM from that graph what nodes it would like to be.
We've been testing this on the GSM infinite benchmark, and we're getting 60% fewer input tokens and 15% more accurate reasoning, which is huge.
All right, so today's large language models have a pretty terrible architecture for memory. They reprocess the entire chat history every time you want to send a large language model.
VoiceTree solves that at the input layer. We convert any type of text into a graph, and then ask the LLM from that graph, what nodes would it like to specific?
We've been testing on the GSM benchmark, and we're seeing 60% fewer input tokens sent, and up to 10% better accuracy and reasoning.
But the cool thing is, is that this is actually much more than optimization. It's a whole new foundation for human AI collaboration. It's because it can become a shared memory. So I'll show you what I mean. Right now, I'm running VoiceTree live.
It's transcribing my voice in the bottom left, and I can now launch an agent from here.
So, it gets sent all the relevant contacts from the group.
And now I'm going to draw some format diagram.
All right, so today's LLMs have a pretty bad architecture.
They just send the whole chat history.
back in for every single prompt.
VoiceTree solves that at the input layer. We convert any text into a graph and then ask the LLM what nodes it would like to...
We've been running this on the GSM infinite benchmark and we're seeing 60% fewer input tokens.
and better reasoning.
Right now, I'm running VoiceTree live.
It's transcribing my voice.
and converting it into that graph.
And the cool thing is, is that VoiceTree is not just an optimization for LLMs.
it actually can result in um so it's like essentially just like a whole new um interaction layer
What is that bug?
Okay, let's reset to where we were
at the input layer we convert any text into a graph and then ask the lm what nodes would like to see
and better reasoning up to 15%.
So
Right now, I'm running Boy Street live.
is transcribing my voice and converting it into that graph. And the cool thing is that VoiceTree is not just an optimization for OMs. It actually can result in a whole new interaction layer.
I'll see you next time.
The current way that LLMs give themselves memory is really inefficient.
they append the whole transcript history whenever you send a new
Boystery solves that at the input layer.
we convert any graph
into a, we convert any text.
So, the current way that large language...
is really inefficient. They append the whole transcript history back into the model every time
VoiceTree solves this at the input layer. We convert any text into a graph and then ask the LLM what nodes it would like to be.
We've been looking at the GSM in...
and we've been seeing 60% fewer input tokens on average and up to 15% more accurate reasoning.
The cool thing is, what we realized is, is that this is not just an optimization. This now becomes a whole new foundational...
interface for human AI.
So I'll show you what I mean. So right now, I'm actually running VoiceTree live on what I'm...
It's transcribing my voice to text and then creating the voice tree graph.
And I can open an agent. I can spawn an agent.
Um, and then...
get it to do something. For example,
Draw me a mermaid diagram.
explaining a simple mermaid diagram explaining what this could look like.
So the current way that large-language models give themselves a memory is really inefficient. They append the whole transcript history back into the model every time you send a new message. Voistry solves this at the input layer. We convert any text into a graph, and then ask the LM what nodes would like in this context. We've been looking at the GSM benchmark, and we've been seeing 60% fewer input tokens on average, and up to 15% more accurate reasoning. The cool thing is, what we realize is that this is not just an optimization. This now becomes a whole new foundational interface for human AI collaboration. So I'll show you what I mean. So right now, I'm actually running Voistry live, on what I'm saying. It's transcribing my voice of text and then creating the Voistry graph. And I can open an agent. I can open an agent.
and then get it to do something, for example.
Draw me a mermaid diagram.
Explaining a simple mermaid, Doug, I'm explaining what this could look like.
All right, I would like you to create a new node on the graph anywhere, please. Thank you, please anywhere, please
So the current way that large...
give themselves memory is really inefficient. They reprocess the entire transcript
VoiceTree solves this at the input layer. For any type of text, we can convert it into a graph and then ask the LLM what nodes from that graph it would like to look at.
We've been playing around with the GSM in
and we're seeing up to 60% less tokens sent and 15% more accurate reasoning on the long contact.
The cool thing we realized is that this isn't just an optimization, it actually is the perfect foundation
for a new interface for human AI collaboration.
shared memory. I'll show you what I mean. So right now, I'm actually running VoiceTree live on my text. It's converting my voice into this graph here.
And what I can do is I can spawn a terminal.
and get an agent to do something for me.
It's already gotten all the relevant content.
from the graph injected into its context. So I don't need to rewrite any.
Alrighty, we are testing some things.
Thank you.
So it's going to be there, that's all right.
And where is the new file going to be? That's the question.
Okay.
So the current way that large language
themselves memory is really inefficient.
They essentially reprocess the entire transcript history for every request.
VoiceTree solves this at the input layer. We create a graph for any type of content.
and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM in
and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
The amazing thing about this is that it's not just an optimization.
It's actually a whole new foundation.
for human AI collaboration.
It's a shared memory. So I'll show you what I mean.
So right now I can open an agent on my shared memory count.
and get it to do some ink.
So here I just got Gemini to add a
A diagram.
then
I can get Claude.
to write the pseudocode for this.
So I don't need to Reprompt ever it gets a full tracing From the graph of what content could be relevant
Hmm.
So, the current way that wide language models give you is
is really inefficient. They essentially reprocess the entire transfer history for every request. Voicery solved this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its context. We've been running this on the GSM Infinite benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now, I can open an agent on my shared memory canvas.
and get to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever, it gets a full tracing from the graph of what content could be relevant.
So the current way that large language models give themselves an memory is really inefficient. They reprocess the entire transcript history on every request. VoiceTree solves this at the input layer. For any type of text, we can convert it into a graph, and then ask the LLM what nodes from that graph it would like.
We've been playing around with the GSM infant benchmark, and we're seeing up to 60% less token consent and 15% more accurate reasoning on the long context problem.
the cool thing we realized is that this isn't just an optimization it actually is the perfect foundation for a new interface for human collaboration a shared memory so right now i'm running voice tree live on my text
So the correct way that large language models can
is really inefficient. They essentially reprocess the entire transcript history for every request. Voice3 solved this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its content.
We've been running this on the GSM infinite benchmark and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now, I can open an agent on my shared memory canvas.
and get to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever, it gets a full tracing from the graph of what content could be
So the correct way that large language models give
really inefficient they essentially reprocess the entire transcript history for every request boistery solved this at the input
So the correct way that large language models can
is really inefficient. They essentially reprocess the entire transcript history.
So the correct way that large language models can be
really inefficient. They essentially reprocess the entire transcript history for every request. Voicetree solves this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM Infinite Benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now I can open an agent on my shared memory canvas.
and get it to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever. It gets a full tracing from the graph of what content could be relevant.
And up to 15% more accurate reasoning.
So the current way that large language models give.
They essentially reprocess the entire transcript history for every request. Voicetree solves this at the input layer. We create a graph for any type of content and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM Infinite Benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reason.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now I can open an agent on my shared memory canvas.
and get to do something.
So here I just want Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
so i don't need to re-prompt ever it gets a full tracing um from the graph of what content could be
All right, testing one, two, three.
Testing one, two, three
Testing one two three
That is awesome. Okay, cool. Yeah, let's definitely do that.
So what we want to do right now is we want to make some improvements to VoiceTree and the first thing is we want to make improvements.
to the output print statements.
We want to make them coloured.
So we want to color our output print statements.
I'm using the term color module.
And then you can just print colored red or colored green, for example. And the way we want to do that.
is
So what we want to use that for is, so in our output models, we currently have text that says creating a node and appending to a node.
and we want to make sure that
Those print statements are in either cyan or green. So cyan for append and green for create.
All right. So while I'm getting that to run, the other thing that I wanted to work on is just a really quick fix up, which is the sidebar isn't.
So we need to quickly fix that.
Thank you.
Yeah
Damn, it's all in love.
Thank you.
Thank you very much.
Yeah
I said yes.
I think we're out.
Yeah, whatever you think.
Give Dad some enzo to those two.
Okay, take some Enzo's, take some Enzo's, an L-theanine and an Ibuprofen, and don't have any more caffeine.
And stay very hydrated.
No, no.
Thank you.
Thank you.
It's a big one.
Okay.
Next.
I'm not supposed to be one.
Thank you.
Thank you.
We're trying to check right now if the color module
is working correctly so we're doing a live test and seeing what colors come out.
Thank you.
Thank you.
Thank you.
Thank you.
I'm not going to do that.
Thank you.
all right cool so i just want to do some quick fix-ups to our working environment um so we have a couple problems right now um so one is that in obsidian we see all our
as well. So we see all the Python and shell files.
And that's actually not the worst thing.
Because it could be cool to have like a voicetree.sh tool, which we can open.
However, it's too cluttered right now with all the prompts and Python tools.
and shell scripts.
So what's the best way to solve that?
So one way to fix it would be if we put all those tools.
one directory lower.
and opened, when we open a terminal, it opens in that environment.
Um, so the way we could do that.
is we could in our zs hrc um when
We open a terminal and we're in.
in an obsidian space.
we can cd dot dot to.
to the parent directory.
But we do need to make sure that that's going to work with our prompting system.
So the prompts.
um how do they currently work i think they currently open in
Let's see
Thank you.
parents.
Thank you.
The core dichotomy that we need to solve is that the agent needs access to both the Markdown Tree Vault directory and whatever Git repository we're working on.
Okay, so maybe the solution then is, so an agent can't enter the directory of a...
whatever directory it started in.
Yeah.
There should be one here now.
once you want to get out of this.
Okay.
Maybe we do.
Thank you.
Thank you.
Stay down.
Thank you.
Thank you.
Thank you.
Thanks.
Let's go.
Thank you.
Thanks.
Yeah.
Thank you.
I'm going to finish that.
I'm going to take a look at it.
Mom, it's a common business.
How are you? Say.
dollars.
Sorry, what is it? A thousand people like to announce $500.
Bye.
So right now
Voice trees running on this little window there. Can you see it there?
Yeah, I see it. So it's it's recording me. It's transcribing And then in this window, I can have my note
And then what happens is as I'm.
for explaining things that's that's for explaining things and then you see how a new node just appears
Yeah, that was from me talking.
And so if I keep on talking about that node see now it's I you can see that it's live because it's
It has that little text there so
like there it's in green and blue the the comments from my my terminal and then there's the actual like
nodes appearing and so while I'm
like the theory for like a minute.
The um...
the nodes will still be appearing, and the logs here are quite clear, they're colored, so they understand what's going on. They say, ah, okay, he's explaining this stuff, but in parallel to that, VoiceTree is actually running here, and you can see the logs, and you can see the nodes appearing.
No, so for the demo
You read the demo script, right?
good. So...
I think that is a big vote.
Well, it's the optimization part and then how that becomes
So right now, this is how I'll do the demo. I'll have Voice Tree running here, and then I'll have my notes for what I'm gonna explain here in the bottom half.
Yeah, they will see these notes.
I mean, they also see Voice Tree running in the top.
And so I'm explaining VoiceTree using some.
And then at some point I tell them I'm doing a live demo right now. You can see VoiceTree running.
And then I show I open like a node here like I can open cloud or Gemini you see that
And then if I open Claude,
Phone away.
So you don't have to, you don't have to.
No, I don't need to write it it automatically
And then I'll show them. I'll show how it...
Anyway, so it gets all the content.
Um, and then I can.
Yeah, it gets all the nodes from the chip.
Actually, it's not working right now.
But it does, and I can show them that.
Um...
And then I'll get it to like do something like write a mark down.
Like to draw like a diagram or something something simple now just paste it in so I'll just do like
draw
Like okay, I haven't copied someone
So if I just say...
Add a new node.
So I'll just paste that in, which just says add a new note of mark.
And it will all work, so there's some bugs right now, but I'll make it all work. Yeah.
Um...
And then...
It's going to add that node. And then what I'm thinking is either I switch over to
Example
Um.
that we were talking about, that I have at the end of the script.
Let me see that one again, one sec.
what did you mean by that four different screens so you're going to just have a lot of different agents doing
No, so, hang on.
So there's two things I can do. So one option is I continue using this Example this like this. I don't I don't restart. I just use the same example
And I just now say, okay, now I'm going to engineer an actual feature in VoiceTree, continuing off the same VoiceTree.
And so, then I'll...
And then after that we speed it up to like 10x so you hear me talking really quick
and the nodes appearing.
um and then we we stop at some point where i'm like now watch this and i get um i get one agent to decompose a task into subtasks and then run all the sub agents on those subtasks
Um...
But is this all possible in three minutes?
that it would have to be really sped up.
And just showing them like, or, so that's one alternative, is that I continue using the same tree to show like a real world example.
Or I can switch over to some real world examples of me actually using it for engineering, sped up and just stop at key moments.
No idea to develop a narrative.
Yeah, yeah, I agree.
of the context. Yeah. And then I can have 10 seconds at the end.
like so like split screen right like like as in there's four screens
And it's just like four videos of me using VoiceTree for work.
So I'll show you an example.
Like if I open
so
So I can show like some of these really sped up examples of me using
VoiceTree to do engineering work.
Yeah, but I think that's a bit true.
But if we just have it for 10 seconds like split-screen for four ones of these and just like just to say Just for the one sentence of me saying I'm addicted to using voice tree engineering has never been this productive nor joyful before Just for that just for that one second
Yeah, nice yeah
Pretty cool, right?
Because I told him, like, no worries, you don't have too much time. Even a quick glance could be useful. And he said, I'm super interested in the project. Just want to be able to give it my full attention in time. And then he says he spoke to a patent lawyer, a friend of his, and they told him it makes sense to file for patents in the countries you will want to sell to.
I guess these are things for Y Combinator.
Okay, so then ideally I should be sending him this this draft of just the text or send him me me doing an actual example like I can probably the next hour get a pretty okay example of me doing it
Yeah, we have some problems, of course.
It's not going to be the... Yeah, that's okay.
Yeah. Do you think that's a good use of your time right now? Is that what you want to focus on?
Getting the demo done
I mean, doing kind of a mock video.
Oh, I mean...
just be practicing for the real video yeah i mean the real video is just gonna be
they record like the best version
whatever the best one is so you just want to keep track
Yeah, but I mean if we record it early and send it to Zarin, he can give some advice. Yeah, exactly, I would do that. Yeah, and say this is maybe, I'll just tell him this is the draft. Do you want me to, do you want to send me Zarin?
Okay, so do you think that's the perfect flow like as in
what I sent you and then what I explained.
What it looked like
Yeah, dude, it makes it sound like it's a really...
Hmm.
So one thing is that I've completely stopped work on the GSM bench.
And it would still be really cool to show like an image
benchmark results.
Yeah.
But don't you have already one that you can show?
I mean...
I've stopped work on it. I think I could come up probably in three hours of work, I could get a nice graph that's actually accurate.
Um, but there's a lot, a lot of.
But I think that's fine. I mean, they just want to know it's promising not that
Let me just check exactly what time it is. It's 5 a.m. Tuesday. Yeah. I think I'll be staying up.
I don't know. I mean, hopefully not, but in case.
All right, 5 a.m. Tuesday, one day left. Exciting. Okay, so can you read the...
Yeah, I've read it. I think it's great. So you think it's perfect just just make that work
What are you going to be showing in the first part of where all the text is?
Before you say, I'll show you what I mean.
So the whole time.
So the whole time it's going to be.
This screen, right? This is the screen I'm recording
My face will be in the top right
And here you see that there's like
Um, there's... What is that?
Well, it's just drawings so I can just draw whatever I want there
So, and I can use that to explain what I'm talking about.
So I could have like some little, you know, circles or whatever I need to explain, whatever I need to explain. Yeah.
and then and then when I'm done with the explanation I drag it away and it's just the
Awesome, he's perfect.
Amazing, amazing, amazing.
Yeah, I don't know. This is just so much better than...
other ways of doing it where I have like a different slide and then I switch to this slide and then they're not sure.
progress you've made in the last week I mean imagine with a few more months to work on this if we don't get this round you have six months to polish it well so what happened is right is that the past year and a half I've just been working on the core algorithm
And in this past two weeks, I've just been working on like UI, which makes it look a lot better, but nothing's actually that functional.
But it makes a huge difference for...
But it's genuinely gone to the point where I prefer using it now to other ways of working.
Yeah, it's so cool, so cool.
That's awesome, dude. Yeah.
It's your own invention. It's really cool. It's so cool.
Okay, I'll try. So what's my priority to get to get you a draft of that?
And Zen.
Yeah, yeah
Yeah, perfect.
Exciting stuff, Manu. Amazing. All right.
Um,
Um...
So, can you hear me?
Now I can hear you. Yeah, you're back. What I want you to work...
Um.
Well, uh-huh.
Well, uh-huh. So when I send you the draft...
You could do the editing.
Um,
I don't know. I don't know. What's...
What do we need once like the demo when you be edited slightly like when you speed up parts of it
Yeah, but you'll have shot it, so you know every single line you said. I think you should do the speeding up, because you'll know what you want to, while you're making.
It'll just be inefficient for me to figure that out. Okay, just stay on standby then.
Yeah, that could be really good. Maybe out of all, you know, I've sent you lots and lots of prompts. I mean, like example scripts and stuff. Maybe we can fill it in a bit more.
yeah and so and any of the personal stuff
Don't worry about filling it out like a time. I've um like hacked something like I'm thinking like they all talk about like biohacking
that's kind of cool and interesting
Um, that I haven't done.
and you could think about maybe because like in that one minute founder video we can i can mention some things about voicetreat to make the next demo more understandable
Because I'm sure they just append them.
um
So maybe you think about if like the whole narrative of like Founder video plus demo video plus written application if it all really makes sense and nothing's left really confusing And they're not like you know what I mean like as in the whole thing is a cohesive story piece of narrative
Best way to see this is if you already have drafts, you know, not the final version, but if you just take the one minute founder video, have mom record it up on the roof.
Or just, I mean, behind with the bookshelf, that looks good, though.
And then Zeronomy can feedback.
Okay, okay, cool.
I will...
I'll record, I'll try to record all the drops that we need and send them to you ASAP. Nice. Okay. Should I make a group with Zarin? Yeah, yeah.
All right, bye. All right. See you, man. Bye.
All right, cool. Can you hear me voice, Tri?
All right, we just need to get some little fix-ups working for our demo. Most things are already working. We just have a few things we need to do.
Um,
So for the demo
Something we need to do is...
for new node placement consider all other existing nodes in the graph so that like right here they don't appear too close to each other
We also need to stop doing restarting layout after we drag a node. That's been really annoying.
And I know that there's somewhere in our code base there's layout start.
stop and we we just need to comment that out so that the layout doesn't restart
Okay, and then the other thing we need to work
is that
Um,
Currently, we've somehow broken the Claude.sh.
So it used to get the whole Python dependency.
and now it no longer does.
So we need to fix that.
You
Alright now I want to figure out
When we're going to go to the gym.
We want to go to the gym. Let's check when the gym's closed.
So it's a Berlin strength.
Maybe it's not open on a Sunday.
Okay, so we fixed the ClawDOSH broken functionality. Let's just quickly check that it's also working for Gemini
Nice looks like it's working
Cool, okay, so now we just want to make sure we need to work on optimized new node placement
Um...
Let's actually yeah, so let's get an agent to work on
Restarting layer.
Let's do Claude.
Okay, and now we want to work on Optimize New Node Placement. Yeah, clone.
Okay, let's add to a list of things we won't do but ideas
Could be cool, but we're not going to do because you know
So one thing is a way to actually delete a node and then have it deleted from our tree data structure as well.
the other thing we wanted to do was to be able to...
collapse a node and all its neighbors.
into one container node.
That's also a won't do task.
All right, so now we need to...
what exactly we need to be able to do the demo where we do agent.
All right, so now we need to consider what exactly we need to be able to do the demo where we do agent orchestra.
So, I guess the first thing we need to do is to be able to...
So I guess the first thing we need to do is to be able to...
So go down a chain of nodes.
So go down a chain of notes.
have those nodes created.
have those nodes created.
And then...
Um, and then...
run an orchestration agent.
run an orchestration agent.
Um...
So to run an orchestration agent, we just give it the specific orchestration prompt. That's easy and then
So, to run an organization agency.
the specific orchestration prompt that's easy and then
So we'd give it the orchestration prompt. It would turn, it would create the four subtasks.
So we'd give it the orchestration prompt. It would turn It would create the four subtasks
And then on each subtask.
And then on each subtask.
We could uh
we could uh
So in each sub-task, we could then run the sub-agent.
So in each sub-task, we could then run the sub-agent.
and
And...
the sub-agent
the sub-agent
Hmm, hmm, hmm, hmm.
Hmm, hmm, hmm, hmm.
Because before we didn't really have it completely integrated into our existing infra So the old way of doing agent orchestration was to create a new sub
Because before we didn't really have it completely integrated into our existing infra. So the old way of doing agent orchestration was to create a new sub-agent shell.
script. But now, ideally...
But now, ideally,
Just the subtask itself to contain all the information.
Just the subtask itself to contain all the information.
So we can try that in a minute
So we can try that in a minute
Um,
Um...
But for now, I just want to make sure that our new node plays
But for now I just want to make sure that our new node placement logic is okay
is okay.
Okay, so.
Okay, so...
Give me one second.
Give me one second.
I'm recording.
I'm recording.
Okay, there's another problem with Gemini is that the ampersand doesn't actually inject the prompt file So we need to actually inject the content With an environment variable, I guess
Okay, there's another problem with Gemini is that the ampersand doesn't actually inject the prompt file So we need to actually inject the content With an environment variable, I guess
Let's add to our won't do list
Um, let's add to our won't do list.
that when we close a terminal
that when we close a terminal,
that when we close the hover editor for a terminal that the actual terminal node is also
That when we close the hover editor for a terminal that the actual terminal node is also
It cleared, deleted, hidden.
It cleared, deleted, hidden.
okay another won't do problem that we've noticed is that um so we just made uh so two orphan nodes just created got created actually three orphan nodes
okay another won't do problem that we've noticed is that um so we just made uh so two orphan nodes just created got created actually three orphan nodes
15, 16 and 16.
15 16 and 16 and so two files
And so two files...
with the same
with the same
Node ID got made.
node ID got made.
And so we need to figure out exactly why that
and so we need to figure out exactly why that
That's a bug
That's a bug.
you
Okay, so right now we're trying to figure out exactly what sort of layout logic and physics we actually want. So one thing I realized is that if I drag a node, ideally I would like all the nodes that it's connected to, to also be pulled towards that node.
Okay, so right now we're trying to figure out exactly what sort of layout, logic, and physics we actually want. So one thing I realized is that if I drag a node, ideally I would like all the nodes that it's connected to, to also be pulled towards that node.
Okay, so one thing we need for our orchestration mode is we need it to somehow be able to click
Okay, so one thing we need for our orchestration mode is we need it to somehow be able to click
Also children.
Also children.
Um...
Um...
So maybe we can do like dependency traversal can do.
So maybe we can do like dependency traversal, can do dependencies.
dependencies.
Plus, go find...
Plus, go find...
Find the children of that node
Find the children of that node.
Um,
Um,
Um,
Um,
Uh...
Uh...
Yeah, yeah, we just need to then also do dependency traversal on the...
Yeah, yeah, yeah, we just need to then also do dependency traversal on the
Children of the node that we're running on and the children's children's children's children, etc, etc, etc
children of the node that we're running on and the children's children's children's children, etc.
One thing on our non-actionable ideas list should also be that our tree should be able to reference nodes which an agent has created, not just the ones that we've made manually.
One thing on our non-actionable ideas list should also be that our tree should be able to reference nodes which an agent has created, not just the ones that we've made manually.
You
Again everything we're going to quickly add to our non actionable ideas list is that for some reason
Again everything we're going to quickly add to our non actionable ideas list is that for some reason
Gemini is not using the same color for nodes.
Gemini is not using the same color for nodes.
Um, so we need to just ask it why.
Um, so we need to just ask it why.
Oh, good.
The Book of Al-Rajanesh is out now.
Oh good, the Book of Arrajneesh is out now.
in the shadow of enlightenment
The Shadow of Enlightenment.
Mm-mm.
Mm-mm.
I don't know.
I don't know.
Oh
Cough.
My darling.
My darling, is that you?
No.
Yeah.
Yeah.
You
Mm-hmm.
Mm-hmm.
Thank you.
Alright, alright, alright, alright.
Alright, so while working on that, we made a non-actionable ideas list.
With all the little things.
up but we didn't want to get distracted.
And now we're going to get.
Claude to act as an orchestrator
for creating all those stuff
So let's just, so now we should be able to say.
So it's one of the...
I'll see you next time.
Okay.
What are you going to do, have a run?
Just do a bit of exercise.
I'll see you next time.
We'll see you next time.
for new node placement, consider all other existing
Nose in the graph, so that like right here they don't appear too close to each other
um we also need to stop
Alright cool, we're going to open our voice stream.
That comes out.
All right, we just need to get some little fix ups working for our demo. Most things are already working. We just have a few things we're going to do.
So, for them.
Cool, can you hear me voice three?
We just need to get some little fix-ups working.
Most things are already working, we just have a few things we're going to do.
so for the demo something we need to do is um uh for you know placement um consider all other existing nodes in the graph so like right here they don't appear too close
We also need to stop doing restarting layout after we drag a node, that's been really annoying. And I know that there's somewhere in our code base that there's a layout.
And we have to comment that out.
And I know that there's somewhere in our code base there's
And we just need to comment that out so that the layout doesn't restart.
Okay, and the other thing we need to work on is that currently we've somehow broken the clod.sh. So it used to get the whole Python dependency, and now it no longer does. So we need to fix that.
The other thing we need to work on.
Restarting layout after we drag a node, that's been really annoying. A Python dependency, and now it no longer does, so we need to fix that.
So we need to fix that.
I want to figure out when we're going to go to the gym. Let's check when the gym's closed.
So it's Evelyn's rink. Maybe it's not in a Sunday.
All right, so we fixed the cloudless edge broken functionality. Let's just quickly check this.
Nice, looks like it's working. Cool. OK, so now we just want to make sure we need work on optimized.
So it used to get the whole Python dependency.
Python.
All right, so we fixed the Clotus Edge.
Nice, looks like it's working. Cool. Okay, so now we just want to make sure we work on optimizing node placement.
Let's actually, yeah, so let's get an engine to work on resetting now.
And let's add to a list of things we won't do, ideas that could be cool, but we're not going to do because of priority. So one thing is a way to actually delete a node and then have it deleted from our tree.
The other thing we wanted to do was to be able to collapse a node and all its neighbors into one container node.
That's also one of the new tasks.
Alright, so now we need to consider what exactly...
the demo where we do agent orchestration.
So we can try that in a minute.
That's me.
Gemini is not using the same color for nodes, so we just ask it why.
to add to a non-action ideas list, is that for some reason...
Bum!
Thank you.
Currently we've somehow broken the...
cloud.sh
So it used to get the whole Python dependency, and now it normally does, so you fix that.
One running.
All right, cool. Can you give me a voice drink?
All right, we just need to get some little fix-ups working for our demo. Most things are already working. We just have a few things we can do.
So for the demo, something we need to do is, for new node placement, consider all other existing nodes in the graph, so that right here they don't appear too close to each other.
We also need to stop doing restarting layout after we drag a node, that's been really annoying. And I know that there's somewhere in our code base there's...
stop. And we need a comment out so that the layout doesn't restart.
Okay, and then the other thing we need to work on is that currently we've somehow broken the clod.sh. So it used to get the whole Python dependency, and now it no longer does. So we need to fix that.
All right, so we fixed the clodis edge functionally.
All right, so we fixed it on this edge.
and now it no longer does, so we need to fix that.
All right, so we fixed the clodis edge button functionality.
So it used to get the whole Python dependency.
Ice, ice we fixed it ice we fixed the glottis edge
All right, so we fixed the cludders. All right, so we fixed the cludders. Edge broken functionality. Let's just quickly check this out.
As we fix the clodest edge functionality.
OK, so we fixed the cloudless edge broken functionality. Let's just quickly check this.
One ring.
as we were sharing.
I was six years old when I was missing a day.
All right, I want to figure out when we're going to go to the gym. Let's check when the gym's closed.
So it's evidence-wing.
of followers of the Indian Guru.
Bye.
Children like me.
All right, so we fixed the Clodos H broken functionality.
Nice. Looks like it's working. Cool. Okay, so now we just want to make sure we work on optimized unit placement.
Let's actually get an agent to work on restarting now. Let's do Claude.
To know
You
Thank you.
Thank you.
Brrrr!
Thank you.
Hmm.
See what mom's doing.
Thank you.
Working out.
I'm going to have a run.
Maybe, huh? Maybe.
You
I don't know how it works.
work
Crashing a bit
Yeah, I've got some great ideas.
Very useful.
So current large language models performance becomes worse as their context length.
Attention will always be complicated.
You only know for sure of a new piece of
by checking each one. There's quadratic complexity.
VoiceTree solves this by optimizing the input itself.
We take any text and progressively convert it into a graph containing those concepts and the relationships.
Then, instead of repeatedly sending LLMs the same appended unstructured text, we work alongside the LLM to prune to only the relevant sub-tree.
The results we have been seeing are amazing. We're seeing 70% fewer input tokens on average and 15% greater reasoning on the GSM Infinite benchmark.
Moreover, VoiceTree provides those same performance benefits to humans.
So humans, too, suffer from context degradation.
And in fact, what we can do is.
We can...
Remove this.
Entirely.
and instead just work on a shared memory graph.
I'll show you what I mean.
So what you're seeing right now has been running live.
I've been running VoiceTree.
on my voice, and then it's converting it to text and then to this graph.
I'll now launch an HR.
Now, so what you can see here...
Oh, what the fuck did I do here?
ah
LOL.
All right.
So LLM's performance decreases the longer their context gets. This is called context degradation. And the key reason for this is because they reprocess the entire context.
VoiceTree solves this at the input layer. We optimize the input itself. We take any text and progressively convert it into a graph containing the concepts and the relationships
Then, instead of repeatedly sending LLMs the same appended unstructured text, we work alongside the LLM to prune to only the relevant subtree.
The results with this have been amazing. We're seeing 70% fewer input tokens.
and 15% greater reasoning accuracy.
I'm the GSM Infinite.
Moreover, VoiceTree
provides these same performance benefits to humans. Humans too suffer from context degradation.
And in fact, the really exciting thing is
Why don't we just remove this entirely?
and work only
with the graph presentation. Have the graph as a shared memory.
between you and AI.
i'll show you what i mean so what you're seeing above is voice tree actually running live um so it's converting my voice to text and then updating the graph
What I can do now is I can spawn an agent.
And tell it to do something, like for example, create a mermaid diagram explaining how this could work.
So the agent
got only the relevant context from the graph.
Now that it's finished its task.
it adds a new node back to the graph.
Thank you.
Thank you.
Thank you.
Thank you.
Hmm.
Thank you.
Thank you.
We'll be right back.
Thank you.
We'll be right back.
Thank you.
Yeah.
Thank you.
The first thing is we want to change.
one of our tools.
which is I think called cleanup vault.
Let me see exactly what it's called.
Um...
Yeah, it's called cleanupvault.sh.
in the tools directory in
the VoiceTree repo, and currently that's getting rid.
of
some files that we don't want to get.
So currently it's getting rid of the XcaliDraw folder, which contains our drawings.
And we don't want that folder to be.
cleaned up when we run the cleanup.
So we don't want that folder backed up, we just want to leave that folder.
Um, yeah.
in the markdown tree vault directory, just leave it alone. Don't remove it, don't back it up, don't move it.
Now I get with the left hand.
It's hiding my text.
And he'd get rid of it.
You're going to have to zoom out. Command minus. Try command minus.
Okay.
It should be possible to do that.
it's just because you have your
Display to load.
I just restarted. No, just wait.
Let me know it.
Did you want to actually format it at all?
double-face that.
See, this would have... Double spacing. Yeah.
They're a double.
It's here.
I don't know. You found it. It was ready. It was just there.
it's really double spacing
Yeah, I don't know sir
If you ask AI or you Google it, you'll be able to do it.
So current LLMs have a performance problem with large context inputs and that's because for every request they append the, they resend the whole
Very inefficient.
and leads to context.
So Voice Tree solves that by converting the text into a graph
And then from that graph, so progressively converting your chat into a graph and then identifying relevant nodes and sending just the pruned tree back to the LLM, the pruned relevant nodes back to the LLM.
So we've our early results so we have a system work
And our early results are showing that on the GSM Infinite Benchmark, we're seeing significant...
Um,
a significant up to 70%
fewer tokens sent to the LLM and Accuracy improvements and it gets the right answer more often
Because there's just less context bloat, less to get confused about.
Cool, so I'll show you now.
So this has been running live, right now I've been running VoiceTreat live on my voice. So you can see here it's transcribing my text and then...
uh, providing, um, tree update events.
Um...
So, we can see here the tree.
And now, so I can also now spawn an agent.
Oh yes, what I wanted to say as well was...
We then had a breakthrough where we realized, hey, why even deal with linear text at all and instead send everything interface
with the AI, do all your interactions with the AI through this tree as the interface, almost like a shared memory. So I'll show you what I mean.
So, right, the tree is running and now I can open an agent.
Um...
you see here it gets all the
relevant context from the graph.
and
I can say, create simple.
So let's download Mackie.
And I can tell the AI to now also add back to our tree, our shared memory.
um
Amazing. So now we have that document.
Um,
And...
Cool. Yeah
So today's large language...
have a problem with performance at large input size.
And partly that's because they reprocess the entire transcript.
for every request to mimic a sort of memory but attention to items and memory
scales quadratically because it's an all-to-all comparison so you get degrading performance
VoiceTree solves this by progressively converting a linear text into a graph.
we're optimizing the input itself.
We then send the LLM only the pruned relevant
So we're seeing really promising early results on the GSM Infinite benchmark, where we're seeing up to 70% fewer input tokens.
And a step change from models previously not being able to answer the question at all and just going in circles, wasting output tokens.
to getting the correct answer consistently.
So current large language model...
have a performance problem.
especially at long input context length.
And that's partly because they reprocess the entire transcript.
for every request, to mimic a sort of memory, but attention to items and memory scales quadratically because it's an all-to-all comparison.
So Voice Tree solves that problem.
by optimizing the input itself, progressively converting the linear raw text into a graph of those key concepts and relationships.
We then work with the LLM to identify the relevant nodes and prune the context only to what's useful.
question. Our early results
on the GSM Infinite Bench.
have been really promising. We're seeing up to 70% fewer input tokens.
and step function change in output where previously models that couldn't generate the correct answer and would just go in circles wasting out
can now consistently get the correct answer to hard questions.
So I'm running VoiceTree now live on my voice and so my I'm transcribing my voice to text and then you can see VoiceTree there doing graph
Why? Why am I doing that? So what we realized is, is that once you're using this system...
You don't actually
Need to deal with this.
format, linear text at all, and instead you can deal, you can interface with LM's directly through the graph.
Uh, humans too.
degrade, their context to degrade.
So, when that happens, our memory is now, our working memory is now shared. And that's really cool because if I now...
launch an agent here
I can tell it to do something.
it's going to get only the relevant content.
from the graph.
And when I get it to do something, it's going to add its response back to the graph itself.
There we go. So I've been using this for software engineering.
to build VoiceTree itself every single day. It is so, it's amazing. I'll show you an example now.
I'll show you a real feature I've built with VoiceTree now.
So today's large language models have a problem with performance at large input sizes. And partly that's because they reprocess the entire transcript history for every request to mimic a sort of memory. But attention to items in memory is scaled quadratically because it's an all-to-all comparison. So you get degrading performance. VoiceTree solves this by progressively converting linear text into a graph. We're optimizing the input itself. We then send the LM only the pruned relevant content.
So, we're seeing really promising early results on the GSM Infinite benchmark, where we're seeing up to 70% fewer input tokens, and a step change from models previously not being able to answer the question at all, and just going in circles, wasting output tokens, to getting the correct answer.
for every request um to mimic a sort of memory um but attention times memory scales quadratically because it's an all-to-all comparison so voice tree solves that problem by optimizing the input itself progressively converting the linear raw text into a graph of those key concepts
We then work with the LM to identify the relevant nodes and the prune, the context only.
question. Our early results on the GSM infinite benchmark have been really promising. We're seeing up to 70% fewer input tokens and step function change in output where previously models that couldn't generate the correct answer and would just go in circles wasting output tokens can now consistently get the correct answer to hard questions. So I'm running VoiceTree now live on my voice and so I'm transcribing my voice to text and then you can see VoiceTree there doing graph actions. Why? Why am I doing that? So what we realize is that once you're using the system you don't actually
Alright, testing one, two, three, testing one, two, three, look at me, look, look, look at me, testing one, two, three, look, look at me, I'm testing one, two, three, look, look at me.
All right, we're testing currently how well where the position of new nodes spawn in voice
All right, we're testing right now where our nodes are.
Alright, we're testing right now.
what the exact position will be of a new note.
All right, right now we're doing a completely live demo of VoiceTree.
Let's create a new node for that.
All right, right now we're doing a live demo of VoiceTree.
and we'd like to see where the new note appears.
So large language models currently have a performance problem.
where especially at long input contact sites.
they struggle.
So current large language
have a performance problem at long input lengths. And part of the reason for this is because they reprocess the entire chat history for every request. And then this is to mimic a sort of memory, but attention to items and memory scales quadratically because it's an all-to-all comparison.
So VoiceTree solves this by optimizing the input itself. So we progressively convert input into a graph containing the key concepts and relationships between them. We then work together with the LLM.
to identify what exactly is needed to answer.
a specific question.
So, our early results on the GSM infinite
are amazingly promising. We're seeing up to 70% fewer input tokens.
as well as a behavior where previous models couldn't even answer a hard question and would just go in circles wasting output.
We're seeing them now get the correct answer.
So I'm running VoiceTree right now live on my voice itself. So you can see this window here. It's transcribing my voice to text and then resulting in.
tree actions.
But, why am I doing that? Well, one of our key realizations...
is that humans as well suffer from...
and can get the same benefits from VoiceTree.
So then what we realized is why not in fact completely get rid of this linear text format and instead only interface with the AI through this tree
I'll show you what I mean by that.
So our memory is now shared
So if I open
an agent on one of these nodes.
it's going to get only the relevant input context from the tree.
And then, if I tell it to do something...
It's going to save its output back to the tree.
So current large language models have a problem where their performance degrades at long input context length.
And part of the reason for this is because they reprocess the entire chat history for every single request.
to mimic a sort of memory, but attention to items and memory is quadratic, it's an all-to-all comparison.
So, what VoiceTree does is we optimize this input itself, we progressively convert it into a graph with the key concepts and...
And then we work with the LLM to identify what nodes are relevant and return only the pruned relevant.
Our early results on the GSM Infinite Bank
are very promising.
we're seeing up to 70% fewer input tokens.
we're seeing previous models that couldn't answer certain questions now being able to consistently as opposed to going in circles wasting output tokens confused on the hard problem
So I'm running it right now live. I'm running the VoiceTree algorithm on my voice.
that's what you're seeing in this top right corner and it's creating this concept graph here of what I'm saying
And why am I doing that?
So
Mmm, so close, so close.
All right, we're doing a live demo of voice.
I would like that to appear as a new note.
a new node to represent the live demo.
So current large language models have a performance problem at long input context length And part of the reason for that is that they took mimic memory. They reprocess the entire transit transcript history for every single
And then on top of that, attention to items and memory scales quadratically.
So VoiceTree solves this performance.
by optimizing the input it
We progressively convert any raw text into a graph.
All right, I would like to draw a new...
to represent the live demo of
Please create it.
A live demo of VoiceTree.
All right, so current large-language model.
All right, so current large language models have a performance problem at long input.
And part of the reason for that is because they mimic memory by reprocessing the entire transfer.
for every request.
And on top of that, attention to items in memory scales quadratically because it's an all-to-all comparison.
So, VoiceTree solves this by optimizing the input itself. We progressively convert raw text into a graph containing the concepts and key relationships.
We then work with the LLM to identify what nodes are
and return just that pruned context back.
So we're seeing very promising results on the GSM Infinite Bench.
where we're seeing up to 70% fewer input tokens.
and behavior where models that previously couldn't answer certain hard questions now are able to, instead of just wasting output tokens on the wrong answer.
So, I'm running VoiceTree right now.
completely live. So it's converting my voice to text to then do a tree action.
such as that one there. But why am I doing that? Well, one of the key realizations we had was that,
the same context integration that lms have humans have as well and voice tree can provide those same performance benefits to humans um but then we don't stop there what we realize is well then we can do away with linear interface completely and instead interact with ai purely through this tree itself
And then our memory
shared. I'll show you what I mean by that.
I'll launch an agent here.
And you can see that it gets only the relevant context from the tree.
and inject it into it.
And then if I get to do something like draw a diagram explaining what this would look like
it's going to save its output back to the tree.
like it just did here.
So this is incredibly useful for software engineering and I'll show you some real examples of me using it to build features for VoiceTree using VoiceTree itself.
All right, so current large-language models have a performance problem at long input lengths. And part of the reason for that is because they mimic memory by reprocessing the entire transcript history for every request. On top of that, attention to items in memory scales quadratically because it's an all-door.
So, VoiceTree solves this by optimizing the input itself. We progressively convert raw text into a graph containing the concepts and key relationships within them. We then work with the LLM to identify what nodes are relevant.
So, we're seeing very promising results on the GSM Infinite benchmark, where we're seeing up to 70% fewer input tokens. And behavior where models that previously...
certain hard questions, now are able to, instead of just wasting output tokens on the wrong answer. So I'm running VoiceTree right now, completely live. So it's converting my voice to text to then do tree actions, such as that one there. But why am I doing that? Well, one of the key realizations we had was that the same context iteration that elements have, humans have as well. And VoiceTree can provide the same performance benefits.
But then we don't stop there. What we realize is, well, then we can do away with linear interface completely. And instead, interact with AI purely through this tree itself. And then our memory is now shared. I'll show you what I mean by that.
So, I'll launch an agent here.
And you can see that it gets only the relevant context from the tree injected into it. And then if I could do something like draw a diagram explaining what this would be.
It's going to save its output back to the...
like it just did here.
So this is incredibly useful for software engineering, and I'll show you some real examples of me using it to build features for VoiceTree using VoiceTree itself.
All right, so current large language models have a performance problem.
at long input length and part of the reason for that is because they mimic memory by repressing
Thank you.
contains the backups of all our Markdown volts of where our VoiceTree outputs are stored.
All right, good.
Let me show you the light.
It's good to go in.
Okay, let me show you.
Or we can walk through the...
How is the algorithm?
Do you have my Roger letter? Yeah.
You
You
Kndelri und Kndelro hat Gesundheit ebenso.
All right, Lance calling.
Hey bro, I just I just recorded a live demo and I'm about to show it to the parents
Why don't you watch at the same time?
while I show it to them, so I'll mute myself, I'm muting myself now.
All right. So, so, so, so, so, so should I show you the early final founder video as well?
Ilan, I'm showing them first the founder video and then immediately after the other video because I'm sure that's what YC will do as well.
Hey YC, I'm Manu. I have a passion
performance optimization really in all areas of my life but especially software hey yc i'm manu i have a passion for
Really, in all areas of my life, but especially software. I just started out with my first role in the doctor.
to now my work at Atlassian where last year I saved the
$3 million through database optimization.
I'm now applying that same mindset to AI. So I built VoiceTree, an algorithm which optimizes the inputs to LLMs by representing it as a tree.
nodes. This decreases token usage and increases accuracy. Great, it's amazing, but the real breakthrough has been that I now, using this technology myself, no longer use linear chat. All my interactions with AI are through this tree itself, where it acts as sort of like a shared memory. So I'll show you what I mean in the demo, but it's real, it's working, it's very exciting, and now I'd like your help.
For me to get this into the hand developers everywhere as quickly as possible. Let's do it
Okay, next.
They won't? Okay. Martin, what do you think?
You can hear it. She's moving
All right, so current large language models have a performance problem at long input.
And part of the reason for that is because they mimic memory by reprocessing the entire trance
for every request.
And on top of that, attention to items and memory scales quadratically because it's an all-to-all comparison.
So, Voice Tree solves this by optimizing the
We progressively convert raw text into a graph containing the concepts and key relationships.
We then work with the LLM to identify what nodes are relevant and return just that pruned context back to the LLM.
So, we're seeing very promising results on the GSM.
where we're seeing up to 70% fewer input.
and behaviour where it models
couldn't answer certain hard questions now are able
instead of just wasting output token
on the wrong answer.
So I'm running VoiceTree right now completely live. So it's converting my voice to text to then do tree actions such as that one there. But why am I doing that? Well, one of the key realizations we had was that...
the same context iteration that lms have humans have as well and voicetree can provide those same performance
to humans. But then we don't stop there. What we realize is, well, then we can do away with linear interface completely, and instead interact with AI purely through this tree itself.
And then our memory is
shared. I'll show you what I mean by that.
I'll launch an agent here.
And you can see that it gets only the relevant context from the tree.
and inject it into it.
And then if I get to do something like draw a diagram explaining what this would look like
it's going to save its output back to the tree.
like it just did here.
So, this is incredibly useful for software engineering. And I'll show you some real examples of me using it to build features for VoiceTree using VoiceTree itself. Yeah. I'm going to actually land clean. Hey, the parents just finished watching it. That was it? Yeah. So, there's not the real world use case there yet. How are you going to add that in because it's only a three month? Yeah. Well, I'm thinking I can cut this one down to two minutes. There's a bit of fat that I can trim. Um, this one I was trying to do without a script. Like, I was just using bullet points. But I realized that for the demo, it's actually better if it's perfectly scripted.
Yeah.
It's a good product. Yeah.
i think we can just own yeah yeah i can do that um a bit more polished okay yeah that's true um and what do you think about um so you don't want me to use
the
the the pencil to rub it out or you want me to do it better just give me I think
Okay, and do you think, um...
Um...
Should I get, like, mom?
control my computer so I can focus just on speaking.
it. Yeah. Can't you just speak afterwards? Record it after. That's what I would do.
Um...
Yeah, I could do that as well, but then getting like it's just an extra step of work that I need to Record on top of it and make sure it times up well And then it's not actually live which is
They might be able to figure it out. I don't know how, but little small clues, right? Yeah, yeah. Yeah, I realize that's bad because it looks like I'm not typing. So should I film from the side so you can see my side profile of me typing? Or should I just...
Yeah.
Yeah, you think from the side with my iPhone camera. Yeah
Yeah, the iPhone better cameras.
That's when I filmed the founder introduction.
Okay, nice.
So, today's large language models become worse as context length increases because to mimic memory, they reprocess the whole chat history for every request.
but full all-to-all attention.
So today's large language models become worse as context length increases because to mimic memory, they reprocess the entire chat history for every request.
but full all-to-all attention scales quadratically. So VoiceTree's solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Our initial tests on complex long context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70 percent and more importantly models are solving questions they just couldn't answer
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from context degradation.
So the really exciting breakthrough we've made is why not just remove this chat interface?
Completely.
Instead
Interact directly with the AI through a shared memory.
I'll show you what I mean.
What you're seeing is the voice tree algorithm running live on my voice, building this tree.
I'll now launch an agent.
And the agent is going to get injected into its context, only the relevant context.
the graph.
If I then get it to do a task, like draw me a diagram.
It's going to add it back to the tree.
Voila!
So today's large language models become worse as context.
Because to mimic memory, they reprocess the whole chat history for every request.
but full all-to-all attention.
So today's large language models become worse as context-
because Dominic memory, they reprocessed the entire chat history frame.
but full ultra-alt attention scales quadratically. So Voice Tree's solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model. Our initial tests on complex, long-context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions that is...
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from chronic degradation.
So the really exciting breakthrough we've made is, why not just remove this chat interface completely? And instead, interact directly with the AI through a shared memory. I'll show you what I mean. So what you're seeing is the voice tree algorithm running live on my voice, building this tree in real time. I'll now launch an agent.
And the agent is going to get injected into its context, only the relevant content from
If I then get to do a task, like draw me a diagram.
It's going to add it back to the tree.
Voil.
So today's large language models become worse as context length increases because to mimic memory they reprocess the entire chat history for every
But full, all to all attention.
So voice-tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to...
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions that just remove this chat and instead interact directly with the AI through a shared memory. I'll show you what I mean. So, what you're seeing is the VoiceTree algorithm running live on my voice, building this tree in real time. I'll now launch an agent.
And the agent is going to get injected into its context, only the relevant content from
If I then get to do a task, like draw me a diagram.
It's going to add it back to the tree.
Voila.
So today's large language models become worse as context.
because to mimic memory, they reprocess the entire chat history.
but full ultra-alt attention scales quadratically. So voice-tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to mod-
Our initial tests on complex, long context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly before.
Moreover, these benefits aren't just for AI, they're for us too.
We humans also suffer from...
So the really exciting breakthrough we made is.
So today's large language.
So today's large language models become worse as context limiting.
Because to mimic memory, they reprocess the whole chat history.
But full ultra-ol tension scales quadratically.
VoiceTree's solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model. Our initial tests on complex, long-context reasoning
So today's large language models become worse as they're...
because to mimic memory, they reprocess the entire chat history.
but full all-to-all attention scales quadruple.
Voice tree solution is to progressively convert the conversation into a tree and then only then send the relevant
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%. And more importantly, models are solving questions
Now, these benefits aren't just for the age.
They're for us to we humans also suffer from.
So then the really exciting breakthrough
Why not?
Just remove this chat history entirely.
and instead interact directly with the AI through a shared memory.
I'll show you what I mean.
So what you're seeing up top is the voice tree algorithm running live on my voice, building this tree in real time.
I'll now launch an
and
As I launch the agent, it's going to get injected into it, only the relevant context from the tree.
Then if I get it to do a task
its output is going to be saved back to the tree.
Hey Siri, where are you?
So today's Largelink
So today's large language models become worse as...
increases because to mimic memory, they reprocess the whole chat.
for every single request.
but full all-to-all attention scales quadratically. So voice tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from...
So Ben, they're really exciting.
Why don't we just remove this chat history entirely?
and interact with the A.I.
through directly with this shared memory.
I'll show you what I mean.
So right now, what you're seeing is VoiceTree, the VoiceTree algorithm running live on my voice, building this tree in real time.
I can now spawn an agent.
and it will get injected into it.
only the relevant parts of the text.
from the graph
Now when I get it to do something for example explain this with a diagram its output is also going to be saved back to the
Amazing.
So today's large language models become worse as context link increases because to mimic memory, they reprocess the whole chat history for every record.
But full all-to-all attention scales quadratically.
VoiceTree's Solution
is to progressively convert the conversation into a tree and then only send the relevant branches
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions
Moreover, these benefits aren't just for the
We humans, too, also suffer from...
So then the really exciting breakthrough.
Why not remove this chat interface entirely?
and interact directly
with the AI through a shared memory.
I'll show you what I mean.
Today's large language...
become worse as context length increases. Because to mimic memory, they reprocess the whole chat history.
But full all-to-all attention scales quadratically.
VoiceTree's solution is to progressively convert the conversation
to a tree, and then only send the relevant branches.
Our initial tests
On long context, reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%. And more importantly, models are solving questions
couldn't answer directly before. Moreover, these benefits aren't just for the
They're for us, too. We humans also suffer from context degradation. So then the really exciting breakthrough is, why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
So I can spawn an agent.
on any one of these nodes
Once I spawn the agent
It gets only the relevant content.
The graph.
If I get it to do something.
It's going to now also
add back its context.
back into the graph.
Today's large language models become worse.
increases because to mimic memory, Ray reprocessed the whole chat history.
But full all-to-all attention
VoiceTree's solution is to progressively convert the conversation into a tree.
And then only send the relevant branches to the model.
Our initial tests on complex long context reasoning
We're seeing input token reductions of up to 70%, and more importantly, models are solving problems they just couldn't answer correctly before.
Now, these benefits aren't just for the AI, they're for us too. We humans also suffer.
So then the really exciting breakthrough.
Why not just remove this chat interface entirely and interact with the AI directly through a shared memory?
I'll show you what I mean.
So right now,
What you're seeing is the VoiceTree algorithm running live online.
building this tree in real time.
If I now launch an agent.
It gets injected into it only the relevant content.
If I get it to do a task,
For example to explain this with a diagram
It's gonna save it back to the graph
Today's large language models become worse as context length increases because to mimic memory, they reprocess the whole chat history for every...
But fool, out of-
today's large language models become worse as context length increases because dominic memory they reprocess the whole chat history for every
but full of attention scales.
Wastry solution is to progressively convert the conversation into
and then only send the relevant branches to the model. Our initial tests on long context reasoning benchmarks are incredible.
We're seeing input token reductions up to 70%. And more importantly, models are solving questions.
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from...
So then the really exciting breakthrough is why not remove this chat interface entirely and instead interact directly with the AI through a shared memory. I'll show you what I mean.
So I can spawn an agent on any one of these nodes.
Once I spawn the agent, it gets only the relevant context from the graph.
If I get to do something, it's going to now also add back its context.
Back into the graph.
Today's large language models become worse as contact length increases because to mimic memory they reprocess the whole chat history for
but full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation
into a tree, and then only send the relevant
Our initial tests on complex long context reasoning benchmarks.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly before. Moreover, these benefits aren't just for the AI, they're for us too. We humans also...
So then the really exciting thing
why not just remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
What you're seeing is the voice tree algorithm running live on my voice, building this tree in real time.
I'll now launch an ad.
As I launched the agent,
it will get injected into it only the relevant content.
from the existing tree.
If I get it to do a task,
It's going to add back its result to the tree.
Today's large language...
become worse as contact length increases because to mimic memory, they reprocess the whole chat history for every request.
But full, all-to-all attention.
VoiceTree's solution to this is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Our initial tests on complex, long context reasoning
are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Now, these benefits aren't just for the AI, they're for us too. We humans also...
So then the really exciting breakthrough.
Why not just remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
So right now, VoiceTree has been running live on my voice.
building this tree in real time.
I'll now launch an agent.
As I launch it, it's going to get injected into itself, only the relevant content.
Then if I get it to do a task like draw a diagram
It's going to add that back into the tree.
saving its progress.
Today's large language models become worse as context.
Because Dominic memory, they reprocess the whole chat history for every time.
Full ultra-attention scale is quadratic.
VoiceTree's solution to this is to progressively convert the conversation into a tree and then only send the relevant branches to them.
Our initial tests on complex, long context-reasoning benchmarks are incredibly promising. We're seeing input token reductions of about 70%, and more importantly, models are solving questions they just couldn't answer correctly before. Now, these benefits aren't just for the AI, they're for us too. We humans also suffer from...
So then the really exciting breakthrough is why not just remove this chat interface entirely and instead interact directly with the AI through a shared memory. I'll show you what I mean. So right now, VoiceTree has been running live on my voice.
building the street in real time. I'll now launch an agent.
As I launch it, it's going to get injected into itself, only the relevant context.
Then, if I get to do a task, like draw a diagram, it's going to add that back into the tree.
Saving its progress.
Today's large language model.
You
...become worse as contact length increases. Because Dominic Memory, they reprocess the whole chat history for every...
Before all attention scales.
VoiceTree's solution to this is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Today's large language model
I'm worse.
length increases. Because to mimic memory they reprocess the whole chat history.
for every request but full all-to-all attention scales quadratically.
VoiceTree's solution is to progressively convert the conversation
into a tree and then only send the relevant branches.
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
We're seeing input token reductions.
of up to 70% and more importantly models are solving questions.
Now, these benefits aren't just for the AI. They're for us, too. Humans, too, suffer from...
So then, the really exciting breakthrough.
Why not just remove this chat interface entirely?
and interact directly with the AI through a shared memory. I'll show you what I mean.
What you're seeing is the voice tree algorithm running. Oh, I didn't do very well
I'll see you next time.
Today's large language
become worse as context length increases.
Today's large language models become worse as context length increases because to mimic memory, they reprocess the whole chat history
But full all-to-all attentions
VoiceTree's solution is to progressively convert the conversation into a
and then only send the relevant branch.
Our initial tests on complex long context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%.
And more importantly, models are sold.
but they just couldn't answer correctly.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from...
So then the really exciting.
why not just remove this chat interface entirely and Interact directly with the AI through a shared memory
I'll show you what I mean.
So
What you're seeing is the voice tree algorithm
running live on my voice.
building this tree in real time.
But it's a bit too far away, so yes.
Today's large language models become worse as context length increases. Because to mimic memory, they reprocess the whole chat history for every request.
But full, all-to-all attention scales quadruple.
VoiceTree's solution is to progressively convert the conversation into a tree.
and then only send the relevant brand.
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Now, these benefits aren't just for the agent.
They're for us too. Humans as well suffer from...
So then the real exciting
why not just remove this chat interface entirely?
and interact directly with the AI through a shared memory instead.
I'll show you what I mean.
Are you fucking kidding me?
Today's large language models become worse as context length increases because to mimic memory they reprocess the whole chat history.
But full all-to-all tension scales quadratically.
Voice Tree's solution is to progressively convert the conversation into a tree.
and then only send the relevant brand.
Our initial tests
Today's large language models become worse as...
Because to mimic memory, they reprocess the whole chat history.
and full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation.
tree, and then only send the relevant branches.
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from context degradation.
So then the really exciting.
why not just remove this chat interface in entirely, and instead, interface directly.
through a shared memory.
I'll show you what I mean.
What you're seeing is the voice tree algorithm running live on my voice
building this tree in real time.
I'll now launch an agent.
it's going to get injected into itself, only the relevant content.
If I get it to do a task.
it's going to add back its output back into the tree.
Today's large language models become worse as contact
Because mimic memory, they reprocess the whole chat history for everyone.
and full all-to-all attention scale.
Voice 3 solution is to progressively convert the conversation
and then only send the relevant branches to the
Our initial tests on complex long context reasoning
We're seeing input token reductions up to 70%, and more importantly, models are solving questions.
Now, these benefits aren't just for AI, they're for us too. Humans as well suffer from content.
So then the really exciting breakthrough is.
why not just remove this chat interface in entirely and instead interface directly through a shared memory i'll show you what i mean
What you're seeing is the voice tree algorithm running live on my voice, building this tree in real time.
I'll now launch an agent.
It's going to get injected into itself, only the relevant context.
If I get it to do a task.
as context length increases because to mimic memory they reprocess the whole chat history
and full all-to-all attentions.
Voice tree solution is to progressively convert the conversation into a tree and then only send the relevant
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Now these benefits aren't just for the AI, they're for us too. Humans as well suffer from
So then the really exciting breakthrough is
Why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
What you're seeing is the VoiceTree algorithm running live in my voice.
building this tree in real time.
I'll now launch an agent.
No context warming, warm-up, or re-prompting.
It's all
already injected into the tree.
Let's get it to draw us an architecture diagram.
its progress is being saved back to the tree.
Today's large language models become worse as context.
Because to mimic memory, they reprocess the whole chat history for everyone.
and full all-to-all attention scale.
Voice tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to model
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70.
And more importantly, models are solving questions they just couldn't answer correctly.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from...
So then the really exciting breakthrough is why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean what you're seeing is the voice tree algorithm running live in my voice
building this tree in real time.
I'll now launch an agent.
No context warming, warm up, or re-prompting.
It's all already injected into the tree.
Let's get it to draw us an architecture diagram.
Its progress is being saved back to the tree.
back into the tree
Today's large language models become worse at this con.
Because to mimic memory, they reprocess the whole chat history for everyone.
and full all-to-all attention scale.
Voice tree solution is to progressively convert the conversation into a tree and then only send the relevant branch
on complex, long context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%
And more importantly, models are solving questions they just couldn't answer correctly.
now these benefits aren't just for the ai they're for us too humans as well suffer from
So then the really exciting breakthrough is, why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean. What you're seeing is the voice tree algorithm running live in my voice.
building this tree in real time. I'll now launch an agent.
No context warming warm-up or re-prompting.
it's all already injected into the tree.
Let's get it to draw us an architect.
Its progress is being saved back to the tree.
memory they reprocessed the whole chat history.
But full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation into a tree, and then only send the relevant branches to the model.
Our initial tests on complex, long context reasoning
are incredibly promising.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from...
So then the really exciting breakthrough is
why not just remove this chat interface entirely and instead interact directly
with the AI through a shared memory.
I'll show you what I mean.
So, right now...
Voice tree algorithm running live on my voice, building this tree in real time.
I'm worse as.
increases because to mimic memory they reprocess the whole chat history
But full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation in your voice.
and then only send the relevant branch.
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%. And more importantly, models are solving
now these benefits aren't just for the ai they're for us too humans as well suffer from
So then the really exciting breakthrough is
Why not just remove this chat interface entirely and interact directly with the AI?
Through a shared memory
I'll show you what I mean.
What you're seeing here is the voice tree algorithm running live on my
Building this tree in real
I'll now launch an agent.
So no context warmup or reprompting needed, it's already injected into the
and all progress is being saved back to the tree as well.
Today's large language models become worse as context.
because the mimic memory they reprocess
but full all-to-all attention scale
Voice tree solution is aggressively convert the conversation.
and then only send the relevant branch.
Our initial tests on complex, long-context-reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly.
Now, these benefits aren't just for the AI. They're for us too. Humans as well suffer from contact.
So then the really exciting breakthrough is, why not just remove this chat interface entirely and interact directly with the AI through a shared memory? I'll show you what I mean.
What you're seeing here is the VoiceTree algorithm running live on my voice.
building this tree in real time.
I'll now launch an agent.
Oh, so no context, it's already indicated the tree and all progress is being saved back to the tree.
Okay.
Thank you.
You
All right, cool. So today I'm working...
feature for VoiceTree, which is the ability to take an input forest and from that forest...
color each subtree.
And so what exactly I mean by that is we're going to take each forest and then for each tree in the forest.
we're going to find the subtrees within that forest.
Sorry, sorry, we're going to find the subtrees within a tree. And then we're going to...
find all the possible
groups, the themes within the...
tree. So we'll find all the themes within the tree, general containers that might...
projects, it might be different abstractions, who knows, it's dynamic, so we'll get a large language model to be given a tree, as in all the nodes in a markdown folder, and then to
convert
Okay, cool. So let's talk about what exactly.
do um to get that done um all right so
We want to talk a little bit about the term
So I'm not 100% sure if I'm using the correct terminology of subtrees and trees.
forests, so it would be good to clarify that and maybe get an LM.
And then we need to figure out what are the steps required to do this.
Theme identification is going to be done with a large language model agent.
Um, and it's going to be given, um,
input of a tree. We already have a function called, we already have a function to load a Markdown repository, put it into a tree data structure, and then send that tree data structure to LLM. We have a function called format nodes. So that should be quite helpful there.
Um...
Um, okay, so...
The full flow, let's think about the full flow. So we need to take a folder and then convert it into our tree dial structure.
And then we, for each subtree in that tree data structure,
We send it to the LLM and say, hey, within this subject,
um, find what are the, um,
available nodes uh hang on let's hold that thought
Um,
Alright, so what were we saying?
So one question I have is I don't know whether to input just a subtree Or for example, we could start maybe the MVP
is just inputting a whole folder. Our folders really never have more than 100 nodes.
Um, so it should be, um, not too large for, um,
Gemini to be able to handle, Gemini Flash, but then let's create another node to connect.
which is the future performance optimization we know we're going to have to make where in the future we know we're going to have to be able to run on
a very large forest of potentially up to a thousand nodes.
Um,
And so...
Um...
Okay, cool. So for now, we need to figure out just the core components for the MVP, which is input a folder up to 100 nodes, convert it into a data structure, and then our tree data structure. So I think we have functions that already do that, markdown to tree. And then with that tree data structure, we need to call an agent to find all the groups within that tree, which can itself contain disconnected components. That's the word.
And we want it to identify maybe.
however many clear subtrees there could be and then for now I think what we want to do is we just want to color those sub
Oh, okay, and one small thing we need to do is when we load the folder, the markdown folder, into our tree data structure, we want to remove all existing color in the YAML front matter, because we're going to overwrite that.
with our new color, which will be the subtree color.
Um...
Okay, cool, I think...
Um...
we can now spawn and orchestrate.
so
Claude
then I'm going to bring you.
here
Okay, and so then the voice tree performance optimizes
The key thing there is, so if we want to find common groups among a thousand nodes, but we can't send all a thousand nodes at once, what that means is if we send a hundred at a time, they might find distinct groups. And then we need a way to sort of merge those groups back. So in the end, if we sent a thousand nodes, we still only want about 10 different subtree colors. And so the problem is, is that each of those, when we send a hundred nodes each, they might each come up with like five different subtrees. And so we need a smart way of ensuring that...
We don't end up with 5 times 10 different...
We don't end up with 5 times 10 different
subtree groups.
Sorry, that'll be 10 times 10.
And I don't know the best way to do this, so we're going to need...
Um, and then what we're going to do.
after the VoiceTree sub-tree coloring feature, so what's going to come after that?
is we want a way to then, out of those sub-truths,
Collapse the subtree actually.
into just one node and redo all the...
edges as would be required so that you can sort of click on it on a subdry and it either expands or collapse
It collapses into all the nodes that are contained within that subtree.
So what we really want here is sort of like recursive trees. So each node itself can be a tree. Cool.
Oh, okay. So I really like that solution for merging batched voice tree groups, which an AI just came up with, which is we can just send the existing subtree names that we already have.
to the LLM call and that way it can know what existing groups it can already reference.
No. Stop. Stop.
Please go around me.
You
Here we go.
Okay, at VoiceTree theme identification, I'm going to clarify that we don't need Carol, the implement subtree processor, because for now, for the MVP.
Um, we're just going to implement.
We're just going to input a folder, and in fact that folder can be called input forest, input underscore forest. And so we don't need to identify disconnected components yet or extract subtrees, we can do that later.
Um,
Okay, we're at the stage now where we want to test our solution. It looks like all sub-agents have finished their work.
Um,
Mm-hmm.
Okay, cool. So now we're at the stage of testing again with our input first. So the agents have said that they've done it. However, I don't see the colors, so...
All right, so we're talking about right now the next step.
which is Voice Tree Performance Optimization.
So let's open here Claude
And we identified a key problem in the current solution of Voice3 theme identification, which is that it's too sparse.
So we will need to improve that.
All right, we've done it. It's good enough for now. We'll take a break and then we'll come back
So we've now finished analyzing subjects.
And our solution was quite complex, but we were able to manage more than.
Um...
So
It worked in the end.
So here, sub trees now have their own
color, which is pretty cool.
Uh...
Let's look at the...
Yeah, good enough.
All right. I want to record an example of me using VoiceTree to build a new feature for VoiceTree, and quite a large feature, which is the ability to identify subtrees, wind trees, actually wind forests, and then to color each subtreet. OK, cool. So let's launch VoiceTree.
All right, cool, today I'm working on a...
feature which is the ability to take an input forest and from that forest color each subtree. And so what exactly I mean by that is we're going to take each forest and then for each tree in the forest we're going to find the subtrees within that forest. Sorry, we're going to find the subtrees within a tree and then we're going to find all the possible groups, the themes within the subtree. So we'll find all the themes within the tree, general containers that might be different projects, it might be different abstractions, who knows, it's dynamic. So we'll get a large language model to be given a tree as in all the nodes in a markdown folder and then to convert.
Thank you.
Hein.
what's happening um just final fix-ups
Well, actually some major, major work I still need to do.
Would you just use the beef cutter money to be created? Well, I mean...
I don't have time to think now.
Okay.
How much longer do you need?
you know, a few hours.
I mean, but this is it.
Mmm.
I need to work at eight.
I don't have to. Do you have any meetings? I don't think so.
Did you have a meeting with him today?
Yeah. Mm-hmm.
Ah.
Okay. Yes, vegetables if you want.
okay i'm gonna go to sleep do you need me can i do anything for you
Um,
I have water goods, pineapple on the fridge.
Fresh pineapple.
if we get hungry.
Hmm, I think you know, thank you for this
Five minutes of stretch.
Did you go to the gym? Yeah, briefly.
Thank you.
I'm going to get rid of them tripped up. Ah, I'm going to...
and then I went to the gym. That was good.
Today it's very crowded but fine, I'm in such a big gym.
Mm-hmm.
Alright, we made one mistake.
in the create agent sub-agents
When they create the subtasks, they need to fill out all the content already in the tools slash prompt slash subagent prompt.md
and create the MD file of that template filled out. How should we do that?
we have some options
But the problem is...
that in...
the create sub-agents command.
dot MD It gives an example of creating sub tasks Not using the template, but rather just using a string Like an example string
which is misleading. So you wanna fix that example.
in the createSubAgentCommand.md on line 117. It's in the git diff as well, you can just check the git diff for it.
Thank you.
You
Thank you.
Okay another problem I have I think is that the cloud hook configuration so the cloud settings dot JSON file
which is in the tools folder.
Um...
I don't think the format of that JSON looks correct. I think there should be an outer hooks.
thing and I also want to move that file to our project
or global or local.
code settings.
Um...
So let's fix the type.
the json the format of the json and integrate it with our normal settings
Thank you.
I'll see you next time.
Thank you.
You
We'll be right back.
Thank you.
You
fix that when I give you a massage.
I'll see you next time.
Hmm.
Okay, now I want an agent.
To for me
Create a detailed architecture diagram.
of the current.
system for launching agents.
The key files we have for launching agents.
and what prompts they get for the orchestrator agent or the subtosk agent.
Um,
And as a hint.
I know that this system currently starts when you launch dot slash clod dot sh.
So when you run dot slash cloud dot sh
I think...
that does something.
Um...
It calls like a common agent script.
And then the common agent script.
calls Claude with the prompt main file.
So yeah, you have to figure all of this out by looking at the code.
You
I'll see you next time.
Thank you.
I'll see you next time.
I'll see you next time.
Okay, our current tree update reminder.py.
which is meant to send agents updates when new nodes have been added to the tree. That is...
It's not.
What am I trying to say?
It's not good, it's overly complex. All we want to do is look for new files that have been created.
the folder maybe we can just use a set of scene markdown files and a CSV for that might be the easiest
that actually could be really easy.
and then if the file hasn't been seen before.
Oh, actually, we can just run.
Uh...
Yeah, yeah, let's just save state in a CSV. That's the best. So tree update reminder should just send over the
recently added nodes to the markdown folder that haven't been
sent out yeah actually we can just do that that haven't been
Files that have been modified in the last like two minutes
And then we don't even need it.
keep state except for has this been called two minutes ago and then you're only including
Do we do it so that you're only seeing so that whoever calls it only sees files updates they haven't seen before? No, let's just do everything
Oh, fuck. Actually, it's really hard. I know it's a hard problem to solve, but I know what to do.
Thank you.
You
I'll see you next time.
I thought you were going to help mom.
Yeah, where is she?
Okay, if you get a hero.
Can you hear the lights on as well?
Turn the lights on.
Bye.
I'll see you next time.
I'll see you next time.
Okay, now I want an agent to please help me brainstorm ways to test this whole...
So I think we have somewhere made a read-me of the whole agent orchestration system, and we now want to understand.
test it so I don't need to manually test it.
So currently what I manually do is I start a voice tree, I open a terminal or an agent, and then I need to figure out like what hooks it got.
And you figure out what what got injected into its prompt if it looks about right And then when that agent creates a new file
I need to make sure that that new file is the right color, that I actually remembered to add a new file.
So I think what I need...
I need a little to make like a like a testing lab
Um, right. Like where I have like an example set up where, where we actually do call an agent.
So like an end-to-end system test. We do actually call an agent on a dummy task.
and then that agent actually creates a subtask.
And then we see what the subtasks actually look like and we can just view the output full
So we want like an end-to-end system test of like what the user would be doing
You
Thank you.
You
We'll be right back.
I'll see you next time.
I'll see you next time.
You
Knudler, a bit of a slar thing.
Hmm.
No.
Do you need me to?
Mmm.
computer okay I'll look in a minute yeah me too
it's
And I'm on the face train. Mm-hmm.
Here's a distraction.
Mommy, can I have a...
Dessert
Okay, I'm logging into your thing.
Check for send grid
Thank you.
Mm-hmm.
I haven't.
Can you put it on?
Is this something? Look at all my agents work.
What are they working on?
Voice tree.
The black ones are the eight.
Yeah.
I need an adapter for them.
Two two nine three
You
Tell them all that
I'm trying to sign in yeah
I already tried of one account.
Was not working. Do I have that?
Yeah, it's how you send email.
It's a great sport for you.
No, it's the other one.
Where do I need to go?
Maybe I should do it afterwards.
Is that good? Yeah.
I need a two-factor authentication.
Okay, but I'm not getting it
and my computer is running out of bed.
Do you know I always get bills from your bill remains of
Yeah, but I paid it now, right?
Yeah, I assume you always just pay it.
it is it should be automatic today at 1 30 a.m
It's overdue? Yeah.
78.38
Okay, I didn't pay that. Okay, put credit card. I'll give you my credit card. Just do it now. But is it really not a scam message?
But do you usually pay because I never pay?
You want me to pay now?
It's the seventh. Last number is for two.
Why don't oh
I think I just...
at the bottom there's a date 0128
We have to do it.
Two, three, one.
I must be wrong here, critic.
Oh, mom.
You
Thank you.
Thank you.
You
Thank you.
Thank you.
.
Thank you.
Thank you.
Thank you.
All right, testing one.
Retesting.
All right, so today we want to get Voicetree to be able to handle the NoLima benchmark.
um and so the first step is just cloning that repo we let's clone that into
Yeah, for now, let's just clone it into VoiceTree itself so that it makes it easy to work on.
um
All right, so that's going to involve.
going
to the GitHub and pulling it.
Okay, thanks everyone.
Git clone
Nice, that got cloned.
And now we're going to have to sort of.
Investigate.
Um...
the NoLima repository.
Um...
um
So, yeah, we want to investigate the repository.
um research what what's the first thing we need to in research is um we need to figure out what the actual data sets look like so we can have just like a look through an example question on this benchmark and figure out exactly what what that looks like
Oh, you made it! That's it.
Okay, and then what I'm realizing right now, while just using this to test VoiceTree, is that we have a problem with the Python script that traverses.
the dependency graph to inject relevant content.
launch an agent. That is not working right now.
um it oh it doesn't actually do the full traversal it just gets the uh parent know that you're launched from and it's and then it just doesn't actually do a traversal to the dependents
which is a problem.
Thank you.
Thank you.
Thank you. Love that.
Thank you.
Okay.
Okay.
Thank you.
Thank you.
Well, that's right.
I'm going to hide.
It's been a long time for a long time.
Well, it was at 3,000 months.
I didn't go back.
Okay.
You
Oh
No, one month.
Now we're done.
because
We were out of the day.
project and then do it again.
We kind of want to have a...
Thank you.
It's all. It's all. It's all. It's all. It's all.
Right.
No, no, no, no.
Okay. Love you, my darling. Bye.
Let me go on with a movie.
Thank you.
Thank you.
Thank you.
Yeah.
Hello
All right.
Thank you.
What is this?
Thank you.
Yeah.
Thank you.
You
Let's watch.
Thank you.
We'll do it.
Okay. Okay.
Thank you.
Well
I think
Well,
Oh no!
I bought the ticket and it says it's only good for one time.
What
On the ticket? Yeah.
Thank you.
Yeah, but then I'm good to go, sir.
Okay, I'll do that. That's really fair. You just booked a training ticket on the Deutsche Bahn. It's like, you're learning to care it.
It's the second part of the TV. I hope that it's the TV. It's the TV. It's the TV. It's the TV.
Thank you.
Thank you.
Indeed you can't wait on the left pre-exar, it's possible until...
To death.
All right.
all
Thank you.
Perfect.
Thank you.
Thank you.
Okay, what are we working on now?
Um, um,
so
We want to be able to run the benchmark on LongLemur.
And we have an agent working on creating the system for that right now. We're creating a Python file called Get Voice Tree Nodes.
Um...
We're also working on our test lab in parallel with an agent working on the test lab.
Um,
And we have an agent working on.
Max depth and traversal which I think is done we can close that agent
Nice.
Now we see a problem with the test lab, which is the test lab should be testing our actual end-to-end flow where we create an agent from a node on the graph and it gets given automatic instructions.
Um, and so we shouldn't be giving the agent any specific instructions.
because
That's just a bad idea.
We want the test lab to mimic the real world, and the real world should already have sufficient
instructions for the agents to create new nodes. And so if they're not, then that's a real test failure.
Did you already figure out a plot of it? You've made it.
No.
big
Okay, does that make sense?
That's your hat. That's great. Okay.
I also have just one little fix-up we're gonna have to make where we need to make give agents unique names Which are not just their colors because we have limited colors An agent could clash with a color But we don't have limited name
create basically infinite first names.
Um,
And so we currently use
the agent name to for when it gets tree update reminders it has a scene set for each agent name but currently agent names are just colors so we need to have a differentiate between an agent color and an agent
Okay.
Thank you.
Thank you.
You
Are we going to the gym?
You
Oh
Brilliant, just brilliant.
Good app.
When I love.
He went all out.
Oh my God.
Hmm.
Bugged.
Thank you.
Thank you.
Thank you.
All right, coolio. So today I'm working on VoiceTree.
VoiceTree converts voice to text to then a tree, a visual tree, like a mind map.
And we're focusing on the text to tree component of that because we're using that.
um to actually build a better context representation for large language models themselves by representing
the
Okay.
Uh-oh, uh-oh.
And D3.
All right, coolio. So today I'm working on VoiceTree. VoiceTree is the system that I've been working on that converts voice to text to graph or tree, like a visual tree, like a mind map.
And...
Um, I'm focusing right now on the text to tree component.
Um, because that I'm seeing a very cool use.
which is um better representation of large language model context um representing the context you input to an llm as a um tree um and then when you
ask the LLM a question, you can send over only the relevant parts of the tree. So you prune the tree to only what is relevant for a given query. And that solves a big problem in LLMs where as you build a long chat history, you sort of
uh you every request um send that whole chat history each time um which is very inefficient because that chat history has lots of irrelevant context and so we want to filter the context to only what is truly relevant and i think a good way of doing that is represent
as a tree and then doing various
tree searches on that and algorithms to get relevancy. Okay, cool. I've been playing around with various different benchmarks to prove this hypothesis.
So my hypothesis is that representing it as a tree is going to make it the LLM.
faster
have lower input
because we're not studying the whole context.
have greater accuracy because it doesn't suffer from context blur.
And, um,
Yeah, it also lets you in the end sort of have like infinite length chats of LLMs because each time you can just be sending the.
relevant context.
Cool. So the benchmarks that I've been playing around with to prove that hypothesis...
are um
So I started out of GSM.
And now I then went to No Lima.
Um, and now I'm working.
Longbench V2. And so for Longbench V2, I did it on a sample question from Longbench V2 and it did really well. And now I just want it to be able to scale up to larger input.
Lengths, so I think that was like an 8k input question from long bench and now we want to do like a
Uh, perhaps a, uh...
like a 32 length, 32,000 length, and then 64,000 length, 128,000 length, up to probably 256,000 length, or wherever the LLMs really start struggling.
Okay, so for scaling long bench v2, one thing we need to do right now, there's a problem where during voice tree creation, so converting the input context to a tree.
you end up with lots of disconnected components and that's because the current algorithm for VoiceTree doesn't have a way.
Connecting the components.
That is...
One thing I want to address and the way I want to address that is by creating an agent which looks at all the root nodes in a tree for each disconnected component within the tree. So each disconnected tree has a root node, right? One of the nodes of node.
Each sub, each disconnected tree must have one of those. And then for each of those routes, you send them to an LLM and you say, hey, hey LLM.
which of these roots have a common grouping whereby we should connect them through a parent node.
Or perhaps one of the other alternatives is that instead of connecting it through a parent node, they should actually just be connected directly.
Um
So...
that's what we need to create.
And I'm thinking that during tree creation, we do this like every 10, 20 nodes, and we pass in also the relevant chat, like relevant history of the text. So it sort of has some idea of what's going on to the LLM. However, for now, for an MVP, what I want to do.
is I want to
Um...
I want to just do this on an already existing tree. So, for example, in...
In our benchmarker output, we have user guide QA audio.
That is an example.
Um,
of the LongBenchV2 question that I've converted to a voice tree. And there's maybe like 10 or so disconnected components. And so the MVP should be that already built tree with various disconnected subtrees, connecting those subtrees together. For now, we can just not worry about the difference between connecting them directly versus connecting them through a parent node. Let's just always do it with a parent node.
Um...
So you get.
at least two roots from disconnected subtrees.
Um, so you, you give, you give the LM all of it, the.
and the LLM must form groupings of at least two if they are directly related through an obvious parent grouping and it should output the...
the parent grouping name and a summary for that parent.
Um
And as input, it's going to get all the root nodes. And of the root nodes, it's only going to get titles and summaries. We don't need to send through the whole content. That's unnecessary. Although we should also send over a neighborhood around the root node that we're sending. So you're sending a root node, and that root node has various children.
those children have children um so we want to send the neighborhood but let's not do that for the mvp let's save that as a to-do item for um mvpv2 um same for um directly connecting uh disconnected subtrees let's not do that yet just we always form a parent node as the parent
Um, and let's make.
That, also a to-do item for V2.
So just add two comments there.
Okay, cool. So I think that is clear enough.
And then the next steps on top of that is once our tree is well-connected, okay, so yeah, one caveat is that we
be careful to not over encourage the LLM to connect disconnected components. We want to say only if there is an obvious relationship between some of these routes where you can connect them through a parent. We don't want to force unrelated subtrees to then get connected.
just one caveat there all right cool so next step then is we need to implement a vector search so given a tree and given a query we want to
convert each node into a vector embedding and then do a vector embedding search for the query against each node to then get the five most relevant nodes.
And then with those five most relevant nodes, we're going to send over a.
to the LLM, a dependency traversal from the root of our tree to the relevant node.
um accumulating the context as we go um so we'll go from course to fine so we'll start with um
uh just the titles of the node as we um traverse and then as we start getting closer like let's say
seven hops away, 10 hops away, we start including summaries. And then as we start getting three hops away, we start including...
full node content.
And then we should also be doing this traversal for traversal v2. So that's MVP traversal. Traversal v2 will also start including the neighborhood around the traversal.
to the LLM and also like the children after the
target node that's relevant so you sort of continue the traversal a bit extra to the children we already have very very importantly we've we already have code to do dependency traversal in the tree but we'll want to clean it up and extract the dependency traversal into its own file to make sure that we're modular and
and reusable and testable and all those good stuff.
We will also want
And then for a vector search and vector embeddings, I think we have some example code somewhere in the code base. But important there is that we want to use Gemini embeddings, and there's a very clear, short, concise example on the Google Docs. It makes it very easy to create the embedding and then do a for loop over all nodes to do.
All right, and now there are some other fix-ups that I want to do in general outside of this main task. So let's just add like a won't do node where we...
that content. So for example, hierarchical layout for VoiceTree, I want to encourage hierarchical layout, a hierarchical color layout. Currently you have to do sort of do it manually.
I think we can do much better.
Like right now I'm doing it manually doing the hierarchical layout, but it'd be nice for just
for it to audit it.
Um...
Okay
Let's now start launching our agents.
So...
All right, and we need to fix one problem that's blocking us right now before we start working on this which is that For our agent
Um, orchestration, um,
relevant context for some reason is broken so you can see that it's not getting any relevant context
But I think it's just we've made some changes with environment variables.
names and agent colors. So it definitely just has something to do with that that we could quickly fix up. So let's fix that first.
Okay, so while that's working
I don't really want to launch the agents until that problem is fixed.
Oh, another won't do task is that currently the agents have like Z score a thousand. So they overlay on top of everything. That's really bad. We want to remove that manual hack. We just want them to overlay as per usual.
as Hover editors.
um so that other other hover editors can appear on top of them because right now everything appears below them um so
might actually that might be a really easy fix um it should just be in
obsidian um sorry in the in juggle juggle main um in our repo for um
i think it's called agent canvas or voice tree ui um where we we just set the z score for terminal hover editors in the juggle in the juggle repo um important
Cool that is running
Nice. This guy fixed the context problem so we can get rid of him.
Some other fix-ups that I would really like is to be able to just quickly close, when you close the editor for terminal for it to just...
remove the node as well for the terminal. And also I really wanna be able to collapse and expand nodes so I don't see everything all at once.
Um,
And that can also provide hints to the LLM for what context might be relevant or irrelevant currently.
Cool. All righty. Now we should be able to run Claude on our agent LLM connection mechanism.
Cool, and now we have...
so we now work on implementing vector search we'll launch an agent there
Okay, now I'm thinking...
So right now I have
Um...
I have...
Um, so I would like to figure out a way. Um, so first I want to make dependency traversal for relevant nodes. I want to make sure that, um, in the current setup we have, that it does go through all children as well as all parents.
And the reason why is because I what I wanted to do was I wanted to make a So for our agent orchestration system, you often want an orchestrator agent which manages sub agents
Um, but often a workflow is you create the sub agents manually yourself. Um, but then later on you want an or.
under all their work.
For that reason, I would probably launch an agent on a root node, which has lots of children, and those children contain the agents. And I'd want the...
Uh...
the
So if I launch an agent on a root node which has children with agents, I want it to get injected into its context all the child node relevancy, which just means that dependency traversal has to do with relevant nodes.
Fine, not a problem.
So, and then the next step on top of that is actually.
agent orchestration system properly aware of what the agents are doing and the way to do that is it can just look for colors or agent names in the in the markdown nodes right very easy so it gets first dependency traversal of
the all the human notes or human and agent um and then it gets uh specifically um hey here are all the different agents working under your um in your relevant content
and your task is to manage them.
All right, so the main thing I actually want the dependency traversal agent to currently work on is...
making the
Making the current dependency traversal file much better organized, much less tech debt, breaking it into the components so that I can then test each individual component and reuse components. So I want to have essentially my dependency traversal function very robust because I know I'm going to have to rely on it a lot. So what I'm going to want it to do is I want it to first write me a proposal document of...
what changes it can make to improve the quality of the existing dependency traversal. None of the logic, just the code quality and organization and tech debt.
Okay, so at implement vector search, I want to really specify that it should just be using Gemini embed.
So, because it's got really simple code there, and I think right now it's using some random Python library. So I'll include the docs here. Is there a way to just...
Um,
so
I'm just gonna copy
All of this as well.
Um, and the...
Okay, so let's.
Edit that.
nice
Cool. Now I'm telling the agent to make sure to use Gemini.
Yeah, what is sentence transfer?
Interesting.
maybe we could in the future um yeah we should ask the LM we should ask the agent as well um whichever agent is the
to tell us like is it actually better to use Gemini versus the other library. The other library being...
Sentence Transformers, which I think is something to do with SBIRT. So we should ask it as well, like what it thinks is actually better. Cool.
Um, okay. And so for the simplified traversal proposal for dependency traversal code refactor.
The simplified proposal the agent has done. It looks pretty good.
One thing I'm thinking here is, so I like that it's just a single module.
Um, one thing is, is that, um, uh,
load load node that should actually be so in voice tree back-end text to graph pipeline tree manager we have markdown to tree and tree to markdown so load node
should be part of the markdown to tree module.
And I think we should actually extract markdown to tree out of the text to graph pipeline, out of tree manager. And the reason for that is that we're going to be using it for both tree creation and context retrieval. So for pruning the tree to give LLM just a relevant context.
So I think what we should have in back.
have text to graph pipeline and we should also have
Context Retrieval.
And then the markdown to tree becomes a third module there.
since both are going to be using.
Does that make sense?
it's a really interesting bug here where um claude decided
Yes, I'm going to make this into a subtask, but I'm actually just gonna do it myself and pretend that it's a subtask by faking the colors. Document progress of AdiNodePi using orange color. Instead of just letting me...
the subtask, so I think I need to explain to it.
Um,
I mean, it should be clear from the prompt already that it doesn't actually, it shouldn't actually be running the agents.
But, oh well, it doesn't usually do that.
Okay, now we are launching the sub-agents.
That's right.
So right now we've...
The proposed system architecture. I'm just wondering do we want it in back-end or tools?
That's fun, that's fun.
having it in back-end or tools.
Okay, so the bug with agents trying to launch their own sub-agents is being reproduced. So something must be different now. So let's just investigate that.
Nice, vector search apparently is done.
Um...
Yeah, we can get Gemini to review a little bit.
Okay, so I'm working on the bug for
the fact that we're automatically spawning sub-agents from the parent agent. We shouldn't be doing that, we should be loading users manually, spawn them, although in the future we...
We actually do want automatic spawning, but I'm just gonna work on that bug right now.
So create subagent commit.
Line 104.
We can just tack that on for now, we just added to the prompt.
to not spawn the subtasks.
Which I think is...
Good genug
Another little minor
is that the...
The first node that's made by an agent has their agent name underscored, but future nodes created with agents.
don't have their um
agent name appended so that would be good
Thank you.
Cool so we've launched an agent to work on that. Do you understand what the problem is?
Okay, cool already found add new node.py, which is perfect. So let's just get it to Tell us if it understands the full problem and then we're going to want to get the agent to just make sure that the Nodes created through add new node.py have the agent
pre-pended to the file.
I think we need to be a bit more specific, so I want...
Nodes made by agents
You always.
Have agent name ended to file name.
However.
I'm trying to look at some of the code that's been made.
Okay, so in tree action decider workflow, I'm seeing that it actually added code to do it every 15 nodes but remember This is actually fine. We'll just leave the code the code looks fine But I did say that we wanted to leave this for version 2 not the MVP. So please explain why that happened
Embedding search looks fine.
Hmm.
So in tree functions, we're now using embeddings for our semantic search.
Um...
However,
There's going to be a lot of complexity there because we don't yet actually Create the embeddings while we're doing tree creation. So this is again something I said
for version two um because
For now, we only actually want to use the embeddings for...
uh, context retrieval, not for tree creation. Okay. Very important.
Okay, let's look at add new node.py
Add new node.py.
Looks fine.
What is it doing?
We might need to...
Okay, cool.
Cool, now I'm looking through the code it wrote for
um so now we split into context retrieval and mark down to tree so i'm just reviewing that
Um...
Ah, it's actually really neat. It's really good code.
Mmm.
Now I'm reviewing the Connect Orphan Nodes prompt.
All right, so it's a common problem with the connect orphan prompts, which is it's asking for an array of node IDs to
However, LLMs don't actually work well with NodeID.
We instead want to give it node titles.
Um,
and then we can
I want to return both a node ID and
so this is how we currently do it in all our prompts you return a node ID and a node title and then if the node ID doesn't exist we can just fall back to fuzzy finding the node ID based on
Um,
And the other thing which is directly related to that is that you don't actually need to include the output format because if it's in state.py, if it's in a pedantic model, it gets injected into the prompt automatically. So we don't actually need that.
Okay, now when you figure out which agent changed tree functions to use embedding-based search...
um so we don't want that so where was that
Oh, yep, here we are.
Um,
So we do actually want that code somewhere, right? So we want it in...
So it's going to be in.
Context retrieval? No.
It's going to be, oh yeah, we'll be in.
retrieval, right? Because context retrieval...
So we have markdown a tree, text graph pipeline, context retrieval.
Let's add a um
Uhhh...
Boo, boo, boo, boo.
Yeah, let's add it to context.
Thank you.
So just want to make sure we have end-to-end tests for what are we even doing here? So we are doing
Agent LLM Connect
Oh, yeah, yeah, connecting subtrees.
And we want to make sure it's tested
So we're approaching opus usage limit, so we should start making sure that some of the subagents run on.
and then start optimizing the plot itself so we don't run into that problem as often.
Nice, um, title.
naming conventions working
Um...
Just an agent to work on the little fix-ups that Iris mentioned.
Cool that looks like
We are done.
Pretty amazing, huh?
So there you see it gets hooks about all new files and the LM itself can decide if they're relevant or not.
Um...
And.
So that was content filtering done.
Um...
Okay, so.
okay yeah so i should have also explained so um uh there's always we keep on getting this problem with um them wanting to have like parent relationship patterns like specific um relationship words for parents um we don't want that so this is for um when looking for children or parents in um dependency traversals um so parents are always just the links in a markdown note so you have like um square bracket square bracket uh parent and then um
That will link to a parent and the node that linked to the parent
child and the parent is a parent so for a node X all children of X are nodes Y which have a link to X does that make sense
So I would send that to the orchestrator as well but
Um...
I will tell it.
Uh...
So.
So here I'm getting them to communicate.
the tree.
Nice so vector search looks like it's done
Um,
So it says it ran on...
Um,
So Grace says she ran on
the actual
input
Uh, but I don't quite believe her.
All right, cool. So I'm listening now to, um...
I think it's Grace explain why when they were.
end test it actually didn't connect any orphans and that's because we're only including the root nodes so as we discussed in version 2 of this for connecting disconnected subtrees we're going to have to include not just the route but the neighborhood around the route
Um, so that the root sort of acts as, um,
a summarization of the whole subtree.
So we actually don't want to send it roots. We want to send.
summary of that sub-tree, disconnected sub-tree. So I'm going to tell Grace that and let's get it to save that understanding to the tree and its context only has 13% context left so we'll have to be careful here.
Oh, and then another problem here would be as well that we're not actually giving the LLM enough context.
So we're just giving it these nodes, but we're not giving it like comprehensive overall context of the input text.
um which we definitely should be doing as well um so almost like we need like uh like summary nodes of like just like overall cohesive
of a text. So maybe that's like every 10 nodes as we're creating
Summaries
Um, we want.
So every 10 nodes as we're creating, so every 10 nodes, we want to then connect disconnect.
subtrees. And if it understands, if it gets also given the transcript history for the during the time those ten nodes were created, it has the context to understand the bigger picture.
And then I think we're sorted. So that will fix it as well. So include that in your proposal as well, Grace.
Awesome, I think that task is done.
Um,
I think.
Let's see here.
Grace wrote.
Let's pin that
Um...
Cool
We are done with this voice tree. Let's do the final git commits.
Um...
Might have some fix-ups to do on top of this, but as a first draft, we are chilling.
Battery's going away anyway, so...
So I'm doing my review now, depends each version, so we don't want...
Okay, that's fine. What did I do?
Why is um
Why is markdown a tree?
So mark down a tree, all the code is in...
the init.py that seems kind of strange let's see
so
Yeah, so.
In markdown a tree, all the code is in the underscore underscore init, which just seems like bad practice.
Um,
Let's just get an agent to quickly fix it up.
Let's comment out.
phase three connect orphans
I mean, actually, we don't need a comment now. We can just see if it works, and if not...
I'll just add a two to a comment.
Um, so
So I think, yeah, the idea here is that this is orphan connection.
should be
sub tree connection next where we don't want to connect orphans we want to connect whole subtrees
Okay
It's just a move.
Okay, now this file.
uh graph dependency traversal in a community graph content should no longer be necessary
So let's see.
So yeah, we want to keep that file, but redirect its usage, its actual logic to using our new modules.
Um...
These are just fix-ups.
Okay, let's quickly check the...
uh tests
These tests actually look genuinely pretty okay
Um...
So let's just commit.
I can review that later.
Um,
Ah, we have some Unversion 5.
Okay.
So now we have markdown to tree extraction done.
done um let's just see so that would be um marked down to tree ah so we still have marked down a tree
So
We don't want to mark down a tree anymore.
because we have the Moktano Tree module.
um so let's see what
worked on a tree
Um,
So that's actually gonna have to be a whole agent to we're gonna want to launch in a whole agent to do that but
My computer's about to run out of battery.
Um, so let's just.
I'll add that as a comment.
Um,
And I think
We are at a good place to pause.
We've done some really cool stuff.
Um,
Everything's starting to lag now a little bit because of how huge everything's getting.
But how many agents do we work on here?
1, 2, 3, 4, 5, 6, 7, 8, 9, 10 at least. Not including the ones I've removed. This is pretty epic. This is a voice tree. How awesome is that?
Thank you. Bye.
Thank you.
myself one second i'll be back in a minute
Yeah.
Thank you.
I've never read such a thing that changed out books in my book.
Thank you very much.
We don't seem to do what?
Thank you.
No. Wonderful.
What we have.
I'll bring you more Linda. Look at this beautiful book. My favorite book.
you
That's nice.
Nope.
Thank you.
yeah okay so just like yeah tell me about the overall essay you're writing the project
Starting from the big picture.
big picture like my research question being the big
Yeah, the question and then how you're...
Just like everything you're thinking about.
And then see what happens here.
You
Thank you.
Okay. Okay. Okay. So the essay's about the challenges that regional governance structures face in preventing the mobilization and mitigating the impact of state-sponsored militia groups, focusing on M23 in the Democratic Republic of Congo.
i'm going to be focusing with the conceptual framework which is a literature review looking into the theory behind state sponsored militia group and regional governance and the theories behind that i need to do all my work on that still
Wait, hang on.
So what does that really mean?
What does that mean? So regional governance, it's like things like African Union and other bodies. Yeah. And then looking at their role and how they're supposed to prevent state-sponsored militia groups. Yeah.
existing okay because they're currently doing a really bad job okay and then you start with a conceptual framework a literature review which is just like the background okay and then the theories
that stuff security government state fertility and regional government regional security complexes what are those three things three theories that are those the three main theories yeah i actually could not tell you i don't know anything on the theory part okay um the theories are very small
But then I'll have to do my methodology, which is breaking down how I will do my case study and explore my case study. And so that will kind of, I don't know, explain to them.
Rita.
why I'm doing what I'm doing.
Thank you.
And so yeah then I'll be doing
23, who is
A state-sponsored militia group backed by Rwanda.
Even though Rwanda says they're not.
Um...
This all dates back to the Rwandan genocide, because the...
who tos and the tootsies like drc and rwanda are like
Yeah. It all dates back to that.
So what do you want to talk about M23?
So they're like seizing territory and...
fighting against the democratic republic congo but within the democratic
See y'all see.
Like capturing land and they're basically just like a terrorist organization
Mmm.
And so the first part of your essay would be why is it M23 doing that? Why are they fighting? Be it like that's like background.
like why are they fighting the main part of my
Like, so this is all background.
Um, yeah.
So, the main part of our essay will be focusing on the African Union and the...
uh east african community and
to do to stop the conflict.
It will be focusing on how they failed to really stop it and how Qatar and the U.S.
Yeah.
Mmm, so are you gonna have like a main argument?
I mean, given is that the current.
like
They're failing the regional government structure
to do with this.
Hmm, and are you going to propose a different
or it's more about critiquing?
More about cruelty.
Yeah, I don't have space.
structure. It's more just trying to find where their failures are and why they're failing to.
Like why they're failing
Okay
and and and so then you have some core reasons for why they're failing yeah
Currently, this is all just...
And so what are the core reasons?
I haven't gone yet.
So we can have a look here
So this is the last thing we talked about.
Wait, this is so cool.
So the idea here is that, like, you have, like, the core concepts of what you've been discussing, right, like, M23 and regional governance failure, question, main argument exist.
Oh, that was me asking him.
um and then there there's you're welcome um like that's a very early date
the idea here now is that like you can look at this and think well what do i actually want to think about right now maybe i want to think more about this and you start talking about it
That's actually so cool. Maybe as you're researching it or...
You wanna try a little bit?
You can start brainstorming a bit.
Are you showing, like, why there's failure?
What do you think is most important here? Is it this one?
Mm-hmm.
Okay, so core reasons for regional governance failure.
You want to like.
That's where the next step is to figure out what they are and how are you going to approach that? Research.
What are you going to research?
current reasons why peace negotiations have broken down the history of the regional governance structures and linking back to how they were set up because how they were formed influences why they are failing and kind of their lack of
interventionary power, which leads to then things like the UN having to get involved with peacekeepers.
So it sounds like you have a pretty good intuition for what to research and, like, what the main problems are. So should we expand on this? Like, what does that actually entail? Or, like, what's the next steps from your research plan?
So next step would be looking at journal articles.
which are good for historical information, but for current things I'm relying mostly on news sources to find information around current failings, but the failings are mostly historic and systemic, so...
hopefully relying on journal articles.
other literature books published and how will you prioritize your research review like what are you going to be your first search searches for in the research bank and like how are you going to because i'm sure it's a fair bit to read
How are you going to prioritize?
So currently I'm just looking into backgrounds of these regional governance bodies, like how their structures were originally formed, because I feel like you need to understand how they're formed.
do anything more so understanding the history in their formation and then maybe looking into
things that are critical of them or pointing out flaws and areas that they're maybe failing.
Thank you.
Jesus.
Okay, so you want to research.
governance body criticisms to the existing criticism.
And do you think that's going to match what your intuition is for the governance failure reasons?
Do you want me to expand on why, but... Well, yeah.
I don't know. What do you think is important to expand on here?
Yeah.
Let's see it all.
I don't know that kind of feels like
I don't know, I have still so much more.
i'm not very good at expressing what's in my head so now let me show you what's cool do this
And then you go here.
And then you say.
So.
What do you actually want to do? Generate the research?
All right.
Whoa.
It's so cool.
We're actually
Just a little bug in fact that
Are you telling me you don't have flawless code?
Yeah.
Trifix
Yeah, so here's your Leo did a research plan.
Whoa.
It's a bit like tech oriented. That's why you have like these diagrams.
Yeah, that's so cool, but now what you do is
This is... I wouldn't be doing this.
Thank you.
Here.
And then you do
Mm-hmm.
Let's do this.
I don't want this to work, but...
I'll do it manually. Normally you wouldn't have to do this, but...
AI you pay. Yeah.
You'll get the point for it
Vague research requests needing therapy.
The idea is that I should just be able to right-click and it will give me everything relevant here. Yeah. But... That's pretty cool.
I do it a bit better
Wait, why are some of them green?
that's just when they're new okay and if you can just like it's like a notification yeah and they're blue if they just got
This one else is working with bugs right now.
Hello, testing one, two, three. Hello, testing one, two, three.
Hello, testing one, two.
All right, so let's see if we create a testing node.
Hello, testing 1, 2, 3. Hello, testing 1, 2, 3. Hello, testing 1, 2, 3. Alright, so let's see if we create a testing node.
And if we create a test node, where does it appear?
And if we create a test node, where does it appear?
it's the main things i want to know now so let me say
This man things don't know and also so many sick
Wait, wait, wait, wait, wait, wait.
Hmm
Hmm
Oh, my aunt.
My wife's so mean to me
Bye-bye.
One and one.
No, I got that running down.
I would be having a lot of room.
Go, under these.
You're very good friends.
All right.
That's a video.
Yeah.
How does it work here for us?
Good.
Thank you.
Ha, ha, ha.
Okay, tell me more of them, and here we go.
Let's go. Let's go. Let's go. Let's go. Let's go.
Oh fuck, now we have a dumb truck.
Thank you.
Thank you.
Thank you.
Thank you.
Okay, so we're working right now on the context retrieval algorithm
Okay, so we're working right now on the context retrieval algorithm.
So that takes as input
So that takes as input...
a concept tree representing an existing context.
a concept tree representing an existing context.
And...
And...
A query.
A query.
and okay so we got two fucking hell we got two running at once right now
And, okay, so we've got two, fucking hell, we've got two running at once right now.
Oh my god.
Oh my god.
Okay, let's activate it.
Okay, let's activate it.
All right.
All right, cool, so today I am working on
um context pruning and context retrieval um so
The idea is, is that we want to optimize LLM context.
through removing irrelevant.
content from the context
Um, cool. Okay.
and
The current idea we have is, so we have voice tree which converts
text or voice into a concept graph.
And right now our idea is given that graph and given a user query, we can prune the graph to only what is relevant.
Or, alternatively, we can...
Um...
yeah basically that um and then that is the context for the llm
All right, and we have version zero of
pruning algorithm.
which involves a few steps, involves creating vector embeddings for each node in the tree, and then does a search against the query for the most relevant nodes with vector similarity search.
And then...
does a traversal
from the roots of the tree to those most relevant nodes, plus their children.
Um...
and includes all that context.
Um, and right now...
We want to sort of just...
Create a version of the algorithm.
That is the idea we have with the relationships working with the full content.
Right now we have some hacks to get that working but it's not quite right. So we want to create a generally more correct version.
So the current version is in backend slash context retrieval.
and also uses the embeddings underscore output folder.
Um...
and it was traverse underscore all underscore relevant underscore nodes dot pi.
that we were using but that there you had to specify
the relevant nodes manually, it was a bit hacky. So I think first step is to create a script that sort of puts this all together.
So we have, let's assume for this version that we have the embeddings outputs already generated. So we have that ready.
But then we need to do a search against that, against the query to find the most relevant node.
Um...
And...
So what do we want that to specifically do? So.
I think we can use the...
second half of Traverse All Relevant Nodes.
but what we want it to do is we want it to be given a query.
Um...
Do the search as well So we have vector search dot PI
Um...
And we have traversal relevant nodes.
So we just don't want to manually specify the relevant nodes.
search-based, so only input is the query and the tree.
so the embeddings are in that
output folder I think the
The vectors file might still already include the...
weary.
but we can just ignore that.
for now.
So like, you know, when you do a vector simulation,
query is going to show up because it's in the um tsv so it would be like 100 similarity so we just want to ignore that
Um...
Later on, we'll have to fix up the dependency traversal script. I think apply content filtering is a bit fucked up.
Um...
There's just multiple layers of indirection which is completely unnecessary It's just been made overly complex
applying content filtering
um
has like this casey's filter which i think
doesn't actually, shouldn't be named that.
Like we should just do apply content filter and then...
Um, we don't need a whole, uh,
We don't need the apply content.
Well, we still want that, but we don't need the...
We just don't need both that method and the Casey filter method. We can just merge those into one.
Thank you.
Thank you.
Mm-hmm.
Thank you.
Mm-hmm.
She is hungry.
Thank you.
Yep.
Thank you.
All right.
Thank you.
Cool.
Thank you.
No
Thank you.
Thank you.
Thank you.
Thank you.
Hey Shane, this is Manu.
I'm hoping to, I want to start doing hair loss treatment.
Uh, this is the hair loss treatment clinic, right?
Yes. Yeah, I'm hoping to have an appointment.
or start looking into hair loss treatment.
Okay, Thursday or Friday would be ideal.
Um, afternoon.
and anytime after 11 a.m.
Should be good
On Thursday, yeah, 2 p.m.
Manu. M-A-N-U.
Uh, probably by car. Is it in Redfern?
Cool, just on street parking
Yeah, sweet.
Great, thank you.
Sweet, thank you.
All right, so what I'm working on
is let's get our context first so what are we working on
So, I think...
The state we were in before was we had improved our pruning algorithm.
And now we want to...
Um...
do an accuracy score so in back-end context retrieval qa test
We now have...
four questions.
And what we wanted to do was set up just a simple script to run our traversal on each of those four questions and run it 10 times.
And...
get the accuracy score so out of those 10 executions how many times do you get the correct answer
Um, and we also want.
to
um also run the just the default option which is just running the um without our context pruning um just doing the og mode
Uh, hmm.
It's like you just paste the original context.
Um...
okay now another thing that we just need to quickly fix up is we're getting an error during um orphan connection
Um,
That would be nice to see.
Okay, so let's paste that error in.
And we know where the error is going to be.
It's going to be in...
Tree manager.
All right, so with the accuracy score, so this is related to the long bend.
long contact benchmark.
And they do have a way of...
Queering the LLM
Um, so...
We could maybe try to use their code, so that's pred.py.
Um, which like read us like certain problems that we know are going to come up like, um,
predicting i mean sorry extracting the answer so if the lm says the answer is c um we need to extract that and so that code already exists um pred.py
So that could be good to use.
Um...
okay yeah I mean we may as well use
should actually really use the benchmarks.
function because that
um then we're using the benchmarks if we want to publish our results we want to be using the um the open source runner um and then so in um context retrieval
Um,
So I think there's a, what is it?
Uh...
So traverse all relevant nodes currently has the query hard-coded, and so instead it's going to have to be given an input query, and then we need the driver to run it on each question in the QA test questions, and do it 10 times. Once with the context retrieval algorithm that we're building, and once with...
with just the full output without it being pruned.
Um, and
That should then work
Okay, let's try now.
Mm-hmm.
Okay, now we do want to do some general fix-ups to VoiceTree as well to enhance.
ability I'm realizing that's very important the ability to restart voice tree voice to text to tree and have it
Um,
correctly resume from where you left off.
Oh, yeah.
The other thing I want to do is I want to make start making
VoiceTree algorithm start using actually
vector embeddings plus keyword search to find relevant nodes.
Okay, one feedback is more me
flash, did you have a 2.5 flash light?
not Gemini 2.5, not Gemini 2.9, Gemini 2.5 is the one we want to use.
Um...
And then I'm trying to see, it doesn't look like it's using pred.py.
Let's see why it's doing that. It looks like you would have it. Use pred.py.
Thank you.
You
Buh.
Mm-hmm.
Okay, okay, okay, fine.
what i've done here is i've just launched voice tree it's currently a terminal app because that's that makes it easy to develop quickly on yeah um actually i'm not so the it does voice to text to tree that's what voice three is and i'm not sure how well it's going to
um because there's a lot of background noise but we'll see so
Yep.
So it's essentially now it's like there's now a node for voice stream right and it has
essentially what I talked about.
Um, and now the connection to that node is like, so now I'm going to.
Like we want to start talking about...
like a bit more about VoiceTree, about what it does and what the use cases are and so
This, originally, and it's still how I'm thinking, is framed for software engineering.
So as you're working, you can talk about what you're doing.
and all the different tasks you have to do.
like in your day-to-day self-engineering and you know because there's just like so much random stuff we do so much like context and so this can help you like decide what to work on and problem solve
And as you're talking about problems, you'll start to see the tree emerge and reflect the way you've been thinking about the problem.
So here you can select the use cases.
Um, and
it's yes and then there's software doing this so like
it is a little bit slow right now like you can see there was a bit of a lag
But it's like, you know, if you're not hyper concerned about the latency, it's you look back 10 seconds later and it's more or less where you're.
working from.
Cool. And so you can see sort of like the branch of what we've been talking about. It's like I mentioned like the performance concerns in the cafe, right? And so that metanode there. And then we started talking about like software engineering and thought visualization. And it's...
As I'm talking, it's reflecting what I'm saying, which is a really nice way to work.
If you're explaining everything you're working on.
um you could also imagine like it'd be useful for meetings right now like right like if we're talking about the business and the different things um that we can do and the different options and if it all appears as a tree it's pretty easy for you to point at one thing and be like oh what about that let's talk about that yeah it's like it provides like a a um a visual
It's a visual canvas for mind mapping and thought exploration.
Cool. And now the main feature right now.
is that you can launch an agent.
Oh, wow.
Here I hover over it
and hit enter and then Claude opens up.
quad code the agent and it gets like the relevant context from the tree so you can see here
Thank you.
Like it gets like all the content in the dream
So it has some understanding already of what's going on.
Um...
And so then I can tell it to do something like, wait. Like if the task is clear, I would just start doing it.
But here is a bit unfair, so I'm just going to say wait for the...
you are made for something
Hey
And green, when the nodes are called green, it means like there's a
So, they already talked a little bit about meetings, right?
And then here we have the agent feature. Blue means there's new.
Um...
like when you're talking if there's an existing node that's relevant it'll just append it to that instead of making a new node
um
and so these the agents like draggable
and so they stay in the space so you can imagine like i'm working on
I'm going to launch an agent and say...
Um, let's say like.
Um...
explore ways to
is check the performance.
Thank you very much.
It's very great.
And so then this guy finished.
and he now sends over like a little diagram right like he just he did a task essentially yes and like obviously this is like a dummy example
can imagine if you're actually working on something you just get it to do it right like it it has the context of your work um you can zoom out and stuff and the idea is like you know
Slowly this grows and grows and grows and represents your whole work context.
So then you never have to like reprobate.
all the context is already there. If you start a new, new task.
It's like a contact screen.
Yeah, yeah. Chronix cache. And it can, it can, but it can extend. So right now it's like a cache as in like a short term memory.
But it can also be extended to be like medium term, long term memory.
With some extra features.
one thing i really need to build is if you zoom out it needs to sort of collapse yes right so that if you have a hundred things going on yeah it collapses into like um the clusters
Yeah, yeah, yeah, I see what you mean, like, that would be essentially a case on top of that.
I like when I say Keisha I just mean
a conceptual summary as opposed to like a short living.
This is super cool. The more you talk about it, the more I'm like, my mind is just going in so many different directions to how you speak this.
and that's honestly the problem with that which is like this core technology is pretty dope yeah i mean it has so many use cases now
um and okay the other cool thing is that this the like one of the main things that built right is just the API to convert voice to text the tree to concept tree yes and that API uh
can be used.
to, like, um...
can be used for context printing so like given this tree like you could represent a whole like conversation in LLM as a tree and then you can like discard some branches of the trees that are irrelevant Like maybe this section here Like maybe this branch here of performance concerns. Yeah, maybe I'm asking you something about this and so it doesn't need to include that in context Yeah, and that's why I'm seeing
on long bench, benchmark for long context. You can represent the context as a tree and then only select relevant branches.
Oh, okay.
I feel like there's so many ways that you can go about doing that though. It's like, like algorithm wise.
Yeah.
this is off the top of my head like the context reading sounds like the best option right because you reduce your search your search points
Fuck life.
Like, can you consider, like, I'm sure you have a question, like, did you do anything into, like,
how
like I don't know anything I don't know much about LLM's but
Is it possible to have, like, cold storage?
Right, so like when you set up pruning you put the the less relevant concepts into old storage So the element makes slightly longer to go find those concepts, they can't find them under graphs, but those concepts haven't disappeared
Yeah, it's an interesting idea, like L1, L2.
yeah so like
The thing is, it's like, so all the files are here, right? We're talking about, it's just like markdown notes.
You can just open one and like edit it, for example.
And because storage is so cheap, you can actually include all of these.
And then you have like nice algorithms, which let you scale, right? Like as in.
You don't? You can scale this to hundreds of thousands of nodes with vector embeddings and things like that.
I'll show you now, that's the demo.
I think a few extra features like a I feel like one little thing
Here, like, stop hook, it gets new file detected.
So it tells you.
It says there's a new node.
and it says um so this gets the agent gets told in your notes
and it gets it has to say like is it if they're relevant to your work
Oh, I need to fix that. It's meant to be the opposite of that.
Anyway, all right, let's.
Wait, sorry, what was it? So, like, when there's a new node appears, like, if I keep talking, the agent will get alerted of what I've said, like, without me having to type in here. And it will get alerted of what other agents have done as well. Because it gets, like, messages sent of new nodes.
Thank you very much.
Let's do it.
Okay.
Thank you.
This has become corrupted.
Thank you.
Okay.
So the vision is is like it's a whole this becomes then a whole voice tree becomes a whole platform
for managing agent.
so you have at the top this canvas interface which is um like this yeah
Um,
And then below that is the context tree. Yeah.
right which is like the data data storage format which as as markdown notes
And then under that, you have like the semantic retrieval, like smart rag, basically like rag plus plus, like a rag that can take into account the meaning because the meaning is encoded within the tree. The tree is like a concept tree and it contains like how each concept relates to other concepts. So you can do like a much smarter search.
and then you both
Thank you.
Both agents and humans.
can interact with the same tree like it's it's shared right like as in You know, I can edit the tree editing now
Right. And so kind of agents and agents can like add here and then if I like, you know, did, right. Like.
Yeah, thanks.
And so that's like, this is like the ecosystem for man.
um under the assumption that like
um you know uh ai is gonna get better and better but at some point you're still gonna have to have human feedback in the loop yeah uh or direction human direction and it's gonna the level of abstraction is going to grow and grow and grow like um humans are going to take a higher and higher level approach but my assumption is is that having a human in the loop
is going to almost always be useful and massively, like as in you could build a fully automated agent system, but that's going to require like 10 times the dev effort and require like, you know, some sort of like a closed environment. But if you keep it slightly open and it's, you can have humans like managing it as well and providing feedback, like 10 agents at once.
then you can build things way way faster. It's like human orchestration. Yeah.
And
Um, and then there's, um,
uh use cases on top of that yeah
So, like, for software engineering, I think is the first, like, use case. Yep.
yeah um and then things like question answering
because of the semantic retrieval can be like really effective.
Um, and that's like the dotted room could come in like the dotted room could, um,
Yeah, be a product sitting on top of this whole platform. Yeah.
And one other really cool thing,
You could, in 30 seconds, describe an AI workflow with your voice.
And then you could mention like, oh, yes, step one is checking the benchmark data. Step two is comparing it.
to the upper version and then understanding where it went wrong and then looking at the code vets to figure out where the bug is. You could like describe an agent works well.
And then it would come as cross nodes, and then you could just run an agent on.
And so now you've like
you can run your workflows within this ecosystem and build them within like 30 seconds. But then keep them well maintained because you can edit them and build on top of them.
lots of cool like use cases and products um but all sitting on top of this uh
agent management
That's the that's the vision
That's it!
That's a lot to think about.
Yeah, yeah. I mean, yeah, this is like two years in the works for me.
Like I've been,
This has been going on for ages. At least a year and a half. And this is my current thinking, but it's always changing.
But I do keep on gravitating towards this sort of like platform-ish system with software engineering as the initial wedge or like target.
and
There is, yeah, I might like to be flopping, I have pink. I'll say, feel free to order me out. Mine, grab something if you want.
You know what I did right now to like help me get my thoughts down
Thank you.
first the um initial product um and
That's what you can see here, it's called Voice Tree.
And what it does is it converts what you say. So it converts your voice. So it's transcribing my voice right now. Oh yeah. And then it converts it into a tree. Essentially like a visual mind map.
And so...
Quite soon, you'll see, like it'll pop up in a second. Oh, wait, sorry, I wasn't clicking the white button.
Thank you.
all right yeah so it'll then like start creating a mind map what you're saying
um and so here it has a
like a node like a concept node of like the what did it like the initial product called VoiceTree and then if I keep on talking like so I might start talking about the use cases for this and so one of the main use cases I want to start off with is for software engineering
um
or you can describe the work you need to do.
and it will like for uh it'll form a tree like representing your thoughts um which is useful for like uh problem solving and decision
because you can see the exact puffs right in front of me.
So there's a little bit of a lag right now, right?
Maybe 10, 15 second lag.
But as you keep on talking it slowly starts building this this tree here
Oh, wow.
and then
What you can do from the tree.
is you can launch agents.
like AI agents.
so here
this has um
Like here I might go the voice tree system
And I'm going to open an agent.
And this is now like an agent here.
And I can tell it to do something like...
Like, just as I said,
What?
Amen.
And then this agent is going to...
it gets like all the context from the tree already given to it so it knows like
It knows what's going on.
I don't have to, like, re-explain anything.
There are.
And it'll do its work. And then...
it'll add back to the tree which you'll see in a second
Um,
Thank you.
So like here the agent itself
Um, added back to the.
Add it back to the tree, like suggestions on how we can make it faster, reduce lag from 10 to 15 seconds.
And so you're doing work, you can describe the work, you can get agents to like automatically start working on it And you can have multiple agents like all over and so
that's like the current product that exists um and the vision
Thank you.
is that it becomes like a whole...
whole management system for ai agents yeah so you have that canvas interface um which sits on top of this like context tree which contains all your data uh and like memories or context
And beneath that, you have...
Semantic retrieval, which means that.
like when i launched the agent it was given like all the relevant context from the tree and essentially did like a smart search um against the existing uh data in the tree like what is relevant for the given query or task yeah um
And so...
That means that you can have like huge amounts of data, but always get just the relevant context at each time. Yeah, right. That's really cool. Yeah. So just kind of talk to it. It figures out what you're saying, like map that to like a mind map in a way, and then.
with that linemap use the age what do you use the agents for in that linemap so for software engineering
you can use it like to actually do things like I couldn't feel like
I like write the code to do this
Right and it would start like it would go into my code base and start like actually doing the work
Um, maybe like an investment banking example.
You'd say, okay, like...
add this to the investment sheet whatever right and it could go and do that um apparently it's only set up for software
But it has access to all the tools like write code, run code.
And
yeah so that is one product that could sit on this like platform
um
And like QA could be as well, like for data rooms, you could use this whole infrastructure, maybe like more like these two things to ask questions and get really good responses. So like this semantic retrieval, it's it can make large language models.
themselves like uh more accurate um so large language models like as you get more and more context or like as you give like larger and larger documents their performance gets worse and worse yeah um but this way you can
can prune the tree to only what's relevant at each stage um and so you maintain always just the relevant data and so that it actually like uh large language was like more accurate in answering your question it doesn't hallucinate as much it doesn't hallucinate as much yeah right because there's less like um like things to mislead it um
yeah and so like i'm there's a open benchmark called long bench which is like um like different models to compete to see how uh how good they are at handling like problems with a lot of context
um and like gemini is currently the best um oh yeah i reckon yeah i use term and i work way better than chachi yeah but like why in this case yeah absolutely but this gemini plus this uh is even more accurate so right now i'm working on that
So use Gemini to be an AI nerd. Yeah.
So, yeah, it's like on top of Gemini, but it can make Gemini itself more efficient. And so it's looking like we'll beat the benchmark, which is really cool. And so it'll be the new state of the art on the benchmark.
Which hopefully should give like some
right like basically funded that yeah it's 50 million dollars yeah
Yeah, if you can- Fuck, that'd be really- Yeah, that's good. Yeah.
yeah yeah but i'm thinking like even from like what i could use it for like we have a lot of men like looking at looking to buy a new business
Sometimes you just sit there late at night for a few hours, just, like, spippling ideas in a meeting room. You just have this thing turned on with microphone on. You guys could get ideas for a while. It just, instead of me having to fucking sit there and take notes, like, really messy notes, trying to figure out how to structure it, like, it's just doing that. Yeah, exactly. That's, like, I think that'd be really cool. You just have this running in all your team's meetings or something, and, like, over the course of months and months of doing a seal, and, like, build out this mind map, all the issues from PD, and, like, legal, financial.
Yeah, it can like that's like how the mind map breaks out. It all draws it back together like yeah, what's your investment thesis?
Talk generation.
Like let's say you want to invest in cases, you want my train, you want my Yeah.
yeah yeah that's currently my yeah i mean that's what my job is like i'll have a legal dd report yeah and all these legal issues from like the fucking account of emails back and forth and stuff and i have to summarize all of that into one page for my ic paper that's like the job that i do yeah i was gonna go what you could do there is like you know wherever you wherever you've been talking about um like it's a bit messy but let's say like this was your um you
You could just open it and say, like, write a one-page doc.
But yeah, give me a one-page summary. Yeah, then you can feed it This is examples of what legal summaries would look like for the last 20 pages of it. Yeah. Yeah, just do something similar similar language
and you have it so you can make powerpoint files and stuff
That's the big thing.
Gemini, it just gives you a f***ing table, like, make it a PowerPoint card for me. Yeah. Yeah. If you have just PowerPoint engine on top. Yeah. If you can get all the Microsoft stuff on here, like, f***, like, because Microsoft stuff is so embedded in corporate finance, like, Outlook, Excel, PowerPoint, Word.
and within it within a lot of those things like people have their custom like toolbars from their firms and stuff on like all of that it's like so embedded and graded to like workflow do you think you'd use it like as in if we if we built that made it so you could edit powerpoints like what would what would convince you to actually use it
i mean i've i've been using more and more ai for that like for that purpose of just like
i'll have a lot of mess like a lot of thoughts right yeah messy i've been using like gem and i'll just
it like a brain dump yeah and i'll get i'll use like i'll think through my thoughts structure my thoughts and all of that it's like that's like that's something i'll probably use a lot like so i can just have this running in a meeting just hit record and yeah yeah i think um the thing is
like loom for example can do that yes there's a loom note taker if you take notes should be meetings right put it in a dock yeah but there's no order like context like context
Yeah, I can't relate it to like.
discuss in the meetings yeah go like like get relevant connections like legal
yeah that's the thing like there's so many siloed work streams and then like if you're using ai for something
it's only like whatever you feed it at one time if you can have something that like from the very inception of like a deal all the way to the end that's like you fed every fucking bit of context like thousands and thousands of emails thousands hundreds and hundreds of documents um all the meetings all the meetings
basically like a copilot copilot
copart is good it links to like your outlook and your tribe and all of that but like i i think it's true yeah
Well, I think the nice thing is like what you're describing applies to sort of any knowledge-based work
not just like investment like you have your vision for how you use it for investment but like when i talk to my brother who's in
like policy and government policy it's it sounds quite similar to what you're describing like how he would use it yeah i mean a lot of jobs is just taking a lot of knowledge and like kind of synthesize it to an idea
And like, that's what AI is really.
off so yeah yeah the barrier to it is like feeding it the information feeding the contact at the moment so the easier you can make someone to feed it that information i think that like yeah if it's automatic like you know like even right now if you open like on teams and zoom calls and stuff people have like those automatic like note taking agents to join it you can just have this it's like an automatic note-taking agent it's a good idea because that's something that people already use like there's a couple apps that people download yeah every time you join team school it's like this random bot with its camera turned off yeah it takes notes
and then yeah like
you have like another app on your phone similar like the gemini or chat gpt app and like you can like you know you have this thing on the gemini app yeah you just have that sitting in like the corner of your meeting or something and then um
It like takes any contact from that conversation. I'll be cool.
I could, yeah, maybe.
like, which is, the last thing is all about.
So, we have all of these broad jobs, so it basically is that, everything, like, one thing is, like, sorting that. Like, is that the gap year or the annual? Yeah. Two is, like...
cross product, high texture, let me see. That's the whole reason we're going to see my crap.
But more than that.
It's, there's no concept.
Like there's no, like, male, black, here's what these dogs actually mean, like, completely underrated.
Absolutely. And that captures the document, the concept, and how it relates to other documents.
yeah yeah so yeah as you can see it gets a bit messy after a while right like i need a way to like better like you know start like collapsing things as i zoom in and out
Um, but
Here we talk about like here you talk about like voice tree for automated note-taking for knowledge-based work and Microsoft Office and PowerPoint editing
legal due diligence and so it has all the different things you've talked about um and how it how it relates to the larger idea of voice free and the product and
um it's really cool i like the candacy it's kind of it's kind of neat right yeah
And like the agents, I thought it was so good that the agents can just go start by watching out your ideas.
Yeah.
did you do that yeah like um i'll say like um let's do we can do one that's a bit more relevant so i'll show you what exactly happens like i'm opening an agent from this node of um
voice tree for knowledge-based work and it gets like all the relevant data from the tree so it says the following content shows what other context might be related to you and then like it starts from voice tree product and core functional
right and then it just goes all the way up to like where we are here and i can say um
like write a one-page business plan with it.
And then it takes 30 seconds, a minute, but it'll do it in the end.
It's a rolling question. Yeah.
It's like the little stuff you put in it, you know what I'm just gonna have.
The better it's going to be able to go get those phones, it's like a big chunk of text.
and it's like add another chunk of text to it the more you add the shitter it gets
reach back for those old-time selves. Yeah. Yeah, the longer you chat to them, it starts hallucinating, making shit up, I forget some things you told them before, yeah.
So you say it always saves this like knowledge tree and then you can open this three months later and feed thousands of documents, but it still knows that the relationships are doing everything. Actually, all the documents are just files. So these are just markdown, like text files basically. So it's all just saved on your computer.
Um,
oh so it's all just it saves us all it's just text files so it's like yeah pretty it's like
yeah yeah yeah um because a lot of like they don't you don't want like the stored like on my servers or something right like a so it's like actually pretty like simple for your computer to run it's all text files it is very simple yeah
Yeah, it is, it's better than, like, proper infrastructure. Yeah. I reckon, like, something like this is sure to be, like, what Microsoft is thinking is, like, an end goal for, like, Copilot. Yeah. Because that's, Copilot's, like, the benefit of Copilot, it's got the integration into, like, Outlook.
call your onedrive outlook um or the microsoft suite one note all of that kind of stuff and that's like the tools my people in corporate finance
I'm sure like government or like everyone uses Outlook kind of thing.
So here I wrote a business plan.
Voice tree transforms how knowledge workers capture organizing synthesize information honestly not that bad trying to turn a billion dollar knowledge man
Boom done 200 billion dollar mouth
It's all right, it's done the consultant's job. Yeah, it's done. Fuck, I don't know how to consult it now. No you don't. Dude, literally a PowerPoint agent...
Yeah, it would be really good.
Um, yeah, dude. I mean, if you, if you want to start using it, I'll, you know, I'll send it to you for free. You're welcome to just like play around with it. Run it as you say, just in a meeting and see if it's useful. I don't know if my company would let me do that. Yeah. Yeah. Fair enough.
It is a weird analogy, but it's kind of how a brain works.
It's like, like, different parts of Ukraine are good at doing different things, right?
Like you split it up into concepts and then those concepts look differently handled at the possible pressure.
Like specificity exists and that's why it's such an efficient system.
It's like different agents like one for PowerPoint
so I guess that's a really interesting point where like of like oh my my company might not let me use it like yeah because I don't think I get around that because if you're using like public Gemini it feeds that like
Gemini will train itself in that. We have an enterprise agreement with Gemini. Right, right. So if you can create like a secure like. Yeah, so maybe if either a call on device or.
yeah wall on device AI but then it's not gonna be as powerful as like Gemini or like or provide proof that it's not training on your data like it's not we're not storing any of your data yeah
which is there's a startup that exists right now which essentially just candles boil them off.
So it's like, they do all the plants to every standard, they make sure that your daughter is not being stored in the models that are doing it. I think that would help, because if you have certification for Banter, which is just the guys that are doing it, then we're going to...
The company be more, I say that in turn in.
Thank you.
yeah i mean like for my company like we're a small business we're not like a massive multinational organization
So our compliance checklist is just like, data hosting locally, it's not being shared outside of anything.
the weed i wanted to be shared with um that's part of the so what would i have to tell you to you for you to be like yeah i can use it at work you'd have to tell an it guy convincing
They have an IT guy. Oh, really?
it would be like we have like we have an agreement with with the companies that none of your dollars will be still which is actually yeah yeah yeah yeah i mean like my company already has an enterprise gemini yeah so if you could like sell it if a client already has enterprise gemini you can like to use their gemini yeah yeah and then you know like that's where you secure
Like, it's Google's guaranteeing that security.
So, yeah, yeah, it might be nice to say, I think it would be better to do as like.
it's like oh you like you have those enterprise agreements like really enterprise agreements about data storage yeah yeah it could models that you employ
Maybe.
every time there's a new model that we have but i like bigger organizations they're so like if you're trying to sell this to a bank like it's like three years of compliance checklists to go through and by the time it gets into a bank like what i can say is like like there are like this is a known problem and like there are current i know that for a fact that there are some solutions already share for this as in like solutions where like there is no doubt being involved by the
yeah like it was like he just instantly took everything we don't hold any of it they don't hold any of them
Yeah. I mean, the barriers, I don't think it's necessarily the compliance checklist. You can always vote for that. The barriers, like the six-year-old board director to approve this and is like, I don't trust AI.
yeah that's like what a big guy is going to be
audience, five, we're going to talk to you.
Yeah, I mean, ANZ just announced 4,500 shots.
I'm going away, that's like 10% of their workflow.
so do you have a recommendation and how i turn this into a business because there's like there's this product that i can start like i have like people who want to use it like i've got a wait list of
And I can start giving it to them for software engineering. I mean, there's like the larger vision of this whole platform.
but yeah what like what would you what do you advise yeah i think i i think it's really good for how this whole thing works yeah um is this is this hosted on like chrome or like is this a platform that you feel like an app that you feel so this right now is all local and like
Yeah, so one problem I have right now is like I need to spend a month of work to like be able to give it to other people.
Yeah, I guess in like within a month, I'll be able to just like give it to someone. Yeah, right now It's a bit like just connect to my computer a bit too much. Yeah, right. Yeah I mean, yeah, once you get past that, I think being able to host this on like your browser Yeah, whether it's like a browser app or like yeah, it's just a website you can log in to because it makes like I can open my phone I can hear Browser solves the whole like Right and then you wouldn't have to get a security approval I mean you'd still need to like well, you would you could just open the website and get started You just need to make sure like the issues probably from the security perspective Not hosting an AI like
Yeah, I don't know how you get around the AI thing. I'm sure, like you said, people have tried. This isn't the first time someone's come across this issue where people don't want to feed AI sensitive information. Yeah. Someone's probably figured it out.
As in like, as the time it comes, or you pick with this, no one gives a f*** in your own. It's like how 10 years ago, there was that whole thing around...
Like three years ago, everyone was like, oh, we can't give AI access.
it's gonna fucking catch you and kill everything like now it has like yeah access to everything like in the same way as i becomes like hey i like companies will engineer
But how do I?
I'll see you at the bottom. Bye. Bye.
How would I monetize this?
You've told my boyfriend also. Yeah, right. How do I monetize the shit out of this?
that's what I'm saying, there are tons of use cases, but I'm just saying, that is it.
allow other QPPs to save them about using it.
Um, one piece, I didn't actually subcute about it, it's a hard piece.
Oh, so you just create the back end and let someone else put like a GUI rapper on top.
It's not that you are, Rafa, it's more like they have access to these technologies, and they can build it. Yeah, it's like an open source kind of... It's almost like a foundation model. You go into a contract with the foundation model to provide you certain second limits.
We do the same thing. You can use this technology, pay on demand.
what's stopping google from just doing that themselves from building this yeah i think that that's the second part so like when i did analysis of this like in the morning like i'm going to be in the pool
The two big props that I had was...
This is like super useful teaching technique, meaning it's like it can make your long context windows better, so every validation model company would want it.
So you have to pilot it because like...
That will, like, if you've had an internet, then they can't just do it, if they do it, then they have to pay for it. Yeah, I mean, it's hard to fucking fight against Microsoft Meta.
The second point is, this has massive value as an older screen. As an older screen.
So like as models become, as models get.
I've got a competitive advantage, for example, Gemini's better than some stuff. Yeah. Maybe someone comes out with a private equity model.
Thank you.
So you talk to her, right?
and it would go like let's say you talk to it and like suddenly we talk about some legal stuff right yeah like google can't do this google can't go oh let's use some other companies for the legal model
to understand what's being said
agents to solve
But doesn't Google already like Gemini and Chachiki already has an orchestration layer, but like if it's a very mathematical task They won't even plug it to the LM They have that orchestration layer already
and it's not about like which models are better like they have internally maybe they have their own models that are better at certain things that they can rewrap to
But fundamentally, some foundation model company is going to have an actual competitive advantage.
And those changes are only going to grow and change.
That's what you make for yourself.
There will be a legal model that will be better than others.
there will be like
Yeah, like due to like a key diligence model.
Go ahead. That makes sense. And so you don't have to build those models.
but you can all come straight to the jungle.
That's a pretty good idea.
But yeah, something like this I think
Could you even just do this like you just create like like normal consumers can just like you have a website It will make an account pay $10 a month whatever it is Yeah
Then you have no shield, bearded wings. I think that's a really good way to start. Just make a website that people can load up in like 10 seconds.
yeah and just go boom straight to it and start playing around with them then you get you get real-time feedback yeah yeah using it and then they download them all hardcore local versions and then once you like find that you can go to a business and be like hey do you want an enterprise version where you like secure those everything
Is that every kind of... This is like...
I also did this quick flight, they started with my calendar and moved to entertainments.
Like, it's pretty fun to be a shame to grab and then it'll be great. Most platforms have like that. Yeah.
It was like, yeah, because it's hard to go to a business and sell them a product.
you know hasn't been tested yeah if it's like the first time but it could be it could be a great product they don't really need that trust from them and that comes from like having a user base
Well, like, somebody's like, Laurie, it just makes sense to me.
Maybe
It's like, you know, it comes with ice.
they just do that like you can call up on the phone a business for support and it's like that ai agent talking to you oh yeah
yeah except the pan it just so like this makes it look yeah oh yeah we we own um a fucking full center business like that really i don't know why i mean i don't know i'm probably gonna fucking
I wouldn't be surprised. It's not my asset, it's not my asset. Sell it, eh? Yeah, we bought it a year, two years ago. We knew the whole AI thing was coming. I think part of the plan was to, like...
supplement AI with the people and stuff. But I think, like, customer service is a weird one. We have, like, very complex needs from, like, big corporations. Yeah. And a lot of stuff, it's, like, government. Like, you're doing, like, one of your clients is, like, the tax office. And you're calling people up to collect fucking debt money or tax, and people ask them to have questions and stuff.
Tax office lawyer has like very specific requirements of what they need in the customer service.
yeah business so yeah so do you think i should prioritize like getting this to users as quickly as possible making a website which makes it really easy to open or should i focus on getting the benchmark
which proves like the value of this for like foundational models improving like accuracy and like uh cost decreasing cost um which could have like a so like you know a larger make it more valuable long
But, you know, at the expense of that means the next few months, I'm not getting users.
Yeah, I mean, what's your vision and what's your roadmap?
oh that's the question right yeah um yeah like i think yeah it'd be cool my like ideally i spent a month i beat the benchmark yeah the benchmark gives me a lot of publicity and i release while i'll have that publicity for users i get like you know 10 000 users hopefully um people just using it um and then either i transitioned
or I get, like, VC.
funding and then with vc funding i try filled out like the this larger vision
Yeah, which is like more like the platform where like someone else can come in and do
various different um how much money do you think you need to build that out
um this would be hiring devs by this yeah yeah hiring devs so like i would say like
uh like two mil would mean that i could get like eight devs like two teams of devs basically um which would mean that we could like move really quickly
yeah or that's him like is that one here oh yeah for one year yeah
But like, yeah, like even I think like a team of like four maybe it's like
600k, 500k for a year could be enough to get to the next milestone.
It's being milked from a VC, probably not that much, is it? I don't know. They got a lot of money to throw around.
Yeah, I've never I've never dealt with these days. I've not that before. Yeah
Yeah, I've never dealt with PCs. What I've heard, they like really back ideas and they back the founders behind it.
So maybe the VC part, getting the benchmark, this is a good idea, I just need the capital to build it out. Maybe that's what a VC would be looking for versus like, yeah, we've got some users.
yeah and um the benchmarks the benchmarks and then you get really polished products yeah it's that money you have a good front-end developer like a really really polished product then you get out to market
with like a very
Yeah, I just don't know what VCs look for typically, I'm sure if you sit in enough cafes in Sorry Hills, you'll be plenty of VCs around there.
Yeah, just open up some thing with this code on your screen and sit there like a fucking And just talk to my computer. Yeah. That's what X found us. Oh, nice.
Thanks to this, that's useful. I think with the VC, can I just reach out to VC people and get the feedback? I've got, yeah, I've got a couple of VC meetings coming up. There you go. Yeah.
then yeah you can just ask them what do you look for like yeah what i need to do in three months six months one year yeah for you to want to back me and then you just work towards that
Absolutely, yeah. I'm going to grab no coffee. Do you want one? No, I'm with us.
I just had a second. Okay, can I go back?
I could like it.
Good luck.
Oh, no, don't do that.
Thank you.
That's not enough to go so I can do shit like this. It's a fucking slave.
so this
is a question from Lungbench. Okay. It's...
Um...
So I'll find it for you
So it's 10,000 words The question is if the original audio is low clarity blah blah blah, which I do multiple choice And then it gives you like a whole like manual
like a readme basically of some like software. Yes.
and then yeah does the LLM get the right answer yeah and obviously like you know 80% of this is irrelevant to the uh question
So, with the given question.
You know, it might be that.
Zerp, zerp, zerp, zerp, like that is what's useful for the model, like this, this sort of section, right? Yeah. And the rest of the stuff can be like just collapsed. Yeah.
And then you send that to the model and then.
returns.
uh you give it like essentially just like the path to the what's what's relevant yes um
Well, it's sub-tree the sub-tree you get the sub-tree and then you have to wait like convert the sub-tree into like flat text
Because I have to go back into it. Yeah, so I have a shitty way of doing that right now.
which has like roughly like it has slightly worse accuracy.
original I give you just input all the original text but the nice thing about my current algorithm is it does scale so for 10,000 words it's been Ellen can answer fairly easily but at a hundred thousand words it's just it can really struggle where if I can reduce that context by 90% my method I assume will become what
And so my method right now is.
embed every node as a vector.
a vector embedding. Yeah.
and then you can do a vector embedding search and so you can have the query and you have your tree and you can ask like compare the query against each node in the tree and say like which one is the most relevant yeah um and then you can rank them sort them and then like send over like the top 10 um but you don't just send you don't send nodes individually you send the traversal to the nodes um because the traversal to the nodes like contains like where it sits in the context
like how does this fit into the bigger picture because a lot of these types of questions
that require the bigger picture understanding.
Just like what relationship
Yeah, yeah. Relationships between nodes. And that's like the semantic meaning that I'm embedding in the graph. Yeah. Kind of like relates to.
Like, it's a subset of all of them.
Yeah, like this, for example, text segmentation for TTS provides guidelines for text.
like it's just how it
how it relates video memory constraints details constraints
provides a solution for.
describes common issues encountered during yeah do you have subset would seem to say
no what do you mean so like you know how you're talking about
this is really great like this is exactly what you wanted to grab um so like here at the root you have the very like root which is gpt software it's like the overview of the software yeah and then you have a child of relationship yeah of like the user manual
and so like that's a subset um i think i have a nice a nice visualization for this um
Um
Thank you.
so like there's your like subset superset model but then like that is just a rep that's just a different view of a tree yes it is yeah
but it's also it also becomes like L1, L2, L3 like
you you cache like not cache um i keep saying the word cache what i mean is like
Alright, so...
Um, so...
so this is the full context right yeah and what you could do is like
Thank you.
So exclude would be,
You take this
And you just say, um,
You say collapsed
um vpn
right yeah yeah um and that's like the concept of using it like uh it'd be like using bpn to do x
Yeah.
And then you're hiding that data.
If you think it's irrelevant. But the LLM can decide, hey, actually, I do want to.
um that's why i'm doing it the current way i have it is like um i get like you know like six target nodes
And then I output those nodes, the name of the node, the summary, and the full content, and then the parent, like, the relationship to its parent. Yeah. And I do, like, just, like, the...
just i keep on going essentially yeah um and that can be good because then it contains
The relationships.
Like you've pre-computed like where that concept fits into the bigger picture already. Yeah, and so you may as well use that data While the exclusion principle doesn't do that as much
um because the problem is right like
when i convert the context to a tree yeah
If I now find a node that's not relevant, like by exclusion principle.
and then i go to exclude that that concept might not exist one-to-one
back in original text like the original text might have like um concept abc abc abc yeah like the concept is sort of like segmented
so then when i go to remove concept c it's like not removing it in one place it's removing it from like three different places kind of yeah yeah from the original yeah yeah so you by distro by excluding concept you destroyed meaning
this isn't explain right like
Hmm.
Yeah, I just why not just like again like why
Um...
Have I shown you that website where you can see like
Back to Visualization
Can I have your wife, though?
Common? No, no, it should be error 404.
The person is pal
I need a...
my computer. I can grab your charger. Thank you.
Thank you.
Yeah
I don't know if there's a PowerPoint here, but I have to move inside. Oh, okay. Or, let me see if I can actually...
We have been outside for a while.
Yeah, I feel like you might even move the table already.
Yeah, maybe it's 30 minutes.
Thank you.
Thanks.
I'll see you next time.
Alright, let's go.
Okay, so...
load a TSV filter. So here I'm loading the vectors for
Um, this.
This? Yeah. Which is a question from the data set.
about the user manual.
Um.
Anyway, so...
So these are the vectors.
metadata
Alright, so that's...
every node in vector space.
okay embedded and then if i ask for the
So I have a query as well.
Right, which I want to like search query.
Um
And this will be your modern job.
So what was the...
If the original audio has low clarity, after completing the audio, what should I do? And you can also include this in the query. So if I do this, and then...
so
So this is the query here
Um.
And you can find like.
from the
query
You can find what are the nearest points in the...
Yeah. All right.
and so you can see like it's about troubleshooting dense audio cutting
That sounds pretty accurate and that number is that like audio quality and
And then if you...
let's see if i can um
like i can you can see like you know you can keep on searching and these are things that like not very well
yeah like related image assets right it's like
Not that close
Gen.
Yeah.
And so you might want to start with 15 of these.
And then...
that these 15 nodes are going to
correspond to like you know
this node and that node and that node and that
And from there, you can include in the context a traversal to that node from a root.
From a root to that node.
um plus it's children it's like let's say this is our target node yeah one of the 15. we can do the traversal to that node yeah and then it's children as well and maybe like also a neighborhood around
Imagine like I have a highlighter right now and I'm...
like everything sort of like in this area
right? And then you can do that for all your target nodes.
And then you end up with like a subtree. A subtree, yep. It's one way of doing it.
another way of doing it is you could
Yeah, add a score to each note in the tree of like...
essentially like, nodes, so...
Every node initially gets a similarity score from just this initial search to the query. But then you also, nodes close to troubleshooting dense audio cutting also get. So let's say this is a target node.
um and then all the ones dark here we'd also add some points to yeah because it's close to a target
even if they weren't like at all related to the original.
because they're close in the conceptual system.
in our conceptual space yeah so there should be a score on close
Because it's still a relevancy score.
It's like... Mm, mm.
to some weight on the
Yeah. Yeah. And then you can then, yeah. Include.
um, the most.
relevant notes yeah oh like yeah yeah discard the bottom 20 or whatever
Um, so yeah, various, does that make sense from, from like, cause that's like, I think the foundational principles, um,
what yeah what uh do you do you get that because it is it is quite complex yeah i i had to do a lot of research to like understand things like vector embeddings and stuff yeah this is really really cool actually i didn't know you could
Yeah, it makes sense.
Is there existing research on this?
Um... Because this feels like a...
Yeah.
I should really don't sit.
like do you have an existing
specifically um you know what i like i've read some um papers and stuff but i don't have a
that good of a...
Like, I'm not, um, but, um...
Yeah.
yeah so let's yeah research a bit i'll ask yeah and we can find some papers and try read them
Okay.
I'm trying deep research. I haven't tried it before. Yeah, it's useful.
Fucking love you, man.
Mm-hmm.
Yeah, I've got some cool stuff.
Um...
So this is interesting.
Yeah. So this is what the AI is recommending based on based on research.
Yeah.
and it has references, so
So selecting the relevant subtree.
State-of-the-art is treat selection as an optimal connected subgroup.
A prize collecting stunner.
so that's from like this paper um so give each note a prize which is how relevant it is to the query yeah and then choose the cheapest connected sub graph whose total prize minus edge costs is maximized
So I think each node has a score of how relevant it is, and you want to maximize the subtree, but that provides the total relevance. But each node also has a certain cost of inclusion, because you don't want to include all the nodes. Otherwise, that would be the maximum.
So if it's like an optimization problem of like where you...
Yeah, because it may be like maybe one note answers it perfectly and you only need to include one note, but So that's kind of cool. I like that
How that, and then there's algorithms to do.
like so if we just had um if we had each node has a has a score of how relevant we think it is then there's an uh then choosing the sub trees like there's a solve from
Um, G Retriever.
retrieval augmentation for textual graph understanding and question answering.
enable users to chat with the graph.
Oh, cool. Mm-hmm.
so that we could and then
Um,
There's another family of methods. This is...
to use bi-directional traversal refusal walk up to parents summaries and then down to the most relevant children that's kind of what i basically do now okay yeah um but i think
that's interesting because you start from node and you walk up to the parent. Yeah. And then you go back to the node and walk down to the children is what it makes it seem like. Yeah. And that's what you do for...
And that makes sense because in a vector space,
It may not be the parent that is the
It would be, it would just be anything that matches this, this, um, the initial semantic search. Yeah.
Yeah. So one thing to keep in mind is that semantic search...
So that the initial vector search is not semantic.
as in it's like it's smarter than keyword search yeah like you can it has some sort of meaning but it still doesn't understand
It won't capture all relationships.
concepts. So if you ask something like,
What is Neil's favorite dessert?
Um...
And then it's going to have some amount of relevancy to like apple pie because apple pie is a type of dessert. But if the answer was none.
Nio only eats savoury, like that might have very low relevance, right? Because conceptually they're not related, but there is a relationship. Like it's the answer to the question, but in the conceptual space, they're not actually the vectors aren't close to each other. So that's like the limitation of like a vector.
yes yeah yeah that makes sense yeah that that's kind of the question that uh that i started with which was like if i have a a semantic tree and i have a vectors and i have like a vector search how do i combine
to like um essentially output the most relevant result to a query yeah
Yeah, and the vector search is there so that we can see.
so that you can have 10,000 nodes and do an initial.
to the top 15 yeah yeah um because otherwise what you can do as well is you can just like send all the titles of the nerds yeah and ask the lm like which
Okay. And that works well for like smaller contexts, but it doesn't scale.
Yeah, it's like
like you google like you're doing a set result like yeah every like everything is probably irrelevant
because it's relevant but not because
that's rank us
Mm, mm, mm.
Cool, so that is an interesting idea and I don't, this bidirectional traversal is not what I do right now because right now I start from the parents.
Yeah, all the way down. I think it's good to start from the node
get to where it is in the context by going to the parents and then go back down to the children.
I like that. Um...
And then I asked her about the best way to live.
Yeah.
Graph linearization.
So here there's an interesting idea of like
yeah uh red comes mine kept represent mine kept recommending graph that
yeah yeah yeah graph rag is um definitely similar to
um our idea um but they're the thing with graphic they're a competitor um but they
are not, like, generic content.
Like in graph reg an example graph rag
I'm not sure exactly, but like.
Let's see if they have an example, it's more like people or like
um like it's not concepts that are the nodes it's like um objects or people chunks which is much less abstract yeah
Because I'm assuming maybe it's used.
Oh, would be useful for I cut.
yeah yeah yeah just to visualize that like
This is the most relevant note.
that we found with vector search yeah now that gets a score yeah and each one of its neighbors gets a score divided by two yeah and then two away
one two it's a score divided by three or something yeah or you could even have it logarithmic
like a rhythmic yeah because yeah reduce your
Yeah, yeah, true.
two. Yeah. Like divide by two each time.
So, um, 2, 2, 2, 2.
until like you're five away or you can do the whole thing.
Oh, cool. And then...
Fusion ranking.
Reciprocal Rank Fusion.
Oh, thanks for representing you. Yes, 100%.
There are some smart people out there. There are, aren't there?
but the cool thing is like you know initially you you see this research like what the fuck
How would anyone ever come up with that? But then you can see, like, by working in a project.
you can end up with something like that emerges
It's quite complex. Yeah. Yeah.
Um...
Target graph dressers performed on the tree structure to collect a contextual subgraph
Yeah, so this is essentially what we were doing.
which is like one initial semantic retrieval
to structural content.
That just does the, like we discussed, like...
Um.
Like given a tree.
For given query.
Oh my god, is this still going?
VoiceTree is still running.
Alrighty, cool. So today I want to work on improving the, clearing up some tech debt that I have with VoiceTree.
Um,
Because
uh it's slowing down development right now i haven't really done a good um i've coded a bit too much and now i need to go clean up the experiments
it. So what specifically is that? So
First, the typing system.
Tithing system's a bit worked right now, so one problem.
is that one of the recent architecture changes we made
was in VoiceTree, Backend.
Um...
We built the...
context retriever context retrieval module and
What else did we make?
I think markdown to tree.
We also made a module.
and then
Parts of that aren't using the correct tree manager.
So decision tree data structure.
So that's one thing, is to use the decisions tree data structure, because in some places, instead of the tree class and the node class, we're just using dictionaries, which is terrible. So we want to enforce the typing.
And so one thing is just like changing it to use, you know, all the methods to use the tree. But another aspect is I want to prevent that from being possible in the future. Like I don't want anyone to be able to not use the decision tree class to represent a tree. So I don't know how we could like come up with some type checker or something that prevents that.
another thing is decision tree underscore ds that should be
It shouldn't be called Decision Tree. It should actually be called... I'm going to rename it right now, actually. Let's just call it Markdown Tree.
Um,
That's what it is a markdown tree
And then we can rename DecisionTree to MarkdownTreeDS.
And we actually want to move free manager out of text to graph pipeline.
Um, um,
And to back end.
So I'm going to do that reflectoring right now.
um might break some things um so first up we'll probably be making sure that the renaming i just did and the moving the um moving the folder didn't didn't break anything
um
Yeah, so in general, we want to use data classes or identic.
Um, Identic.
Um,
at the API, like anytime the data is dynamic.
but internally just data classes.
or actual Python classes.
And yeah, we want a way to kind of enforce that so that when we use coding agents, they can't just use dictionaries.
That just destroys everything.
and so a lot of the tech that we're worried about is
Context retrieval.
So, for example,
Um,
Which one is it?
Thank you.
I think it was...
Yeah, traverse all relevant notes.
Um...
Traverse all relevant nodes
context retrieval.
It's a dictionary to represent the tree and that's probably the first thing we want to fix
I shouldn't take a dictionary.
should take the actual tree class.
And the other thing we want to do later is traverse all relevant nodes.
just returns the relevant node.
But we actually want it to be a deeper method.
perverse the no
and accumulate their content.
Because right now that's in the...
kind of in the main function, convert the results to text using accumulated content. We want that to be hidden inside the method instead of the main function.
and update any references there's a reference some performance comparison driver that does something similar as well oh that uses it so
Okay, so let's...
Yep.
Thank you.
Okay, the other tech debt type thing we have to do is...
So when using VoiceTree, the positioning of new nodes.
the terminal windows requires way too much
you know, dragging around right now because
not quite right, either the terminal.
big. That's one problem. Terminals are too big. Terminals have too much padding around them.
I don't know if that's something we can fix, but that'd be nice.
And then the other thing is, yeah, we need more spacing between nodes.
and a better node positioning algorithm.
And the reason why this is tech debt kind of is because right now we're wasting so much time.
nodes.
Um.
But I know that's not an easy problem to fix.
Um, the...
And also...
Yeah.
Thank you.
Thank you.
Okay, and now we are seeing that there obviously is a problem with imports and stuff. So we're getting all sorts of import failures. No module named backend, textograph like pipeline tree manager.
Thank you.
Thank you.
All right, one important thing is that we do not ever want to have backwards compatibility.
so do not do not maintain backwards compatibility that just introduces extra complexity
Just make sure there's only one system.
Thank you.
One thing we just saw is that the LM was getting confused with relative and absolute imports.
And we want to add to our claw.md file a very...
It's a single line that says always prefer absolute imports over relative imports. Never use relative imports.
Um,
So...
Thank you.
The other thing that would be nice is to like, not, as with the UI, um,
not let terminals
fly away from the
criminal icon um because that just confuses the fuck out of thing and lets you have bad um visual organization
We just don't want to let that be possible.
Thank you.
Yeah, I think we really want to focus on this session, not making new features, just making sure that our existing
aren't making us waste tons of time.
Psst.
Thank you.
We'll see you next time.
Thank you.
Thank you.

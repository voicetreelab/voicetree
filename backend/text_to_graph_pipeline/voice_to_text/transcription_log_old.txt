Okay, git checkout main
And now let's see
If we have...
Sure verse.
Okay, here we have it
okay so now we want to make sure that this
Scripts, we found it.
um
Um...
Let's also load the latest
Um...
So let's go
Build.
And then let's go.
Um, okay.
So, I think, did we just run films?
160
Reload juggle.
So we have now this
tool.
Um...
graph dependency traversal and now let's say find where we use it
Okay.
Okay, let's restart my computer.
I'll see you next time.
You
Thank you.
Okay, let's end this.
Hallihallo!
Hey, I'm doing, um,
Recordings.
Let's go.
What do you want to do though?
No, I was um, I just stopped my recording, but I'm doing a cool recording like this
Oh, cool.
Can I have a look?
That's what it looks like.
Well, it keeps on flickering. Can you turn my phone on?
I think it's out of battery.
It does not go on? No, it's okay.
Mm-hmm.
I'll see you next time.
Hmm
Okay, and then let's have the logs there, so.
All righty. Okay, so let's see.
if it can now understand that there's a tree loading integration.
can that we just built
So we want to understand if that works.
So
Let's see, creating a new node.
Hmm.
All right, so we've just
Um,
So we just added functionality to the load in existing tree.
We want to see if that works now.
So tree loading integration
access that node
Hmm, okay, let's look at...
Voice tree.log
Alright, did we just...
Mark down.
3 volt
What the fuck is going on?
let's see ready to listen
I have you loaded
loading tree.
Loaded 18 months.
I love you.
Okay, so we should be able to just add that there.
Okay, so we want both a logging and a print statement.
Okay, that should be sufficient.
Okay, cool. Now it's working
So if we're talking about the tree loading integrator.
can it add a new node there?
Um...
Let's see
Yeah, awesome. Okay, that's pretty cool.
Alright, so that's first feature done.
Let's just check the video.
All righty, that is done. Okay, so cool now we can actually work on voice stream. So we had
Um...
What did we have? We were going on a couple of things.
Oh, yeah, the two problems with voice.
So we want to work on the terminal default height issue.
And the, um,
graph flickering issue.
Um...
So...
Let's work on the Terminal Default Height issue.
because that's going to be easy.
Thank you.
Okay, so if we open
A terminal here now.
Cool.
Oh, actually.
Before we do that, we.
Um...
We wanted to look at if we could.
Let's hide some of that
All right
Hide.
All right, so we wanted to see
Um...
So we did that
And now we wanted to see.
Okay, so that worked, cool.
We can...
Clear that node
Okay, so we have demo complete.
Um...
Okay, so...
What do I actually want to do right now?
Um...
Oh, yeah, okay, so we were working on...
the ability to do a graph traversal search.
So that's this graph dependency traversal tool.
So if we open that up...
And we can run Claude.
Okay, and then...
let's make that a little bit bigger
Um...
Um, okay, so...
We have a tool
Voice
Tree
Oh, okay
So graph.
Get a tool for this.
Um...
What do you want to do now?
So we want.
So what do we want actually to happen?
Whenever you open
from a node.
We want to do a graph.
On that node to its parent
So up through its dependencies.
One second.
Hello?
Okay, I hope it's time to go to the side.
Where are you at?
All right, we just had a packet come.
um so yeah what do we actually want to do regarding the
dependency traversal tool.
Um...
So...
Um, we want...
Returning only seven nodes from select-
Why would that be?
Oh, because...
Right. Nothing was relevant.
Hello?
links
Sorry, we're still Orbin or...
Oben oder unten?
Unten.
Robin, okay, I'm going to go to the right side.
Thank you.
Oh man, fucking door.
Um,
Right, so...
What do we want to do we want
Um...
when
So instead of just getting the parents' content...
we want to give the full graph dependency traversal.
So, currently...
when we open Claude.
Oh, come on.
When we...
Okay, so in...
markdown tree vault, add new node.
No, here.
Demo prompt dot wait
Ah, okay, so.
Um...
So we have currently in.
We just load
So we have pump is.
Doom.
Wait.
If you want.
Injected in this prompt to be the full.
I propose a way to do this.
Okay, so I'm running an agent
right now on
So that's running.
Okay, and now I realize that
Um...
when the graph moves
Sometimes the terminal isn't moving.
Do that. That seems to work
But I guess if it happens while it's moving.
Um...
Okay.
So let's see what's decided to do
Okay, so now let's
open
A Gemini instant.
Um...
So
We are getting lost in the tree a bit too easily.
That is one problem because things just sort of flicker around too much, which is...
The second problem we need to...
Okay, so...
Let's um
delete that
So we have here and here
And now we want to go Gemini.
Okay.
And let's get it to review that.
so this is
forward dependency graph traversal into cloud SH.
Let me see, review this.
proposed
navigate to
All right, let's get more.
Okay.
And now let's go.
Opus to work on that.
and say,
review this
this
Also look at the neighbors.
So this does make me think that we're all
have to be able to see
Oh my god.
So annoying zoom.
Um...
So let's see
Um, okay, so...
There's a problem here with the markdown.
Let's, let's get, uh, do it. Okay.
Thanks.
Make sure they're known.
problems with this.
And then exit.
So this guy's saying that the clod is
the build is failing.
So when you figure out why the builds
And what does he actually change?
No, we don't want that.
So let's say no
Shut the man.
So we can't reference files easily
Um...
Yeah, it is a problem. Voice trademarked entry vault.
I'm going to guess. Oh well.
Okay, so let's test if that works.
um so we can
So we can open up a terminal here.
Claude.
Let me see, do the same for Gemini.
And then we want to see.
Um.
There you go.
Enhanced.
So we want to loop.
Okay.
You
I'll see you next time.
Okay, let's see this guy.
Okay, let's see what this guy did
Don't want that action.
Do you need to be able to specify a...
folder for it to work in.
Um...
So maybe
We could do like...
I don't know.
Claude sh
Folder like that
It's not a bad idea
Okay, cool. So now we want to work on a new task.
Um, actually, let me just see if I can open it.
Okay, that did actually work.
Okay.
The flickering is really annoying.
Okay, cool.
So today we're working on
And there's a few different things we want to work on.
um, mostly related to getting, um,
A live demo, looking really good.
Um...
And so some of the improvements we want to do for VoiceTree right now.
Uh...
Um...
So one thing is when a second node appears on voice tree
on the juggle UI.
a
It flickers, the whole graph, the location of the graph.
changes and you no longer see it in the notes.
So when the second note appears, about three seconds later,
Um, there's a large, uh,
um flickering issue so we want to
fix fix that so there it just happened you just saw
Okay, the other thing we were working on.
Um...
was for the demo was improving the graph traversal to the LLM.
Um...
So what do we want? So right now we send a full graph traversal.
But we also want to.
inverse document search
algorithm to find other potentially related nodes.
to the user's query.
Um...
So what is that going to involve?
Um,
So that's going to involve, um,
So when you run
dot sh.
we're going to
So we currently give it, we run the Python.
tool to do a graph traversal.
and we give it the full traversal of the current node.
Other functionality we want
So on that current node,
we want to do a inverse document search.
Against that current node
um
to all other nodes
in the folder.
Um...
Bet that note is in
And also send back, like, for example, the...
10% most related other nodes.
Alright, so what are some things
agent will need to know to work on this
So...
Um, the Python script is in our, um,
root repose directory.
And then there's some related
Files.
in VoiceTree for doing inverse document search.
Um...
And...
Um...
That will be useful to know as well
So let's see here.
Let's go, let's change the link here.
Um...
So let's make this.
Define inverse type in switch scope.
So can we edit a node here?
Okay, so there we edited the voice tree slightly
Um...
And then we have some...
Okay, so I'm going to also add to inverse document search invoice tree.
Um,
Other useful uTools
All right
There
So
Okay, so yeah, so you have access
to the tree functions file.
get most relevant nodes.
But for now, to keep it simple, we don't want to load the whole market.
into a
decision into a tree data structure. We just want to replicate the inverse document search just without loading it in.
tree
Um,
Cool. Does that make sense?
Okay.
So we have our dependency graph now.
Um, okay.
at relevant node retrieval.
I'm now going to launch an agent.
And let me just see if it has all the context.
um
So we'll open an agent here
And we'll go claw.sh.
And let's just see what context.
Um.
do you understand how to perform this task
What other context do you need?
Okay, so that guy is exploring.
I'm going to add the the the the the the the the the the the the the
Let's do it.
Okay, now I'm going to get...
Gemini to review his work.
review Claude's work
Okay, and then another thing I'm seeing
now is that when an agent
Adds a node to our graph
Um, the colors are not that.
Nice.
um sorry the it makes it as a
as a header.
But we don't want that.
Um...
So we want to improve that so that when an agent adds a new node to the graph,
um the formatting is better
Okay, so let's see.
You
Okay, it looks like these people are done.
Okay, now I do want to work on the...
Add New Node Tool.
and the Node Formatting Pro.
um so let's just go there
Uh, open Claude.
Okay, I'm going to do a quick fix-up toss.
which is to combine Gemini
into one script so they don't have to keep on editing them both.
Okay, so cool, we've done that task.
We can just...
Um...
I think it's we can
Collapse selection.
Cool that guy worked
So let's do.
collapse on him as well.
let's actually get um
Gemini to work on.
Cool. Right, we've done that feature.
Okay, let's just end that
Commit.
Thank you.
All righty, cool. So we're working on voice.
We're working on making the live demos right now.
And we're just making...
some small improvements to the VoiceTree system to make the demos better.
Um...
Okay, cool. So one thing we just fixed is that when you run in a
You now get
a
So when you run an agent
an existing node.
You get.
um, a graph traversal.
And it also does a TF-IDF search for the most relevant other nodes in the tree.
Think let's make our voice settings
slightly longer
So we want in our...
Voice to text config we want a bit more of a pause threshold
0.9
Cool, so that's what we've been doing and now we want to fix a few things with the UI.
that have been really annoying for VoiceTree.
Um, so one thing is, is.
Um, one annoying.
UI element.
of VoiceTree is that when a new
the graph restarts its layout.
And that can mean that things sort of fly around.
much too much.
And so we need to think of a general solution to that so that things stay put in place.
Um...
So let's think about what possible solutions could be.
for the VoiceTree Graph Layout Instability.
Um...
Oh, okay, before I start about that, I just want to talk about another VoiceTree UI issue.
which is that newly appended nodes
Um,
get an animation for 15 seconds.
but for some reason the timeout isn't working.
And the animation appears for longer than 15 minutes.
So let's figure out how to fix that as well, it's another task.
I think...
But it was working for the, when we added a timeout.
create animation it was working working I know at some point we had the time
but one of our recent changes must have regressed it.
Okay, let's keep on thinking about solutions for the...
Voice tree graph layout instability.
So one thing is, is that we could, one option is to completely disable
the layout change and juggle.
And I think that's something called the cola layer.
Animation.
And if we do that, I think it's actually fine.
Um,
We just need to make sure that our new notifications
are generally correct.
So we'll sort of have to replicate the cola layout.
but only for new nodes.
So where are we putting you?
the decision for where we put them.
Um...
needs to sort of replicate color logic so that our graph gets built in a nice way.
Or, I'm thinking now,
Cola layout.
only run the layout on the new node.
can choose some nodes and but then I think that gets too
Um...
So
Okay, and now we're talking about another VoiceTree UI fix.
which would be a really nice long-term solution for the current problem we have.
um, with terminals on the canvas.
Um...
behaving really badly when they
move or when the graph zooms.
Um...
And...
what we want there
Um,
is potentially want to represent the terminal hover editor.
itself
as a
Node on the graph with an edge connected to it that way you can always connect
Terminal.
to the node it was opened from.
So currently we have sort of like a
A very, um.
Oh, okay. Actually, I think maybe we're pretty close to having that working.
So actually a related fix so how we could fix that is if we're disabling voice tree layout changes
What that means is
is that new nodes
I'm sorry, that moving node
Um, it would just, whatever, wherever position.
put the new node in manually, it will just stay there permanently.
And what that means is there's no unexpected node movements other than the ones we're doing manually.
And what that means...
that then
if the terminal, if we put it over
then it's only going to move.
if the terminal node moves.
which is the current behavior that works well.
So I think.
we actually have a fairly good solution for that.
All right, cool. So now we want to see if
Um,
if both the new node animation
working now that we've rebuilt.
Um, and also the other thing we want to.
is if
Actually, what did we just work out? Oh yeah, if adding a new node changes the...
animation at all.
And I just saw that it was still happening.
So we can open and develop our console.
Um,
Okay, so we want to see first if we've opened the right.
Plug-in
1.59, okay.
Let's unload that
That is pretty annoying.
Okay, so...
Let's start running our agents to give it the feedback that it didn't work
Okay, cool. Let's rebuild.
juggle.
And let's check if the animation is working.
Um,
and
Look at what.
So we can.
Reload juggle.
So let's make sure we're on the latest version.
Oh, okay.
I see, I see, I see.
We have a problem.
after we've renamed the repo. So let's go voice...
So I think the problem was that we weren't building.
which is why we weren't seeing the animation.
Okay, let's see now it should work
Okay, cool.
let's open up a new graph
and let's try the dragging functionality
Um...
So it works somewhat well.
And now let's try create a new node.
What we're gonna do what we're gonna eat for dinner today. I'm not sure yet exactly what we're gonna eat for dinner
So I want to figure that out.
Um...
And now we want to...
Cool, so it looks like we fixed it. The animation stops.
After 10 seconds or so?
Awesome.
Um...
And it also looks like new nodes don't really force much of a layout.
Which is.
Pretty awesome.
Um...
so that we can close.
Um...
All right, let's get while we're reading that another agent to get to work
So that's, we're going to have.
um, here a
So we have another UI problem.
Um...
And...
That UI problem
that when you open
terminal
the initial size of the terminal.
is a bit too small.
Um, and so we want
Um, the terminal.
when it's converted into a hover editor.
to be slightly bigger.
So we'll get an agent to work on that as well.
Um, so that
problem of the initial size of the terminal.
is when converted to a Hover editor is too...
we want that to become a new node.
Ah, so there we just saw that our position still flicked.
so we need to definitely get that too
Get an agent working on that.
When and you know.
Our viewport can still flick.
to a completely different...
Okay, we just want to check right now if we append to new content.
Does that result in...
Um, um,
Uh, the...
animation not being stopped by a hover.
So that's what we want to figure out right now.
Um,
Okay, so it doesn't stop on hover.
So we want to disable that.
So append animation.
Stop is now done.
Um...
Cool, so the agent thinks they found the root cause for the viewpoint. Flicking.
You
Okay, so now I remember that we have a lot of tech
that in terminal hover editor positioning.
So this is related to terminal size problem.
we want to make a
Um,
to clean up that code and get Gemini to inspect it as well because it just has so many bugs.
All right, so in that existing tech for terminal hover editing positioning
we want to make a new node about
Improving the
Uh,
have our edited positioning.
code because it has currently lots of bugs.
That I'm recording here
Yes, please. Yeah, thank you. But you know that phone
Now the phone.
It's recording me.
The phone.
Well, if you get in the...
in the way. Yeah, thank you.
Thank you
I'm just bringing you a drink.
I'm realizing right now there'd be a really nice
for voice tree if
Um,
If the agents can also modify.
existing nodes.
which they can actually do, I can just tell them that.
Or I guess the other solution would be to have.
Voice tree nodes.
Actually,
That to include the agent node
in our tree, so where we can append to them.
the nodes that the agents themselves have made.
which we currently can't do.
You're here.
You didn't tell me I could come out now?
Hey, wait, I'm recording!
Okay, just give me five minutes and I'll stop
Oh
Done
Okay, cool. So today we're working on VoiceTree, and there's a few different things we want to work on.
mostly related to getting a live demo, looking really good.
Um...
And so some of the improvements you want to do for VoiceTree, right?
Okay, okay, okay.
not completely. I'm going to make it something quickly to eat and then I'm going to film after. Can we see what you've done?
Yeah, when?
Now, okay, we're just going to shake it.
Are you ready to eat something Manu?
Are you ready to eat?
Thank you.
I, uh...
Do you want, I want to make pasta and I have...
Fake meat.
Is that okay?
Was that too boring?
Yeah
What are you liking?
Very good.
Perfect.
Hand from hand from.
It's really keen-shaven and with a little Italian beer.
by the time we were just
I like an Italian beer, but you had a very easy...
Right, did we just load up?
It's not?
But it's the same people. It's the same brand, yeah.
Yes, you can see so much what the other one said.
Tristan's reading this.
And I don't see something else, don't they?
do this.
Follow Beagle
Yeah, ask you to go.
Do you want um
Let me set the table here.
Oh, yeah.
We'll just take everything up.
We have been wondering.
I've watched it a long time.
We're in there.
What's beautiful up there?
There's nothing in the washing machine. Are you sure? Oh yeah, no there is.
There's stuff on the floor. What is that?
Should we do it now?
I'll do it, I'll do it. I have to add my white chalk.
Why don't I put this in the dryer?
It's just going to make noise.
When he's finished the recording...
Yeah, I'm finished recording for now.
soon.
3 load
Thank you.
Roll that in the back.
Yeah
The front stuff on the floor everyone.
Honey, but how's it going? What are you doing?
I was filming live.
use cases of voice stream.
Yeah, you gotta work down.
It's great. Yeah, it's really good.
so
So with that, not enough.
edit that into with your voiceover.
I mean, I don't think Elon's going to do anything.
He's at a party right now.
I don't, I'm not counting on Ilana Dooney.
but I need something. Okay.
I need to add the thing in
going for yoga.
You can get it if you want.
Where is it?
Fake news.
Right top.
Yeah, and foot wash, you need to...
I'm going to put it on my full wash mittel.
white.
And one of those calcium...
What do you want me to use?
I want you to use a full wash method, two tablespoons.
Not that the length of my day.
have good pressure.
Peruvol
Careful with this one.
I bought two new boxes
I'm going to do a little bit of a bowl of water.
the white and the other
Okay.
For the wash metal, two tablespoons.
Yeah, yeah.
One tablespoon.
Two tablespoons, and what else do I put in?
The thing that says we've used the power systems.
Today's LLMs don't.
memory. They reprocess the entire chat history for every single turn, which is inefficient and expensive.
VoiceTree solves this at the input layer. We convert any unstructured text into a graph and then ask the LLM what nodes it would like to have in context.
On the GSM Infinite Benchmark,
This results in 70% fewer tokens and 15% greater reasoning.
But then what we discovered is that this graph itself
can be more than just an optimization.
It's an ideal foundation for an interface for human AI collaboration, a shared memory.
I'll show you what I mean.
So right now we are actually running
I can add an agent.
and tell it to do something like draw a mermaid die.
So this agent that I added starts adding its own nodes to the graph.
I didn't have to...
rewrite any of my context in the prompt
and its progress is visible to me.
in real time. I could even, if I wanted to,
Send its output to another AI.
This workspace is the first product built on our core API, a new structured primitive for building with AI.
I've been using the VoiceTree workspace every day to work on VoiceTree. Here are some recordings of me voice
Today, the LLMs don't have a...
They reprocess the entire chat history for every single turn, which is inefficient and expensive.
Voice Tree solves this at the input layer. We convert any unstructured text into a graph and then ask LLM what nodes it would like to have in common.
On the GSM Infinite benchmark, this results in 70% fewer tokens and 15% greater reasoning
What we then discovered is that this graph itself can be more than just an optimization for LLMs.
Today's LOMs
Today's LLMs don't have true
They reprocessed the entire chat history for every single...
inefficient and expensive.
Voice tree solves this at the
We convert any unstructured text into a graph and then ask the LLM what nodes would
on the GSM Infinite Benchmark.
This resulted in 70% fewer tokens.
Input tokens and 15% greater reasoning.
What we then discovered is that this graph itself can be more than just an algorithm.
for LLMs. It's an ideal foundation for an interface for human AI collaboration, a shared memory.
I'll show you what I mean.
So right now, we're actually running VoiceTreat live on what I'm saying. You can see the transcription in the bottom.
I'll now add an agent.
into our shared workspace.
Thank you.
Maybe we can keep it okay.
Thank you.
What do you think?
All right, so today's LLMs have a pretty terrible architecture.
They just chuck the whole conversation history back in every time you send a prompt.
Boistree solves this at the input layer. We create a graph.
for any content.
and ask the LLM from that graph what nodes it would like.
We've been testing this on the GSM Infinite benchmark, and we're getting 60% fewer input tokens and 15% more accurate reasoning, which is huge.
All right, so today's large language models have a pretty terrible...
memory. They reprocess the entire chat history every time you want to send the large language.
VoiceTree solves that at the input layer we convert any type of text into a graph and then ask the LLM from that graph
What nodes would it like to specifically read?
We've been testing on the GSM Infinite Bench.
and we're seeing 60% fewer input tokens sent and up to 10% better accuracy and reasoning.
But the cool thing is is that this is actually much more than an optimization. It's a whole
new foundation.
for human AI collaboration. It's because it can become a shared memory. So I'll show you what I mean. Right now, I'm running VoiceTree live.
It's transcribing my voice in the bottom left.
And I can now launch an agent.
So it gets sent all the relevant context from the graph
And now I'm going to tell it to draw a simple moment.
All right, so today the LMs have a pretty terrible architecture primary. They just chuck the whole conversation history back in every time you send a prompt. Boistry solves this at the input layer. We create a graph for any content.
And ask the LM from that graph what nodes it would like to be.
We've been testing this on the GSM infinite benchmark, and we're getting 60% fewer input tokens and 15% more accurate reasoning, which is huge.
All right, so today's large language models have a pretty terrible architecture for memory. They reprocess the entire chat history every time you want to send a large language model.
VoiceTree solves that at the input layer. We convert any type of text into a graph, and then ask the LLM from that graph, what nodes would it like to specific?
We've been testing on the GSM benchmark, and we're seeing 60% fewer input tokens sent, and up to 10% better accuracy and reasoning.
But the cool thing is, is that this is actually much more than optimization. It's a whole new foundation for human AI collaboration. It's because it can become a shared memory. So I'll show you what I mean. Right now, I'm running VoiceTree live.
It's transcribing my voice in the bottom left, and I can now launch an agent from here.
So, it gets sent all the relevant contacts from the group.
And now I'm going to draw some format diagram.
All right, so today's LLMs have a pretty bad architecture.
They just send the whole chat history.
back in for every single prompt.
VoiceTree solves that at the input layer. We convert any text into a graph and then ask the LLM what nodes it would like to...
We've been running this on the GSM infinite benchmark and we're seeing 60% fewer input tokens.
and better reasoning.
Right now, I'm running VoiceTree live.
It's transcribing my voice.
and converting it into that graph.
And the cool thing is, is that VoiceTree is not just an optimization for LLMs.
it actually can result in um so it's like essentially just like a whole new um interaction layer
What is that bug?
Okay, let's reset to where we were
at the input layer we convert any text into a graph and then ask the lm what nodes would like to see
and better reasoning up to 15%.
So
Right now, I'm running Boy Street live.
is transcribing my voice and converting it into that graph. And the cool thing is that VoiceTree is not just an optimization for OMs. It actually can result in a whole new interaction layer.
I'll see you next time.
The current way that LLMs give themselves memory is really inefficient.
they append the whole transcript history whenever you send a new
Boystery solves that at the input layer.
we convert any graph
into a, we convert any text.
So, the current way that large language...
is really inefficient. They append the whole transcript history back into the model every time
VoiceTree solves this at the input layer. We convert any text into a graph and then ask the LLM what nodes it would like to be.
We've been looking at the GSM in...
and we've been seeing 60% fewer input tokens on average and up to 15% more accurate reasoning.
The cool thing is, what we realized is, is that this is not just an optimization. This now becomes a whole new foundational...
interface for human AI.
So I'll show you what I mean. So right now, I'm actually running VoiceTree live on what I'm...
It's transcribing my voice to text and then creating the voice tree graph.
And I can open an agent. I can spawn an agent.
Um, and then...
get it to do something. For example,
Draw me a mermaid diagram.
explaining a simple mermaid diagram explaining what this could look like.
So the current way that large-language models give themselves a memory is really inefficient. They append the whole transcript history back into the model every time you send a new message. Voistry solves this at the input layer. We convert any text into a graph, and then ask the LM what nodes would like in this context. We've been looking at the GSM benchmark, and we've been seeing 60% fewer input tokens on average, and up to 15% more accurate reasoning. The cool thing is, what we realize is that this is not just an optimization. This now becomes a whole new foundational interface for human AI collaboration. So I'll show you what I mean. So right now, I'm actually running Voistry live, on what I'm saying. It's transcribing my voice of text and then creating the Voistry graph. And I can open an agent. I can open an agent.
and then get it to do something, for example.
Draw me a mermaid diagram.
Explaining a simple mermaid, Doug, I'm explaining what this could look like.
All right, I would like you to create a new node on the graph anywhere, please. Thank you, please anywhere, please
So the current way that large...
give themselves memory is really inefficient. They reprocess the entire transcript
VoiceTree solves this at the input layer. For any type of text, we can convert it into a graph and then ask the LLM what nodes from that graph it would like to look at.
We've been playing around with the GSM in
and we're seeing up to 60% less tokens sent and 15% more accurate reasoning on the long contact.
The cool thing we realized is that this isn't just an optimization, it actually is the perfect foundation
for a new interface for human AI collaboration.
shared memory. I'll show you what I mean. So right now, I'm actually running VoiceTree live on my text. It's converting my voice into this graph here.
And what I can do is I can spawn a terminal.
and get an agent to do something for me.
It's already gotten all the relevant content.
from the graph injected into its context. So I don't need to rewrite any.
Alrighty, we are testing some things.
Thank you.
So it's going to be there, that's all right.
And where is the new file going to be? That's the question.
Okay.
So the current way that large language
themselves memory is really inefficient.
They essentially reprocess the entire transcript history for every request.
VoiceTree solves this at the input layer. We create a graph for any type of content.
and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM in
and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
The amazing thing about this is that it's not just an optimization.
It's actually a whole new foundation.
for human AI collaboration.
It's a shared memory. So I'll show you what I mean.
So right now I can open an agent on my shared memory count.
and get it to do some ink.
So here I just got Gemini to add a
A diagram.
then
I can get Claude.
to write the pseudocode for this.
So I don't need to Reprompt ever it gets a full tracing From the graph of what content could be relevant
Hmm.
So, the current way that wide language models give you is
is really inefficient. They essentially reprocess the entire transfer history for every request. Voicery solved this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its context. We've been running this on the GSM Infinite benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now, I can open an agent on my shared memory canvas.
and get to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever, it gets a full tracing from the graph of what content could be relevant.
So the current way that large language models give themselves an memory is really inefficient. They reprocess the entire transcript history on every request. VoiceTree solves this at the input layer. For any type of text, we can convert it into a graph, and then ask the LLM what nodes from that graph it would like.
We've been playing around with the GSM infant benchmark, and we're seeing up to 60% less token consent and 15% more accurate reasoning on the long context problem.
the cool thing we realized is that this isn't just an optimization it actually is the perfect foundation for a new interface for human collaboration a shared memory so right now i'm running voice tree live on my text
So the correct way that large language models can
is really inefficient. They essentially reprocess the entire transcript history for every request. Voice3 solved this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its content.
We've been running this on the GSM infinite benchmark and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now, I can open an agent on my shared memory canvas.
and get to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever, it gets a full tracing from the graph of what content could be
So the correct way that large language models give
really inefficient they essentially reprocess the entire transcript history for every request boistery solved this at the input
So the correct way that large language models can
is really inefficient. They essentially reprocess the entire transcript history.
So the correct way that large language models can be
really inefficient. They essentially reprocess the entire transcript history for every request. Voicetree solves this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM Infinite Benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now I can open an agent on my shared memory canvas.
and get it to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever. It gets a full tracing from the graph of what content could be relevant.
And up to 15% more accurate reasoning.
So the current way that large language models give.
They essentially reprocess the entire transcript history for every request. Voicetree solves this at the input layer. We create a graph for any type of content and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM Infinite Benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reason.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now I can open an agent on my shared memory canvas.
and get to do something.
So here I just want Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
so i don't need to re-prompt ever it gets a full tracing um from the graph of what content could be
All right, testing one, two, three.
Testing one, two, three
Testing one two three
That is awesome. Okay, cool. Yeah, let's definitely do that.
So what we want to do right now is we want to make some improvements to VoiceTree and the first thing is we want to make improvements.
to the output print statements.
We want to make them coloured.
So we want to color our output print statements.
I'm using the term color module.
And then you can just print colored red or colored green, for example. And the way we want to do that.
is
So what we want to use that for is, so in our output models, we currently have text that says creating a node and appending to a node.
and we want to make sure that
Those print statements are in either cyan or green. So cyan for append and green for create.
All right. So while I'm getting that to run, the other thing that I wanted to work on is just a really quick fix up, which is the sidebar isn't.
So we need to quickly fix that.
Thank you.
Yeah
Damn, it's all in love.
Thank you.
Thank you very much.
Yeah
I said yes.
I think we're out.
Yeah, whatever you think.
Give Dad some enzo to those two.
Okay, take some Enzo's, take some Enzo's, an L-theanine and an Ibuprofen, and don't have any more caffeine.
And stay very hydrated.
No, no.
Thank you.
Thank you.
It's a big one.
Okay.
Next.
I'm not supposed to be one.
Thank you.
Thank you.
We're trying to check right now if the color module
is working correctly so we're doing a live test and seeing what colors come out.
Thank you.
Thank you.
Thank you.
Thank you.
I'm not going to do that.
Thank you.
all right cool so i just want to do some quick fix-ups to our working environment um so we have a couple problems right now um so one is that in obsidian we see all our
as well. So we see all the Python and shell files.
And that's actually not the worst thing.
Because it could be cool to have like a voicetree.sh tool, which we can open.
However, it's too cluttered right now with all the prompts and Python tools.
and shell scripts.
So what's the best way to solve that?
So one way to fix it would be if we put all those tools.
one directory lower.
and opened, when we open a terminal, it opens in that environment.
Um, so the way we could do that.
is we could in our zs hrc um when
We open a terminal and we're in.
in an obsidian space.
we can cd dot dot to.
to the parent directory.
But we do need to make sure that that's going to work with our prompting system.
So the prompts.
um how do they currently work i think they currently open in
Let's see
Thank you.
parents.
Thank you.
The core dichotomy that we need to solve is that the agent needs access to both the Markdown Tree Vault directory and whatever Git repository we're working on.
Okay, so maybe the solution then is, so an agent can't enter the directory of a...
whatever directory it started in.
Yeah.
There should be one here now.
once you want to get out of this.
Okay.
Maybe we do.
Thank you.
Thank you.
Stay down.
Thank you.
Thank you.
Thank you.
Thanks.
Let's go.
Thank you.
Thanks.
Yeah.
Thank you.
I'm going to finish that.
I'm going to take a look at it.
Mom, it's a common business.
How are you? Say.
dollars.
Sorry, what is it? A thousand people like to announce $500.
Bye.
So right now
Voice trees running on this little window there. Can you see it there?
Yeah, I see it. So it's it's recording me. It's transcribing And then in this window, I can have my note
And then what happens is as I'm.
for explaining things that's that's for explaining things and then you see how a new node just appears
Yeah, that was from me talking.
And so if I keep on talking about that node see now it's I you can see that it's live because it's
It has that little text there so
like there it's in green and blue the the comments from my my terminal and then there's the actual like
nodes appearing and so while I'm
like the theory for like a minute.
The um...
the nodes will still be appearing, and the logs here are quite clear, they're colored, so they understand what's going on. They say, ah, okay, he's explaining this stuff, but in parallel to that, VoiceTree is actually running here, and you can see the logs, and you can see the nodes appearing.
No, so for the demo
You read the demo script, right?
good. So...
I think that is a big vote.
Well, it's the optimization part and then how that becomes
So right now, this is how I'll do the demo. I'll have Voice Tree running here, and then I'll have my notes for what I'm gonna explain here in the bottom half.
Yeah, they will see these notes.
I mean, they also see Voice Tree running in the top.
And so I'm explaining VoiceTree using some.
And then at some point I tell them I'm doing a live demo right now. You can see VoiceTree running.
And then I show I open like a node here like I can open cloud or Gemini you see that
And then if I open Claude,
Phone away.
So you don't have to, you don't have to.
No, I don't need to write it it automatically
And then I'll show them. I'll show how it...
Anyway, so it gets all the content.
Um, and then I can.
Yeah, it gets all the nodes from the chip.
Actually, it's not working right now.
But it does, and I can show them that.
Um...
And then I'll get it to like do something like write a mark down.
Like to draw like a diagram or something something simple now just paste it in so I'll just do like
draw
Like okay, I haven't copied someone
So if I just say...
Add a new node.
So I'll just paste that in, which just says add a new note of mark.
And it will all work, so there's some bugs right now, but I'll make it all work. Yeah.
Um...
And then...
It's going to add that node. And then what I'm thinking is either I switch over to
Example
Um.
that we were talking about, that I have at the end of the script.
Let me see that one again, one sec.
what did you mean by that four different screens so you're going to just have a lot of different agents doing
No, so, hang on.
So there's two things I can do. So one option is I continue using this Example this like this. I don't I don't restart. I just use the same example
And I just now say, okay, now I'm going to engineer an actual feature in VoiceTree, continuing off the same VoiceTree.
And so, then I'll...
And then after that we speed it up to like 10x so you hear me talking really quick
and the nodes appearing.
um and then we we stop at some point where i'm like now watch this and i get um i get one agent to decompose a task into subtasks and then run all the sub agents on those subtasks
Um...
But is this all possible in three minutes?
that it would have to be really sped up.
And just showing them like, or, so that's one alternative, is that I continue using the same tree to show like a real world example.
Or I can switch over to some real world examples of me actually using it for engineering, sped up and just stop at key moments.
No idea to develop a narrative.
Yeah, yeah, I agree.
of the context. Yeah. And then I can have 10 seconds at the end.
like so like split screen right like like as in there's four screens
And it's just like four videos of me using VoiceTree for work.
So I'll show you an example.
Like if I open
so
So I can show like some of these really sped up examples of me using
VoiceTree to do engineering work.
Yeah, but I think that's a bit true.
But if we just have it for 10 seconds like split-screen for four ones of these and just like just to say Just for the one sentence of me saying I'm addicted to using voice tree engineering has never been this productive nor joyful before Just for that just for that one second
Yeah, nice yeah
Pretty cool, right?
Because I told him, like, no worries, you don't have too much time. Even a quick glance could be useful. And he said, I'm super interested in the project. Just want to be able to give it my full attention in time. And then he says he spoke to a patent lawyer, a friend of his, and they told him it makes sense to file for patents in the countries you will want to sell to.
I guess these are things for Y Combinator.
Okay, so then ideally I should be sending him this this draft of just the text or send him me me doing an actual example like I can probably the next hour get a pretty okay example of me doing it
Yeah, we have some problems, of course.
It's not going to be the... Yeah, that's okay.
Yeah. Do you think that's a good use of your time right now? Is that what you want to focus on?
Getting the demo done
I mean, doing kind of a mock video.
Oh, I mean...
just be practicing for the real video yeah i mean the real video is just gonna be
they record like the best version
whatever the best one is so you just want to keep track
Yeah, but I mean if we record it early and send it to Zarin, he can give some advice. Yeah, exactly, I would do that. Yeah, and say this is maybe, I'll just tell him this is the draft. Do you want me to, do you want to send me Zarin?
Okay, so do you think that's the perfect flow like as in
what I sent you and then what I explained.
What it looked like
Yeah, dude, it makes it sound like it's a really...
Hmm.
So one thing is that I've completely stopped work on the GSM bench.
And it would still be really cool to show like an image
benchmark results.
Yeah.
But don't you have already one that you can show?
I mean...
I've stopped work on it. I think I could come up probably in three hours of work, I could get a nice graph that's actually accurate.
Um, but there's a lot, a lot of.
But I think that's fine. I mean, they just want to know it's promising not that
Let me just check exactly what time it is. It's 5 a.m. Tuesday. Yeah. I think I'll be staying up.
I don't know. I mean, hopefully not, but in case.
All right, 5 a.m. Tuesday, one day left. Exciting. Okay, so can you read the...
Yeah, I've read it. I think it's great. So you think it's perfect just just make that work
What are you going to be showing in the first part of where all the text is?
Before you say, I'll show you what I mean.
So the whole time.
So the whole time it's going to be.
This screen, right? This is the screen I'm recording
My face will be in the top right
And here you see that there's like
Um, there's... What is that?
Well, it's just drawings so I can just draw whatever I want there
So, and I can use that to explain what I'm talking about.
So I could have like some little, you know, circles or whatever I need to explain, whatever I need to explain. Yeah.
and then and then when I'm done with the explanation I drag it away and it's just the
Awesome, he's perfect.
Amazing, amazing, amazing.
Yeah, I don't know. This is just so much better than...
other ways of doing it where I have like a different slide and then I switch to this slide and then they're not sure.
progress you've made in the last week I mean imagine with a few more months to work on this if we don't get this round you have six months to polish it well so what happened is right is that the past year and a half I've just been working on the core algorithm
And in this past two weeks, I've just been working on like UI, which makes it look a lot better, but nothing's actually that functional.
But it makes a huge difference for...
But it's genuinely gone to the point where I prefer using it now to other ways of working.
Yeah, it's so cool, so cool.
That's awesome, dude. Yeah.
It's your own invention. It's really cool. It's so cool.
Okay, I'll try. So what's my priority to get to get you a draft of that?
And Zen.
Yeah, yeah
Yeah, perfect.
Exciting stuff, Manu. Amazing. All right.
Um,
Um...
So, can you hear me?
Now I can hear you. Yeah, you're back. What I want you to work...
Um.
Well, uh-huh.
Well, uh-huh. So when I send you the draft...
You could do the editing.
Um,
I don't know. I don't know. What's...
What do we need once like the demo when you be edited slightly like when you speed up parts of it
Yeah, but you'll have shot it, so you know every single line you said. I think you should do the speeding up, because you'll know what you want to, while you're making.
It'll just be inefficient for me to figure that out. Okay, just stay on standby then.
Yeah, that could be really good. Maybe out of all, you know, I've sent you lots and lots of prompts. I mean, like example scripts and stuff. Maybe we can fill it in a bit more.
yeah and so and any of the personal stuff
Don't worry about filling it out like a time. I've um like hacked something like I'm thinking like they all talk about like biohacking
that's kind of cool and interesting
Um, that I haven't done.
and you could think about maybe because like in that one minute founder video we can i can mention some things about voicetreat to make the next demo more understandable
Because I'm sure they just append them.
um
So maybe you think about if like the whole narrative of like Founder video plus demo video plus written application if it all really makes sense and nothing's left really confusing And they're not like you know what I mean like as in the whole thing is a cohesive story piece of narrative
Best way to see this is if you already have drafts, you know, not the final version, but if you just take the one minute founder video, have mom record it up on the roof.
Or just, I mean, behind with the bookshelf, that looks good, though.
And then Zeronomy can feedback.
Okay, okay, cool.
I will...
I'll record, I'll try to record all the drops that we need and send them to you ASAP. Nice. Okay. Should I make a group with Zarin? Yeah, yeah.
All right, bye. All right. See you, man. Bye.
All right, cool. Can you hear me voice, Tri?
All right, we just need to get some little fix-ups working for our demo. Most things are already working. We just have a few things we need to do.
Um,
So for the demo
Something we need to do is...
for new node placement consider all other existing nodes in the graph so that like right here they don't appear too close to each other
We also need to stop doing restarting layout after we drag a node. That's been really annoying.
And I know that there's somewhere in our code base there's layout start.
stop and we we just need to comment that out so that the layout doesn't restart
Okay, and then the other thing we need to work
is that
Um,
Currently, we've somehow broken the Claude.sh.
So it used to get the whole Python dependency.
and now it no longer does.
So we need to fix that.
You
Alright now I want to figure out
When we're going to go to the gym.
We want to go to the gym. Let's check when the gym's closed.
So it's a Berlin strength.
Maybe it's not open on a Sunday.
Okay, so we fixed the ClawDOSH broken functionality. Let's just quickly check that it's also working for Gemini
Nice looks like it's working
Cool, okay, so now we just want to make sure we need to work on optimized new node placement
Um...
Let's actually yeah, so let's get an agent to work on
Restarting layer.
Let's do Claude.
Okay, and now we want to work on Optimize New Node Placement. Yeah, clone.
Okay, let's add to a list of things we won't do but ideas
Could be cool, but we're not going to do because you know
So one thing is a way to actually delete a node and then have it deleted from our tree data structure as well.
the other thing we wanted to do was to be able to...
collapse a node and all its neighbors.
into one container node.
That's also a won't do task.
All right, so now we need to...
what exactly we need to be able to do the demo where we do agent.
All right, so now we need to consider what exactly we need to be able to do the demo where we do agent orchestra.
So, I guess the first thing we need to do is to be able to...
So I guess the first thing we need to do is to be able to...
So go down a chain of nodes.
So go down a chain of notes.
have those nodes created.
have those nodes created.
And then...
Um, and then...
run an orchestration agent.
run an orchestration agent.
Um...
So to run an orchestration agent, we just give it the specific orchestration prompt. That's easy and then
So, to run an organization agency.
the specific orchestration prompt that's easy and then
So we'd give it the orchestration prompt. It would turn, it would create the four subtasks.
So we'd give it the orchestration prompt. It would turn It would create the four subtasks
And then on each subtask.
And then on each subtask.
We could uh
we could uh
So in each sub-task, we could then run the sub-agent.
So in each sub-task, we could then run the sub-agent.
and
And...
the sub-agent
the sub-agent
Hmm, hmm, hmm, hmm.
Hmm, hmm, hmm, hmm.
Because before we didn't really have it completely integrated into our existing infra So the old way of doing agent orchestration was to create a new sub
Because before we didn't really have it completely integrated into our existing infra. So the old way of doing agent orchestration was to create a new sub-agent shell.
script. But now, ideally...
But now, ideally,
Just the subtask itself to contain all the information.
Just the subtask itself to contain all the information.
So we can try that in a minute
So we can try that in a minute
Um,
Um...
But for now, I just want to make sure that our new node plays
But for now I just want to make sure that our new node placement logic is okay
is okay.
Okay, so.
Okay, so...
Give me one second.
Give me one second.
I'm recording.
I'm recording.
Okay, there's another problem with Gemini is that the ampersand doesn't actually inject the prompt file So we need to actually inject the content With an environment variable, I guess
Okay, there's another problem with Gemini is that the ampersand doesn't actually inject the prompt file So we need to actually inject the content With an environment variable, I guess
Let's add to our won't do list
Um, let's add to our won't do list.
that when we close a terminal
that when we close a terminal,
that when we close the hover editor for a terminal that the actual terminal node is also
That when we close the hover editor for a terminal that the actual terminal node is also
It cleared, deleted, hidden.
It cleared, deleted, hidden.
okay another won't do problem that we've noticed is that um so we just made uh so two orphan nodes just created got created actually three orphan nodes
okay another won't do problem that we've noticed is that um so we just made uh so two orphan nodes just created got created actually three orphan nodes
15, 16 and 16.
15 16 and 16 and so two files
And so two files...
with the same
with the same
Node ID got made.
node ID got made.
And so we need to figure out exactly why that
and so we need to figure out exactly why that
That's a bug
That's a bug.
you
Okay, so right now we're trying to figure out exactly what sort of layout logic and physics we actually want. So one thing I realized is that if I drag a node, ideally I would like all the nodes that it's connected to, to also be pulled towards that node.
Okay, so right now we're trying to figure out exactly what sort of layout, logic, and physics we actually want. So one thing I realized is that if I drag a node, ideally I would like all the nodes that it's connected to, to also be pulled towards that node.
Okay, so one thing we need for our orchestration mode is we need it to somehow be able to click
Okay, so one thing we need for our orchestration mode is we need it to somehow be able to click
Also children.
Also children.
Um...
Um...
So maybe we can do like dependency traversal can do.
So maybe we can do like dependency traversal, can do dependencies.
dependencies.
Plus, go find...
Plus, go find...
Find the children of that node
Find the children of that node.
Um,
Um,
Um,
Um,
Uh...
Uh...
Yeah, yeah, we just need to then also do dependency traversal on the...
Yeah, yeah, yeah, we just need to then also do dependency traversal on the
Children of the node that we're running on and the children's children's children's children, etc, etc, etc
children of the node that we're running on and the children's children's children's children, etc.
One thing on our non-actionable ideas list should also be that our tree should be able to reference nodes which an agent has created, not just the ones that we've made manually.
One thing on our non-actionable ideas list should also be that our tree should be able to reference nodes which an agent has created, not just the ones that we've made manually.
You
Again everything we're going to quickly add to our non actionable ideas list is that for some reason
Again everything we're going to quickly add to our non actionable ideas list is that for some reason
Gemini is not using the same color for nodes.
Gemini is not using the same color for nodes.
Um, so we need to just ask it why.
Um, so we need to just ask it why.
Oh, good.
The Book of Al-Rajanesh is out now.
Oh good, the Book of Arrajneesh is out now.
in the shadow of enlightenment
The Shadow of Enlightenment.
Mm-mm.
Mm-mm.
I don't know.
I don't know.
Oh
Cough.
My darling.
My darling, is that you?
No.
Yeah.
Yeah.
You
Mm-hmm.
Mm-hmm.
Thank you.
Alright, alright, alright, alright.
Alright, so while working on that, we made a non-actionable ideas list.
With all the little things.
up but we didn't want to get distracted.
And now we're going to get.
Claude to act as an orchestrator
for creating all those stuff
So let's just, so now we should be able to say.
So it's one of the...
I'll see you next time.
Okay.
What are you going to do, have a run?
Just do a bit of exercise.
I'll see you next time.
We'll see you next time.
for new node placement, consider all other existing
Nose in the graph, so that like right here they don't appear too close to each other
um we also need to stop
Alright cool, we're going to open our voice stream.
That comes out.
All right, we just need to get some little fix ups working for our demo. Most things are already working. We just have a few things we're going to do.
So, for them.
Cool, can you hear me voice three?
We just need to get some little fix-ups working.
Most things are already working, we just have a few things we're going to do.
so for the demo something we need to do is um uh for you know placement um consider all other existing nodes in the graph so like right here they don't appear too close
We also need to stop doing restarting layout after we drag a node, that's been really annoying. And I know that there's somewhere in our code base that there's a layout.
And we have to comment that out.
And I know that there's somewhere in our code base there's
And we just need to comment that out so that the layout doesn't restart.
Okay, and the other thing we need to work on is that currently we've somehow broken the clod.sh. So it used to get the whole Python dependency, and now it no longer does. So we need to fix that.
The other thing we need to work on.
Restarting layout after we drag a node, that's been really annoying. A Python dependency, and now it no longer does, so we need to fix that.
So we need to fix that.
I want to figure out when we're going to go to the gym. Let's check when the gym's closed.
So it's Evelyn's rink. Maybe it's not in a Sunday.
All right, so we fixed the cloudless edge broken functionality. Let's just quickly check this.
Nice, looks like it's working. Cool. OK, so now we just want to make sure we need work on optimized.
So it used to get the whole Python dependency.
Python.
All right, so we fixed the Clotus Edge.
Nice, looks like it's working. Cool. Okay, so now we just want to make sure we work on optimizing node placement.
Let's actually, yeah, so let's get an engine to work on resetting now.
And let's add to a list of things we won't do, ideas that could be cool, but we're not going to do because of priority. So one thing is a way to actually delete a node and then have it deleted from our tree.
The other thing we wanted to do was to be able to collapse a node and all its neighbors into one container node.
That's also one of the new tasks.
Alright, so now we need to consider what exactly...
the demo where we do agent orchestration.
So we can try that in a minute.
That's me.
Gemini is not using the same color for nodes, so we just ask it why.
to add to a non-action ideas list, is that for some reason...
Bum!
Thank you.
Currently we've somehow broken the...
cloud.sh
So it used to get the whole Python dependency, and now it normally does, so you fix that.
One running.
All right, cool. Can you give me a voice drink?
All right, we just need to get some little fix-ups working for our demo. Most things are already working. We just have a few things we can do.
So for the demo, something we need to do is, for new node placement, consider all other existing nodes in the graph, so that right here they don't appear too close to each other.
We also need to stop doing restarting layout after we drag a node, that's been really annoying. And I know that there's somewhere in our code base there's...
stop. And we need a comment out so that the layout doesn't restart.
Okay, and then the other thing we need to work on is that currently we've somehow broken the clod.sh. So it used to get the whole Python dependency, and now it no longer does. So we need to fix that.
All right, so we fixed the clodis edge functionally.
All right, so we fixed it on this edge.
and now it no longer does, so we need to fix that.
All right, so we fixed the clodis edge button functionality.
So it used to get the whole Python dependency.
Ice, ice we fixed it ice we fixed the glottis edge
All right, so we fixed the cludders. All right, so we fixed the cludders. Edge broken functionality. Let's just quickly check this out.
As we fix the clodest edge functionality.
OK, so we fixed the cloudless edge broken functionality. Let's just quickly check this.
One ring.
as we were sharing.
I was six years old when I was missing a day.
All right, I want to figure out when we're going to go to the gym. Let's check when the gym's closed.
So it's evidence-wing.
of followers of the Indian Guru.
Bye.
Children like me.
All right, so we fixed the Clodos H broken functionality.
Nice. Looks like it's working. Cool. Okay, so now we just want to make sure we work on optimized unit placement.
Let's actually get an agent to work on restarting now. Let's do Claude.
To know
You
Thank you.
Thank you.
Brrrr!
Thank you.
Hmm.
See what mom's doing.
Thank you.
Working out.
I'm going to have a run.
Maybe, huh? Maybe.
You
I don't know how it works.
work
Crashing a bit
Yeah, I've got some great ideas.
Very useful.
So current large language models performance becomes worse as their context length.
Attention will always be complicated.
You only know for sure of a new piece of
by checking each one. There's quadratic complexity.
VoiceTree solves this by optimizing the input itself.
We take any text and progressively convert it into a graph containing those concepts and the relationships.
Then, instead of repeatedly sending LLMs the same appended unstructured text, we work alongside the LLM to prune to only the relevant sub-tree.
The results we have been seeing are amazing. We're seeing 70% fewer input tokens on average and 15% greater reasoning on the GSM Infinite benchmark.
Moreover, VoiceTree provides those same performance benefits to humans.
So humans, too, suffer from context degradation.
And in fact, what we can do is.
We can...
Remove this.
Entirely.
and instead just work on a shared memory graph.
I'll show you what I mean.
So what you're seeing right now has been running live.
I've been running VoiceTree.
on my voice, and then it's converting it to text and then to this graph.
I'll now launch an HR.
Now, so what you can see here...
Oh, what the fuck did I do here?
ah
LOL.
All right.
So LLM's performance decreases the longer their context gets. This is called context degradation. And the key reason for this is because they reprocess the entire context.
VoiceTree solves this at the input layer. We optimize the input itself. We take any text and progressively convert it into a graph containing the concepts and the relationships
Then, instead of repeatedly sending LLMs the same appended unstructured text, we work alongside the LLM to prune to only the relevant subtree.
The results with this have been amazing. We're seeing 70% fewer input tokens.
and 15% greater reasoning accuracy.
I'm the GSM Infinite.
Moreover, VoiceTree
provides these same performance benefits to humans. Humans too suffer from context degradation.
And in fact, the really exciting thing is
Why don't we just remove this entirely?
and work only
with the graph presentation. Have the graph as a shared memory.
between you and AI.
i'll show you what i mean so what you're seeing above is voice tree actually running live um so it's converting my voice to text and then updating the graph
What I can do now is I can spawn an agent.
And tell it to do something, like for example, create a mermaid diagram explaining how this could work.
So the agent
got only the relevant context from the graph.
Now that it's finished its task.
it adds a new node back to the graph.
Thank you.
Thank you.
Thank you.
Thank you.
Hmm.
Thank you.
Thank you.
We'll be right back.
Thank you.
We'll be right back.
Thank you.
Yeah.
Thank you.

Okay, git checkout main
And now let's see
If we have...
Sure verse.
Okay, here we have it
okay so now we want to make sure that this
Scripts, we found it.
um
Um...
Let's also load the latest
Um...
So let's go
Build.
And then let's go.
Um, okay.
So, I think, did we just run films?
160
Reload juggle.
So we have now this
tool.
Um...
graph dependency traversal and now let's say find where we use it
Okay.
Okay, let's restart my computer.
I'll see you next time.
You
Thank you.
Okay, let's end this.
Hallihallo!
Hey, I'm doing, um,
Recordings.
Let's go.
What do you want to do though?
No, I was um, I just stopped my recording, but I'm doing a cool recording like this
Oh, cool.
Can I have a look?
That's what it looks like.
Well, it keeps on flickering. Can you turn my phone on?
I think it's out of battery.
It does not go on? No, it's okay.
Mm-hmm.
I'll see you next time.
Hmm
Okay, and then let's have the logs there, so.
All righty. Okay, so let's see.
if it can now understand that there's a tree loading integration.
can that we just built
So we want to understand if that works.
So
Let's see, creating a new node.
Hmm.
All right, so we've just
Um,
So we just added functionality to the load in existing tree.
We want to see if that works now.
So tree loading integration
access that node
Hmm, okay, let's look at...
Voice tree.log
Alright, did we just...
Mark down.
3 volt
What the fuck is going on?
let's see ready to listen
I have you loaded
loading tree.
Loaded 18 months.
I love you.
Okay, so we should be able to just add that there.
Okay, so we want both a logging and a print statement.
Okay, that should be sufficient.
Okay, cool. Now it's working
So if we're talking about the tree loading integrator.
can it add a new node there?
Um...
Let's see
Yeah, awesome. Okay, that's pretty cool.
Alright, so that's first feature done.
Let's just check the video.
All righty, that is done. Okay, so cool now we can actually work on voice stream. So we had
Um...
What did we have? We were going on a couple of things.
Oh, yeah, the two problems with voice.
So we want to work on the terminal default height issue.
And the, um,
graph flickering issue.
Um...
So...
Let's work on the Terminal Default Height issue.
because that's going to be easy.
Thank you.
Okay, so if we open
A terminal here now.
Cool.
Oh, actually.
Before we do that, we.
Um...
We wanted to look at if we could.
Let's hide some of that
All right
Hide.
All right, so we wanted to see
Um...
So we did that
And now we wanted to see.
Okay, so that worked, cool.
We can...
Clear that node
Okay, so we have demo complete.
Um...
Okay, so...
What do I actually want to do right now?
Um...
Oh, yeah, okay, so we were working on...
the ability to do a graph traversal search.
So that's this graph dependency traversal tool.
So if we open that up...
And we can run Claude.
Okay, and then...
let's make that a little bit bigger
Um...
Um, okay, so...
We have a tool
Voice
Tree
Oh, okay
So graph.
Get a tool for this.
Um...
What do you want to do now?
So we want.
So what do we want actually to happen?
Whenever you open
from a node.
We want to do a graph.
On that node to its parent
So up through its dependencies.
One second.
Hello?
Okay, I hope it's time to go to the side.
Where are you at?
All right, we just had a packet come.
um so yeah what do we actually want to do regarding the
dependency traversal tool.
Um...
So...
Um, we want...
Returning only seven nodes from select-
Why would that be?
Oh, because...
Right. Nothing was relevant.
Hello?
links
Sorry, we're still Orbin or...
Oben oder unten?
Unten.
Robin, okay, I'm going to go to the right side.
Thank you.
Oh man, fucking door.
Um,
Right, so...
What do we want to do we want
Um...
when
So instead of just getting the parents' content...
we want to give the full graph dependency traversal.
So, currently...
when we open Claude.
Oh, come on.
When we...
Okay, so in...
markdown tree vault, add new node.
No, here.
Demo prompt dot wait
Ah, okay, so.
Um...
So we have currently in.
We just load
So we have pump is.
Doom.
Wait.
If you want.
Injected in this prompt to be the full.
I propose a way to do this.
Okay, so I'm running an agent
right now on
So that's running.
Okay, and now I realize that
Um...
when the graph moves
Sometimes the terminal isn't moving.
Do that. That seems to work
But I guess if it happens while it's moving.
Um...
Okay.
So let's see what's decided to do
Okay, so now let's
open
A Gemini instant.
Um...
So
We are getting lost in the tree a bit too easily.
That is one problem because things just sort of flicker around too much, which is...
The second problem we need to...
Okay, so...
Let's um
delete that
So we have here and here
And now we want to go Gemini.
Okay.
And let's get it to review that.
so this is
forward dependency graph traversal into cloud SH.
Let me see, review this.
proposed
navigate to
All right, let's get more.
Okay.
And now let's go.
Opus to work on that.
and say,
review this
this
Also look at the neighbors.
So this does make me think that we're all
have to be able to see
Oh my god.
So annoying zoom.
Um...
So let's see
Um, okay, so...
There's a problem here with the markdown.
Let's, let's get, uh, do it. Okay.
Thanks.
Make sure they're known.
problems with this.
And then exit.
So this guy's saying that the clod is
the build is failing.
So when you figure out why the builds
And what does he actually change?
No, we don't want that.
So let's say no
Shut the man.
So we can't reference files easily
Um...
Yeah, it is a problem. Voice trademarked entry vault.
I'm going to guess. Oh well.
Okay, so let's test if that works.
um so we can
So we can open up a terminal here.
Claude.
Let me see, do the same for Gemini.
And then we want to see.
Um.
There you go.
Enhanced.
So we want to loop.
Okay.
You
I'll see you next time.
Okay, let's see this guy.
Okay, let's see what this guy did
Don't want that action.
Do you need to be able to specify a...
folder for it to work in.
Um...
So maybe
We could do like...
I don't know.
Claude sh
Folder like that
It's not a bad idea
Okay, cool. So now we want to work on a new task.
Um, actually, let me just see if I can open it.
Okay, that did actually work.
Okay.
The flickering is really annoying.
Okay, cool.
So today we're working on
And there's a few different things we want to work on.
um, mostly related to getting, um,
A live demo, looking really good.
Um...
And so some of the improvements we want to do for VoiceTree right now.
Uh...
Um...
So one thing is when a second node appears on voice tree
on the juggle UI.
a
It flickers, the whole graph, the location of the graph.
changes and you no longer see it in the notes.
So when the second note appears, about three seconds later,
Um, there's a large, uh,
um flickering issue so we want to
fix fix that so there it just happened you just saw
Okay, the other thing we were working on.
Um...
was for the demo was improving the graph traversal to the LLM.
Um...
So what do we want? So right now we send a full graph traversal.
But we also want to.
inverse document search
algorithm to find other potentially related nodes.
to the user's query.
Um...
So what is that going to involve?
Um,
So that's going to involve, um,
So when you run
dot sh.
we're going to
So we currently give it, we run the Python.
tool to do a graph traversal.
and we give it the full traversal of the current node.
Other functionality we want
So on that current node,
we want to do a inverse document search.
Against that current node
um
to all other nodes
in the folder.
Um...
Bet that note is in
And also send back, like, for example, the...
10% most related other nodes.
Alright, so what are some things
agent will need to know to work on this
So...
Um, the Python script is in our, um,
root repose directory.
And then there's some related
Files.
in VoiceTree for doing inverse document search.
Um...
And...
Um...
That will be useful to know as well
So let's see here.
Let's go, let's change the link here.
Um...
So let's make this.
Define inverse type in switch scope.
So can we edit a node here?
Okay, so there we edited the voice tree slightly
Um...
And then we have some...
Okay, so I'm going to also add to inverse document search invoice tree.
Um,
Other useful uTools
All right
There
So
Okay, so yeah, so you have access
to the tree functions file.
get most relevant nodes.
But for now, to keep it simple, we don't want to load the whole market.
into a
decision into a tree data structure. We just want to replicate the inverse document search just without loading it in.
tree
Um,
Cool. Does that make sense?
Okay.
So we have our dependency graph now.
Um, okay.
at relevant node retrieval.
I'm now going to launch an agent.
And let me just see if it has all the context.
um
So we'll open an agent here
And we'll go claw.sh.
And let's just see what context.
Um.
do you understand how to perform this task
What other context do you need?
Okay, so that guy is exploring.
I'm going to add the the the the the the the the the the the the the
Let's do it.
Okay, now I'm going to get...
Gemini to review his work.
review Claude's work
Okay, and then another thing I'm seeing
now is that when an agent
Adds a node to our graph
Um, the colors are not that.
Nice.
um sorry the it makes it as a
as a header.
But we don't want that.
Um...
So we want to improve that so that when an agent adds a new node to the graph,
um the formatting is better
Okay, so let's see.
You
Okay, it looks like these people are done.
Okay, now I do want to work on the...
Add New Node Tool.
and the Node Formatting Pro.
um so let's just go there
Uh, open Claude.
Okay, I'm going to do a quick fix-up toss.
which is to combine Gemini
into one script so they don't have to keep on editing them both.
Okay, so cool, we've done that task.
We can just...
Um...
I think it's we can
Collapse selection.
Cool that guy worked
So let's do.
collapse on him as well.
let's actually get um
Gemini to work on.
Cool. Right, we've done that feature.
Okay, let's just end that
Commit.
Thank you.
All righty, cool. So we're working on voice.
We're working on making the live demos right now.
And we're just making...
some small improvements to the VoiceTree system to make the demos better.
Um...
Okay, cool. So one thing we just fixed is that when you run in a
You now get
a
So when you run an agent
an existing node.
You get.
um, a graph traversal.
And it also does a TF-IDF search for the most relevant other nodes in the tree.
Think let's make our voice settings
slightly longer
So we want in our...
Voice to text config we want a bit more of a pause threshold
0.9
Cool, so that's what we've been doing and now we want to fix a few things with the UI.
that have been really annoying for VoiceTree.
Um, so one thing is, is.
Um, one annoying.
UI element.
of VoiceTree is that when a new
the graph restarts its layout.
And that can mean that things sort of fly around.
much too much.
And so we need to think of a general solution to that so that things stay put in place.
Um...
So let's think about what possible solutions could be.
for the VoiceTree Graph Layout Instability.
Um...
Oh, okay, before I start about that, I just want to talk about another VoiceTree UI issue.
which is that newly appended nodes
Um,
get an animation for 15 seconds.
but for some reason the timeout isn't working.
And the animation appears for longer than 15 minutes.
So let's figure out how to fix that as well, it's another task.
I think...
But it was working for the, when we added a timeout.
create animation it was working working I know at some point we had the time
but one of our recent changes must have regressed it.
Okay, let's keep on thinking about solutions for the...
Voice tree graph layout instability.
So one thing is, is that we could, one option is to completely disable
the layout change and juggle.
And I think that's something called the cola layer.
Animation.
And if we do that, I think it's actually fine.
Um,
We just need to make sure that our new notifications
are generally correct.
So we'll sort of have to replicate the cola layout.
but only for new nodes.
So where are we putting you?
the decision for where we put them.
Um...
needs to sort of replicate color logic so that our graph gets built in a nice way.
Or, I'm thinking now,
Cola layout.
only run the layout on the new node.
can choose some nodes and but then I think that gets too
Um...
So
Okay, and now we're talking about another VoiceTree UI fix.
which would be a really nice long-term solution for the current problem we have.
um, with terminals on the canvas.
Um...
behaving really badly when they
move or when the graph zooms.
Um...
And...
what we want there
Um,
is potentially want to represent the terminal hover editor.
itself
as a
Node on the graph with an edge connected to it that way you can always connect
Terminal.
to the node it was opened from.
So currently we have sort of like a
A very, um.
Oh, okay. Actually, I think maybe we're pretty close to having that working.
So actually a related fix so how we could fix that is if we're disabling voice tree layout changes
What that means is
is that new nodes
I'm sorry, that moving node
Um, it would just, whatever, wherever position.
put the new node in manually, it will just stay there permanently.
And what that means is there's no unexpected node movements other than the ones we're doing manually.
And what that means...
that then
if the terminal, if we put it over
then it's only going to move.
if the terminal node moves.
which is the current behavior that works well.
So I think.
we actually have a fairly good solution for that.
All right, cool. So now we want to see if
Um,
if both the new node animation
working now that we've rebuilt.
Um, and also the other thing we want to.
is if
Actually, what did we just work out? Oh yeah, if adding a new node changes the...
animation at all.
And I just saw that it was still happening.
So we can open and develop our console.
Um,
Okay, so we want to see first if we've opened the right.
Plug-in
1.59, okay.
Let's unload that
That is pretty annoying.
Okay, so...
Let's start running our agents to give it the feedback that it didn't work
Okay, cool. Let's rebuild.
juggle.
And let's check if the animation is working.
Um,
and
Look at what.
So we can.
Reload juggle.
So let's make sure we're on the latest version.
Oh, okay.
I see, I see, I see.
We have a problem.
after we've renamed the repo. So let's go voice...
So I think the problem was that we weren't building.
which is why we weren't seeing the animation.
Okay, let's see now it should work
Okay, cool.
let's open up a new graph
and let's try the dragging functionality
Um...
So it works somewhat well.
And now let's try create a new node.
What we're gonna do what we're gonna eat for dinner today. I'm not sure yet exactly what we're gonna eat for dinner
So I want to figure that out.
Um...
And now we want to...
Cool, so it looks like we fixed it. The animation stops.
After 10 seconds or so?
Awesome.
Um...
And it also looks like new nodes don't really force much of a layout.
Which is.
Pretty awesome.
Um...
so that we can close.
Um...
All right, let's get while we're reading that another agent to get to work
So that's, we're going to have.
um, here a
So we have another UI problem.
Um...
And...
That UI problem
that when you open
terminal
the initial size of the terminal.
is a bit too small.
Um, and so we want
Um, the terminal.
when it's converted into a hover editor.
to be slightly bigger.
So we'll get an agent to work on that as well.
Um, so that
problem of the initial size of the terminal.
is when converted to a Hover editor is too...
we want that to become a new node.
Ah, so there we just saw that our position still flicked.
so we need to definitely get that too
Get an agent working on that.
When and you know.
Our viewport can still flick.
to a completely different...
Okay, we just want to check right now if we append to new content.
Does that result in...
Um, um,
Uh, the...
animation not being stopped by a hover.
So that's what we want to figure out right now.
Um,
Okay, so it doesn't stop on hover.
So we want to disable that.
So append animation.
Stop is now done.
Um...
Cool, so the agent thinks they found the root cause for the viewpoint. Flicking.
You
Okay, so now I remember that we have a lot of tech
that in terminal hover editor positioning.
So this is related to terminal size problem.
we want to make a
Um,
to clean up that code and get Gemini to inspect it as well because it just has so many bugs.
All right, so in that existing tech for terminal hover editing positioning
we want to make a new node about
Improving the
Uh,
have our edited positioning.
code because it has currently lots of bugs.
That I'm recording here
Yes, please. Yeah, thank you. But you know that phone
Now the phone.
It's recording me.
The phone.
Well, if you get in the...
in the way. Yeah, thank you.
Thank you
I'm just bringing you a drink.
I'm realizing right now there'd be a really nice
for voice tree if
Um,
If the agents can also modify.
existing nodes.
which they can actually do, I can just tell them that.
Or I guess the other solution would be to have.
Voice tree nodes.
Actually,
That to include the agent node
in our tree, so where we can append to them.
the nodes that the agents themselves have made.
which we currently can't do.
You're here.
You didn't tell me I could come out now?
Hey, wait, I'm recording!
Okay, just give me five minutes and I'll stop
Oh
Done
Okay, cool. So today we're working on VoiceTree, and there's a few different things we want to work on.
mostly related to getting a live demo, looking really good.
Um...
And so some of the improvements you want to do for VoiceTree, right?
Okay, okay, okay.
not completely. I'm going to make it something quickly to eat and then I'm going to film after. Can we see what you've done?
Yeah, when?
Now, okay, we're just going to shake it.
Are you ready to eat something Manu?
Are you ready to eat?
Thank you.
I, uh...
Do you want, I want to make pasta and I have...
Fake meat.
Is that okay?
Was that too boring?
Yeah
What are you liking?
Very good.
Perfect.
Hand from hand from.
It's really keen-shaven and with a little Italian beer.
by the time we were just
I like an Italian beer, but you had a very easy...
Right, did we just load up?
It's not?
But it's the same people. It's the same brand, yeah.
Yes, you can see so much what the other one said.
Tristan's reading this.
And I don't see something else, don't they?
do this.
Follow Beagle
Yeah, ask you to go.
Do you want um
Let me set the table here.
Oh, yeah.
We'll just take everything up.
We have been wondering.
I've watched it a long time.
We're in there.
What's beautiful up there?
There's nothing in the washing machine. Are you sure? Oh yeah, no there is.
There's stuff on the floor. What is that?
Should we do it now?
I'll do it, I'll do it. I have to add my white chalk.
Why don't I put this in the dryer?
It's just going to make noise.
When he's finished the recording...
Yeah, I'm finished recording for now.
soon.
3 load
Thank you.
Roll that in the back.
Yeah
The front stuff on the floor everyone.
Honey, but how's it going? What are you doing?
I was filming live.
use cases of voice stream.
Yeah, you gotta work down.
It's great. Yeah, it's really good.
so
So with that, not enough.
edit that into with your voiceover.
I mean, I don't think Elon's going to do anything.
He's at a party right now.
I don't, I'm not counting on Ilana Dooney.
but I need something. Okay.
I need to add the thing in
going for yoga.
You can get it if you want.
Where is it?
Fake news.
Right top.
Yeah, and foot wash, you need to...
I'm going to put it on my full wash mittel.
white.
And one of those calcium...
What do you want me to use?
I want you to use a full wash method, two tablespoons.
Not that the length of my day.
have good pressure.
Peruvol
Careful with this one.
I bought two new boxes
I'm going to do a little bit of a bowl of water.
the white and the other
Okay.
For the wash metal, two tablespoons.
Yeah, yeah.
One tablespoon.
Two tablespoons, and what else do I put in?
The thing that says we've used the power systems.
Today's LLMs don't.
memory. They reprocess the entire chat history for every single turn, which is inefficient and expensive.
VoiceTree solves this at the input layer. We convert any unstructured text into a graph and then ask the LLM what nodes it would like to have in context.
On the GSM Infinite Benchmark,
This results in 70% fewer tokens and 15% greater reasoning.
But then what we discovered is that this graph itself
can be more than just an optimization.
It's an ideal foundation for an interface for human AI collaboration, a shared memory.
I'll show you what I mean.
So right now we are actually running
I can add an agent.
and tell it to do something like draw a mermaid die.
So this agent that I added starts adding its own nodes to the graph.
I didn't have to...
rewrite any of my context in the prompt
and its progress is visible to me.
in real time. I could even, if I wanted to,
Send its output to another AI.
This workspace is the first product built on our core API, a new structured primitive for building with AI.
I've been using the VoiceTree workspace every day to work on VoiceTree. Here are some recordings of me voice
Today, the LLMs don't have a...
They reprocess the entire chat history for every single turn, which is inefficient and expensive.
Voice Tree solves this at the input layer. We convert any unstructured text into a graph and then ask LLM what nodes it would like to have in common.
On the GSM Infinite benchmark, this results in 70% fewer tokens and 15% greater reasoning
What we then discovered is that this graph itself can be more than just an optimization for LLMs.
Today's LOMs
Today's LLMs don't have true
They reprocessed the entire chat history for every single...
inefficient and expensive.
Voice tree solves this at the
We convert any unstructured text into a graph and then ask the LLM what nodes would
on the GSM Infinite Benchmark.
This resulted in 70% fewer tokens.
Input tokens and 15% greater reasoning.
What we then discovered is that this graph itself can be more than just an algorithm.
for LLMs. It's an ideal foundation for an interface for human AI collaboration, a shared memory.
I'll show you what I mean.
So right now, we're actually running VoiceTreat live on what I'm saying. You can see the transcription in the bottom.
I'll now add an agent.
into our shared workspace.
Thank you.
Maybe we can keep it okay.
Thank you.
What do you think?
All right, so today's LLMs have a pretty terrible architecture.
They just chuck the whole conversation history back in every time you send a prompt.
Boistree solves this at the input layer. We create a graph.
for any content.
and ask the LLM from that graph what nodes it would like.
We've been testing this on the GSM Infinite benchmark, and we're getting 60% fewer input tokens and 15% more accurate reasoning, which is huge.
All right, so today's large language models have a pretty terrible...
memory. They reprocess the entire chat history every time you want to send the large language.
VoiceTree solves that at the input layer we convert any type of text into a graph and then ask the LLM from that graph
What nodes would it like to specifically read?
We've been testing on the GSM Infinite Bench.
and we're seeing 60% fewer input tokens sent and up to 10% better accuracy and reasoning.
But the cool thing is is that this is actually much more than an optimization. It's a whole
new foundation.
for human AI collaboration. It's because it can become a shared memory. So I'll show you what I mean. Right now, I'm running VoiceTree live.
It's transcribing my voice in the bottom left.
And I can now launch an agent.
So it gets sent all the relevant context from the graph
And now I'm going to tell it to draw a simple moment.
All right, so today the LMs have a pretty terrible architecture primary. They just chuck the whole conversation history back in every time you send a prompt. Boistry solves this at the input layer. We create a graph for any content.
And ask the LM from that graph what nodes it would like to be.
We've been testing this on the GSM infinite benchmark, and we're getting 60% fewer input tokens and 15% more accurate reasoning, which is huge.
All right, so today's large language models have a pretty terrible architecture for memory. They reprocess the entire chat history every time you want to send a large language model.
VoiceTree solves that at the input layer. We convert any type of text into a graph, and then ask the LLM from that graph, what nodes would it like to specific?
We've been testing on the GSM benchmark, and we're seeing 60% fewer input tokens sent, and up to 10% better accuracy and reasoning.
But the cool thing is, is that this is actually much more than optimization. It's a whole new foundation for human AI collaboration. It's because it can become a shared memory. So I'll show you what I mean. Right now, I'm running VoiceTree live.
It's transcribing my voice in the bottom left, and I can now launch an agent from here.
So, it gets sent all the relevant contacts from the group.
And now I'm going to draw some format diagram.
All right, so today's LLMs have a pretty bad architecture.
They just send the whole chat history.
back in for every single prompt.
VoiceTree solves that at the input layer. We convert any text into a graph and then ask the LLM what nodes it would like to...
We've been running this on the GSM infinite benchmark and we're seeing 60% fewer input tokens.
and better reasoning.
Right now, I'm running VoiceTree live.
It's transcribing my voice.
and converting it into that graph.
And the cool thing is, is that VoiceTree is not just an optimization for LLMs.
it actually can result in um so it's like essentially just like a whole new um interaction layer
What is that bug?
Okay, let's reset to where we were
at the input layer we convert any text into a graph and then ask the lm what nodes would like to see
and better reasoning up to 15%.
So
Right now, I'm running Boy Street live.
is transcribing my voice and converting it into that graph. And the cool thing is that VoiceTree is not just an optimization for OMs. It actually can result in a whole new interaction layer.
I'll see you next time.
The current way that LLMs give themselves memory is really inefficient.
they append the whole transcript history whenever you send a new
Boystery solves that at the input layer.
we convert any graph
into a, we convert any text.
So, the current way that large language...
is really inefficient. They append the whole transcript history back into the model every time
VoiceTree solves this at the input layer. We convert any text into a graph and then ask the LLM what nodes it would like to be.
We've been looking at the GSM in...
and we've been seeing 60% fewer input tokens on average and up to 15% more accurate reasoning.
The cool thing is, what we realized is, is that this is not just an optimization. This now becomes a whole new foundational...
interface for human AI.
So I'll show you what I mean. So right now, I'm actually running VoiceTree live on what I'm...
It's transcribing my voice to text and then creating the voice tree graph.
And I can open an agent. I can spawn an agent.
Um, and then...
get it to do something. For example,
Draw me a mermaid diagram.
explaining a simple mermaid diagram explaining what this could look like.
So the current way that large-language models give themselves a memory is really inefficient. They append the whole transcript history back into the model every time you send a new message. Voistry solves this at the input layer. We convert any text into a graph, and then ask the LM what nodes would like in this context. We've been looking at the GSM benchmark, and we've been seeing 60% fewer input tokens on average, and up to 15% more accurate reasoning. The cool thing is, what we realize is that this is not just an optimization. This now becomes a whole new foundational interface for human AI collaboration. So I'll show you what I mean. So right now, I'm actually running Voistry live, on what I'm saying. It's transcribing my voice of text and then creating the Voistry graph. And I can open an agent. I can open an agent.
and then get it to do something, for example.
Draw me a mermaid diagram.
Explaining a simple mermaid, Doug, I'm explaining what this could look like.
All right, I would like you to create a new node on the graph anywhere, please. Thank you, please anywhere, please
So the current way that large...
give themselves memory is really inefficient. They reprocess the entire transcript
VoiceTree solves this at the input layer. For any type of text, we can convert it into a graph and then ask the LLM what nodes from that graph it would like to look at.
We've been playing around with the GSM in
and we're seeing up to 60% less tokens sent and 15% more accurate reasoning on the long contact.
The cool thing we realized is that this isn't just an optimization, it actually is the perfect foundation
for a new interface for human AI collaboration.
shared memory. I'll show you what I mean. So right now, I'm actually running VoiceTree live on my text. It's converting my voice into this graph here.
And what I can do is I can spawn a terminal.
and get an agent to do something for me.
It's already gotten all the relevant content.
from the graph injected into its context. So I don't need to rewrite any.
Alrighty, we are testing some things.
Thank you.
So it's going to be there, that's all right.
And where is the new file going to be? That's the question.
Okay.
So the current way that large language
themselves memory is really inefficient.
They essentially reprocess the entire transcript history for every request.
VoiceTree solves this at the input layer. We create a graph for any type of content.
and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM in
and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
The amazing thing about this is that it's not just an optimization.
It's actually a whole new foundation.
for human AI collaboration.
It's a shared memory. So I'll show you what I mean.
So right now I can open an agent on my shared memory count.
and get it to do some ink.
So here I just got Gemini to add a
A diagram.
then
I can get Claude.
to write the pseudocode for this.
So I don't need to Reprompt ever it gets a full tracing From the graph of what content could be relevant
Hmm.
So, the current way that wide language models give you is
is really inefficient. They essentially reprocess the entire transfer history for every request. Voicery solved this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its context. We've been running this on the GSM Infinite benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now, I can open an agent on my shared memory canvas.
and get to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever, it gets a full tracing from the graph of what content could be relevant.
So the current way that large language models give themselves an memory is really inefficient. They reprocess the entire transcript history on every request. VoiceTree solves this at the input layer. For any type of text, we can convert it into a graph, and then ask the LLM what nodes from that graph it would like.
We've been playing around with the GSM infant benchmark, and we're seeing up to 60% less token consent and 15% more accurate reasoning on the long context problem.
the cool thing we realized is that this isn't just an optimization it actually is the perfect foundation for a new interface for human collaboration a shared memory so right now i'm running voice tree live on my text
So the correct way that large language models can
is really inefficient. They essentially reprocess the entire transcript history for every request. Voice3 solved this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its content.
We've been running this on the GSM infinite benchmark and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now, I can open an agent on my shared memory canvas.
and get to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever, it gets a full tracing from the graph of what content could be
So the correct way that large language models give
really inefficient they essentially reprocess the entire transcript history for every request boistery solved this at the input
So the correct way that large language models can
is really inefficient. They essentially reprocess the entire transcript history.
So the correct way that large language models can be
really inefficient. They essentially reprocess the entire transcript history for every request. Voicetree solves this at the input layer. We create a graph for any type of content, and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM Infinite Benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reasoning.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now I can open an agent on my shared memory canvas.
and get it to do something.
So here I just got Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
So I don't need to re-prompt ever. It gets a full tracing from the graph of what content could be relevant.
And up to 15% more accurate reasoning.
So the current way that large language models give.
They essentially reprocess the entire transcript history for every request. Voicetree solves this at the input layer. We create a graph for any type of content and then ask the LLM what nodes it would like from the graph in its context.
We've been running this on the GSM Infinite Benchmark, and we've been seeing 60% less input tokens and up to 15% more accurate reason.
However, the amazing thing about this is that it's not just optimization. It's actually a whole new foundation for human AI collaboration. It's a shared memory. So I'll show you what I mean. So right now I can open an agent on my shared memory canvas.
and get to do something.
So here I just want Gemini to add a diagram.
I can get Claude to write the pseudocode for this.
so i don't need to re-prompt ever it gets a full tracing um from the graph of what content could be
All right, testing one, two, three.
Testing one, two, three
Testing one two three
That is awesome. Okay, cool. Yeah, let's definitely do that.
So what we want to do right now is we want to make some improvements to VoiceTree and the first thing is we want to make improvements.
to the output print statements.
We want to make them coloured.
So we want to color our output print statements.
I'm using the term color module.
And then you can just print colored red or colored green, for example. And the way we want to do that.
is
So what we want to use that for is, so in our output models, we currently have text that says creating a node and appending to a node.
and we want to make sure that
Those print statements are in either cyan or green. So cyan for append and green for create.
All right. So while I'm getting that to run, the other thing that I wanted to work on is just a really quick fix up, which is the sidebar isn't.
So we need to quickly fix that.
Thank you.
Yeah
Damn, it's all in love.
Thank you.
Thank you very much.
Yeah
I said yes.
I think we're out.
Yeah, whatever you think.
Give Dad some enzo to those two.
Okay, take some Enzo's, take some Enzo's, an L-theanine and an Ibuprofen, and don't have any more caffeine.
And stay very hydrated.
No, no.
Thank you.
Thank you.
It's a big one.
Okay.
Next.
I'm not supposed to be one.
Thank you.
Thank you.
We're trying to check right now if the color module
is working correctly so we're doing a live test and seeing what colors come out.
Thank you.
Thank you.
Thank you.
Thank you.
I'm not going to do that.
Thank you.
all right cool so i just want to do some quick fix-ups to our working environment um so we have a couple problems right now um so one is that in obsidian we see all our
as well. So we see all the Python and shell files.
And that's actually not the worst thing.
Because it could be cool to have like a voicetree.sh tool, which we can open.
However, it's too cluttered right now with all the prompts and Python tools.
and shell scripts.
So what's the best way to solve that?
So one way to fix it would be if we put all those tools.
one directory lower.
and opened, when we open a terminal, it opens in that environment.
Um, so the way we could do that.
is we could in our zs hrc um when
We open a terminal and we're in.
in an obsidian space.
we can cd dot dot to.
to the parent directory.
But we do need to make sure that that's going to work with our prompting system.
So the prompts.
um how do they currently work i think they currently open in
Let's see
Thank you.
parents.
Thank you.
The core dichotomy that we need to solve is that the agent needs access to both the Markdown Tree Vault directory and whatever Git repository we're working on.
Okay, so maybe the solution then is, so an agent can't enter the directory of a...
whatever directory it started in.
Yeah.
There should be one here now.
once you want to get out of this.
Okay.
Maybe we do.
Thank you.
Thank you.
Stay down.
Thank you.
Thank you.
Thank you.
Thanks.
Let's go.
Thank you.
Thanks.
Yeah.
Thank you.
I'm going to finish that.
I'm going to take a look at it.
Mom, it's a common business.
How are you? Say.
dollars.
Sorry, what is it? A thousand people like to announce $500.
Bye.
So right now
Voice trees running on this little window there. Can you see it there?
Yeah, I see it. So it's it's recording me. It's transcribing And then in this window, I can have my note
And then what happens is as I'm.
for explaining things that's that's for explaining things and then you see how a new node just appears
Yeah, that was from me talking.
And so if I keep on talking about that node see now it's I you can see that it's live because it's
It has that little text there so
like there it's in green and blue the the comments from my my terminal and then there's the actual like
nodes appearing and so while I'm
like the theory for like a minute.
The um...
the nodes will still be appearing, and the logs here are quite clear, they're colored, so they understand what's going on. They say, ah, okay, he's explaining this stuff, but in parallel to that, VoiceTree is actually running here, and you can see the logs, and you can see the nodes appearing.
No, so for the demo
You read the demo script, right?
good. So...
I think that is a big vote.
Well, it's the optimization part and then how that becomes
So right now, this is how I'll do the demo. I'll have Voice Tree running here, and then I'll have my notes for what I'm gonna explain here in the bottom half.
Yeah, they will see these notes.
I mean, they also see Voice Tree running in the top.
And so I'm explaining VoiceTree using some.
And then at some point I tell them I'm doing a live demo right now. You can see VoiceTree running.
And then I show I open like a node here like I can open cloud or Gemini you see that
And then if I open Claude,
Phone away.
So you don't have to, you don't have to.
No, I don't need to write it it automatically
And then I'll show them. I'll show how it...
Anyway, so it gets all the content.
Um, and then I can.
Yeah, it gets all the nodes from the chip.
Actually, it's not working right now.
But it does, and I can show them that.
Um...
And then I'll get it to like do something like write a mark down.
Like to draw like a diagram or something something simple now just paste it in so I'll just do like
draw
Like okay, I haven't copied someone
So if I just say...
Add a new node.
So I'll just paste that in, which just says add a new note of mark.
And it will all work, so there's some bugs right now, but I'll make it all work. Yeah.
Um...
And then...
It's going to add that node. And then what I'm thinking is either I switch over to
Example
Um.
that we were talking about, that I have at the end of the script.
Let me see that one again, one sec.
what did you mean by that four different screens so you're going to just have a lot of different agents doing
No, so, hang on.
So there's two things I can do. So one option is I continue using this Example this like this. I don't I don't restart. I just use the same example
And I just now say, okay, now I'm going to engineer an actual feature in VoiceTree, continuing off the same VoiceTree.
And so, then I'll...
And then after that we speed it up to like 10x so you hear me talking really quick
and the nodes appearing.
um and then we we stop at some point where i'm like now watch this and i get um i get one agent to decompose a task into subtasks and then run all the sub agents on those subtasks
Um...
But is this all possible in three minutes?
that it would have to be really sped up.
And just showing them like, or, so that's one alternative, is that I continue using the same tree to show like a real world example.
Or I can switch over to some real world examples of me actually using it for engineering, sped up and just stop at key moments.
No idea to develop a narrative.
Yeah, yeah, I agree.
of the context. Yeah. And then I can have 10 seconds at the end.
like so like split screen right like like as in there's four screens
And it's just like four videos of me using VoiceTree for work.
So I'll show you an example.
Like if I open
so
So I can show like some of these really sped up examples of me using
VoiceTree to do engineering work.
Yeah, but I think that's a bit true.
But if we just have it for 10 seconds like split-screen for four ones of these and just like just to say Just for the one sentence of me saying I'm addicted to using voice tree engineering has never been this productive nor joyful before Just for that just for that one second
Yeah, nice yeah
Pretty cool, right?
Because I told him, like, no worries, you don't have too much time. Even a quick glance could be useful. And he said, I'm super interested in the project. Just want to be able to give it my full attention in time. And then he says he spoke to a patent lawyer, a friend of his, and they told him it makes sense to file for patents in the countries you will want to sell to.
I guess these are things for Y Combinator.
Okay, so then ideally I should be sending him this this draft of just the text or send him me me doing an actual example like I can probably the next hour get a pretty okay example of me doing it
Yeah, we have some problems, of course.
It's not going to be the... Yeah, that's okay.
Yeah. Do you think that's a good use of your time right now? Is that what you want to focus on?
Getting the demo done
I mean, doing kind of a mock video.
Oh, I mean...
just be practicing for the real video yeah i mean the real video is just gonna be
they record like the best version
whatever the best one is so you just want to keep track
Yeah, but I mean if we record it early and send it to Zarin, he can give some advice. Yeah, exactly, I would do that. Yeah, and say this is maybe, I'll just tell him this is the draft. Do you want me to, do you want to send me Zarin?
Okay, so do you think that's the perfect flow like as in
what I sent you and then what I explained.
What it looked like
Yeah, dude, it makes it sound like it's a really...
Hmm.
So one thing is that I've completely stopped work on the GSM bench.
And it would still be really cool to show like an image
benchmark results.
Yeah.
But don't you have already one that you can show?
I mean...
I've stopped work on it. I think I could come up probably in three hours of work, I could get a nice graph that's actually accurate.
Um, but there's a lot, a lot of.
But I think that's fine. I mean, they just want to know it's promising not that
Let me just check exactly what time it is. It's 5 a.m. Tuesday. Yeah. I think I'll be staying up.
I don't know. I mean, hopefully not, but in case.
All right, 5 a.m. Tuesday, one day left. Exciting. Okay, so can you read the...
Yeah, I've read it. I think it's great. So you think it's perfect just just make that work
What are you going to be showing in the first part of where all the text is?
Before you say, I'll show you what I mean.
So the whole time.
So the whole time it's going to be.
This screen, right? This is the screen I'm recording
My face will be in the top right
And here you see that there's like
Um, there's... What is that?
Well, it's just drawings so I can just draw whatever I want there
So, and I can use that to explain what I'm talking about.
So I could have like some little, you know, circles or whatever I need to explain, whatever I need to explain. Yeah.
and then and then when I'm done with the explanation I drag it away and it's just the
Awesome, he's perfect.
Amazing, amazing, amazing.
Yeah, I don't know. This is just so much better than...
other ways of doing it where I have like a different slide and then I switch to this slide and then they're not sure.
progress you've made in the last week I mean imagine with a few more months to work on this if we don't get this round you have six months to polish it well so what happened is right is that the past year and a half I've just been working on the core algorithm
And in this past two weeks, I've just been working on like UI, which makes it look a lot better, but nothing's actually that functional.
But it makes a huge difference for...
But it's genuinely gone to the point where I prefer using it now to other ways of working.
Yeah, it's so cool, so cool.
That's awesome, dude. Yeah.
It's your own invention. It's really cool. It's so cool.
Okay, I'll try. So what's my priority to get to get you a draft of that?
And Zen.
Yeah, yeah
Yeah, perfect.
Exciting stuff, Manu. Amazing. All right.
Um,
Um...
So, can you hear me?
Now I can hear you. Yeah, you're back. What I want you to work...
Um.
Well, uh-huh.
Well, uh-huh. So when I send you the draft...
You could do the editing.
Um,
I don't know. I don't know. What's...
What do we need once like the demo when you be edited slightly like when you speed up parts of it
Yeah, but you'll have shot it, so you know every single line you said. I think you should do the speeding up, because you'll know what you want to, while you're making.
It'll just be inefficient for me to figure that out. Okay, just stay on standby then.
Yeah, that could be really good. Maybe out of all, you know, I've sent you lots and lots of prompts. I mean, like example scripts and stuff. Maybe we can fill it in a bit more.
yeah and so and any of the personal stuff
Don't worry about filling it out like a time. I've um like hacked something like I'm thinking like they all talk about like biohacking
that's kind of cool and interesting
Um, that I haven't done.
and you could think about maybe because like in that one minute founder video we can i can mention some things about voicetreat to make the next demo more understandable
Because I'm sure they just append them.
um
So maybe you think about if like the whole narrative of like Founder video plus demo video plus written application if it all really makes sense and nothing's left really confusing And they're not like you know what I mean like as in the whole thing is a cohesive story piece of narrative
Best way to see this is if you already have drafts, you know, not the final version, but if you just take the one minute founder video, have mom record it up on the roof.
Or just, I mean, behind with the bookshelf, that looks good, though.
And then Zeronomy can feedback.
Okay, okay, cool.
I will...
I'll record, I'll try to record all the drops that we need and send them to you ASAP. Nice. Okay. Should I make a group with Zarin? Yeah, yeah.
All right, bye. All right. See you, man. Bye.
All right, cool. Can you hear me voice, Tri?
All right, we just need to get some little fix-ups working for our demo. Most things are already working. We just have a few things we need to do.
Um,
So for the demo
Something we need to do is...
for new node placement consider all other existing nodes in the graph so that like right here they don't appear too close to each other
We also need to stop doing restarting layout after we drag a node. That's been really annoying.
And I know that there's somewhere in our code base there's layout start.
stop and we we just need to comment that out so that the layout doesn't restart
Okay, and then the other thing we need to work
is that
Um,
Currently, we've somehow broken the Claude.sh.
So it used to get the whole Python dependency.
and now it no longer does.
So we need to fix that.
You
Alright now I want to figure out
When we're going to go to the gym.
We want to go to the gym. Let's check when the gym's closed.
So it's a Berlin strength.
Maybe it's not open on a Sunday.
Okay, so we fixed the ClawDOSH broken functionality. Let's just quickly check that it's also working for Gemini
Nice looks like it's working
Cool, okay, so now we just want to make sure we need to work on optimized new node placement
Um...
Let's actually yeah, so let's get an agent to work on
Restarting layer.
Let's do Claude.
Okay, and now we want to work on Optimize New Node Placement. Yeah, clone.
Okay, let's add to a list of things we won't do but ideas
Could be cool, but we're not going to do because you know
So one thing is a way to actually delete a node and then have it deleted from our tree data structure as well.
the other thing we wanted to do was to be able to...
collapse a node and all its neighbors.
into one container node.
That's also a won't do task.
All right, so now we need to...
what exactly we need to be able to do the demo where we do agent.
All right, so now we need to consider what exactly we need to be able to do the demo where we do agent orchestra.
So, I guess the first thing we need to do is to be able to...
So I guess the first thing we need to do is to be able to...
So go down a chain of nodes.
So go down a chain of notes.
have those nodes created.
have those nodes created.
And then...
Um, and then...
run an orchestration agent.
run an orchestration agent.
Um...
So to run an orchestration agent, we just give it the specific orchestration prompt. That's easy and then
So, to run an organization agency.
the specific orchestration prompt that's easy and then
So we'd give it the orchestration prompt. It would turn, it would create the four subtasks.
So we'd give it the orchestration prompt. It would turn It would create the four subtasks
And then on each subtask.
And then on each subtask.
We could uh
we could uh
So in each sub-task, we could then run the sub-agent.
So in each sub-task, we could then run the sub-agent.
and
And...
the sub-agent
the sub-agent
Hmm, hmm, hmm, hmm.
Hmm, hmm, hmm, hmm.
Because before we didn't really have it completely integrated into our existing infra So the old way of doing agent orchestration was to create a new sub
Because before we didn't really have it completely integrated into our existing infra. So the old way of doing agent orchestration was to create a new sub-agent shell.
script. But now, ideally...
But now, ideally,
Just the subtask itself to contain all the information.
Just the subtask itself to contain all the information.
So we can try that in a minute
So we can try that in a minute
Um,
Um...
But for now, I just want to make sure that our new node plays
But for now I just want to make sure that our new node placement logic is okay
is okay.
Okay, so.
Okay, so...
Give me one second.
Give me one second.
I'm recording.
I'm recording.
Okay, there's another problem with Gemini is that the ampersand doesn't actually inject the prompt file So we need to actually inject the content With an environment variable, I guess
Okay, there's another problem with Gemini is that the ampersand doesn't actually inject the prompt file So we need to actually inject the content With an environment variable, I guess
Let's add to our won't do list
Um, let's add to our won't do list.
that when we close a terminal
that when we close a terminal,
that when we close the hover editor for a terminal that the actual terminal node is also
That when we close the hover editor for a terminal that the actual terminal node is also
It cleared, deleted, hidden.
It cleared, deleted, hidden.
okay another won't do problem that we've noticed is that um so we just made uh so two orphan nodes just created got created actually three orphan nodes
okay another won't do problem that we've noticed is that um so we just made uh so two orphan nodes just created got created actually three orphan nodes
15, 16 and 16.
15 16 and 16 and so two files
And so two files...
with the same
with the same
Node ID got made.
node ID got made.
And so we need to figure out exactly why that
and so we need to figure out exactly why that
That's a bug
That's a bug.
you
Okay, so right now we're trying to figure out exactly what sort of layout logic and physics we actually want. So one thing I realized is that if I drag a node, ideally I would like all the nodes that it's connected to, to also be pulled towards that node.
Okay, so right now we're trying to figure out exactly what sort of layout, logic, and physics we actually want. So one thing I realized is that if I drag a node, ideally I would like all the nodes that it's connected to, to also be pulled towards that node.
Okay, so one thing we need for our orchestration mode is we need it to somehow be able to click
Okay, so one thing we need for our orchestration mode is we need it to somehow be able to click
Also children.
Also children.
Um...
Um...
So maybe we can do like dependency traversal can do.
So maybe we can do like dependency traversal, can do dependencies.
dependencies.
Plus, go find...
Plus, go find...
Find the children of that node
Find the children of that node.
Um,
Um,
Um,
Um,
Uh...
Uh...
Yeah, yeah, we just need to then also do dependency traversal on the...
Yeah, yeah, yeah, we just need to then also do dependency traversal on the
Children of the node that we're running on and the children's children's children's children, etc, etc, etc
children of the node that we're running on and the children's children's children's children, etc.
One thing on our non-actionable ideas list should also be that our tree should be able to reference nodes which an agent has created, not just the ones that we've made manually.
One thing on our non-actionable ideas list should also be that our tree should be able to reference nodes which an agent has created, not just the ones that we've made manually.
You
Again everything we're going to quickly add to our non actionable ideas list is that for some reason
Again everything we're going to quickly add to our non actionable ideas list is that for some reason
Gemini is not using the same color for nodes.
Gemini is not using the same color for nodes.
Um, so we need to just ask it why.
Um, so we need to just ask it why.
Oh, good.
The Book of Al-Rajanesh is out now.
Oh good, the Book of Arrajneesh is out now.
in the shadow of enlightenment
The Shadow of Enlightenment.
Mm-mm.
Mm-mm.
I don't know.
I don't know.
Oh
Cough.
My darling.
My darling, is that you?
No.
Yeah.
Yeah.
You
Mm-hmm.
Mm-hmm.
Thank you.
Alright, alright, alright, alright.
Alright, so while working on that, we made a non-actionable ideas list.
With all the little things.
up but we didn't want to get distracted.
And now we're going to get.
Claude to act as an orchestrator
for creating all those stuff
So let's just, so now we should be able to say.
So it's one of the...
I'll see you next time.
Okay.
What are you going to do, have a run?
Just do a bit of exercise.
I'll see you next time.
We'll see you next time.
for new node placement, consider all other existing
Nose in the graph, so that like right here they don't appear too close to each other
um we also need to stop
Alright cool, we're going to open our voice stream.
That comes out.
All right, we just need to get some little fix ups working for our demo. Most things are already working. We just have a few things we're going to do.
So, for them.
Cool, can you hear me voice three?
We just need to get some little fix-ups working.
Most things are already working, we just have a few things we're going to do.
so for the demo something we need to do is um uh for you know placement um consider all other existing nodes in the graph so like right here they don't appear too close
We also need to stop doing restarting layout after we drag a node, that's been really annoying. And I know that there's somewhere in our code base that there's a layout.
And we have to comment that out.
And I know that there's somewhere in our code base there's
And we just need to comment that out so that the layout doesn't restart.
Okay, and the other thing we need to work on is that currently we've somehow broken the clod.sh. So it used to get the whole Python dependency, and now it no longer does. So we need to fix that.
The other thing we need to work on.
Restarting layout after we drag a node, that's been really annoying. A Python dependency, and now it no longer does, so we need to fix that.
So we need to fix that.
I want to figure out when we're going to go to the gym. Let's check when the gym's closed.
So it's Evelyn's rink. Maybe it's not in a Sunday.
All right, so we fixed the cloudless edge broken functionality. Let's just quickly check this.
Nice, looks like it's working. Cool. OK, so now we just want to make sure we need work on optimized.
So it used to get the whole Python dependency.
Python.
All right, so we fixed the Clotus Edge.
Nice, looks like it's working. Cool. Okay, so now we just want to make sure we work on optimizing node placement.
Let's actually, yeah, so let's get an engine to work on resetting now.
And let's add to a list of things we won't do, ideas that could be cool, but we're not going to do because of priority. So one thing is a way to actually delete a node and then have it deleted from our tree.
The other thing we wanted to do was to be able to collapse a node and all its neighbors into one container node.
That's also one of the new tasks.
Alright, so now we need to consider what exactly...
the demo where we do agent orchestration.
So we can try that in a minute.
That's me.
Gemini is not using the same color for nodes, so we just ask it why.
to add to a non-action ideas list, is that for some reason...
Bum!
Thank you.
Currently we've somehow broken the...
cloud.sh
So it used to get the whole Python dependency, and now it normally does, so you fix that.
One running.
All right, cool. Can you give me a voice drink?
All right, we just need to get some little fix-ups working for our demo. Most things are already working. We just have a few things we can do.
So for the demo, something we need to do is, for new node placement, consider all other existing nodes in the graph, so that right here they don't appear too close to each other.
We also need to stop doing restarting layout after we drag a node, that's been really annoying. And I know that there's somewhere in our code base there's...
stop. And we need a comment out so that the layout doesn't restart.
Okay, and then the other thing we need to work on is that currently we've somehow broken the clod.sh. So it used to get the whole Python dependency, and now it no longer does. So we need to fix that.
All right, so we fixed the clodis edge functionally.
All right, so we fixed it on this edge.
and now it no longer does, so we need to fix that.
All right, so we fixed the clodis edge button functionality.
So it used to get the whole Python dependency.
Ice, ice we fixed it ice we fixed the glottis edge
All right, so we fixed the cludders. All right, so we fixed the cludders. Edge broken functionality. Let's just quickly check this out.
As we fix the clodest edge functionality.
OK, so we fixed the cloudless edge broken functionality. Let's just quickly check this.
One ring.
as we were sharing.
I was six years old when I was missing a day.
All right, I want to figure out when we're going to go to the gym. Let's check when the gym's closed.
So it's evidence-wing.
of followers of the Indian Guru.
Bye.
Children like me.
All right, so we fixed the Clodos H broken functionality.
Nice. Looks like it's working. Cool. Okay, so now we just want to make sure we work on optimized unit placement.
Let's actually get an agent to work on restarting now. Let's do Claude.
To know
You
Thank you.
Thank you.
Brrrr!
Thank you.
Hmm.
See what mom's doing.
Thank you.
Working out.
I'm going to have a run.
Maybe, huh? Maybe.
You
I don't know how it works.
work
Crashing a bit
Yeah, I've got some great ideas.
Very useful.
So current large language models performance becomes worse as their context length.
Attention will always be complicated.
You only know for sure of a new piece of
by checking each one. There's quadratic complexity.
VoiceTree solves this by optimizing the input itself.
We take any text and progressively convert it into a graph containing those concepts and the relationships.
Then, instead of repeatedly sending LLMs the same appended unstructured text, we work alongside the LLM to prune to only the relevant sub-tree.
The results we have been seeing are amazing. We're seeing 70% fewer input tokens on average and 15% greater reasoning on the GSM Infinite benchmark.
Moreover, VoiceTree provides those same performance benefits to humans.
So humans, too, suffer from context degradation.
And in fact, what we can do is.
We can...
Remove this.
Entirely.
and instead just work on a shared memory graph.
I'll show you what I mean.
So what you're seeing right now has been running live.
I've been running VoiceTree.
on my voice, and then it's converting it to text and then to this graph.
I'll now launch an HR.
Now, so what you can see here...
Oh, what the fuck did I do here?
ah
LOL.
All right.
So LLM's performance decreases the longer their context gets. This is called context degradation. And the key reason for this is because they reprocess the entire context.
VoiceTree solves this at the input layer. We optimize the input itself. We take any text and progressively convert it into a graph containing the concepts and the relationships
Then, instead of repeatedly sending LLMs the same appended unstructured text, we work alongside the LLM to prune to only the relevant subtree.
The results with this have been amazing. We're seeing 70% fewer input tokens.
and 15% greater reasoning accuracy.
I'm the GSM Infinite.
Moreover, VoiceTree
provides these same performance benefits to humans. Humans too suffer from context degradation.
And in fact, the really exciting thing is
Why don't we just remove this entirely?
and work only
with the graph presentation. Have the graph as a shared memory.
between you and AI.
i'll show you what i mean so what you're seeing above is voice tree actually running live um so it's converting my voice to text and then updating the graph
What I can do now is I can spawn an agent.
And tell it to do something, like for example, create a mermaid diagram explaining how this could work.
So the agent
got only the relevant context from the graph.
Now that it's finished its task.
it adds a new node back to the graph.
Thank you.
Thank you.
Thank you.
Thank you.
Hmm.
Thank you.
Thank you.
We'll be right back.
Thank you.
We'll be right back.
Thank you.
Yeah.
Thank you.
The first thing is we want to change.
one of our tools.
which is I think called cleanup vault.
Let me see exactly what it's called.
Um...
Yeah, it's called cleanupvault.sh.
in the tools directory in
the VoiceTree repo, and currently that's getting rid.
of
some files that we don't want to get.
So currently it's getting rid of the XcaliDraw folder, which contains our drawings.
And we don't want that folder to be.
cleaned up when we run the cleanup.
So we don't want that folder backed up, we just want to leave that folder.
Um, yeah.
in the markdown tree vault directory, just leave it alone. Don't remove it, don't back it up, don't move it.
Now I get with the left hand.
It's hiding my text.
And he'd get rid of it.
You're going to have to zoom out. Command minus. Try command minus.
Okay.
It should be possible to do that.
it's just because you have your
Display to load.
I just restarted. No, just wait.
Let me know it.
Did you want to actually format it at all?
double-face that.
See, this would have... Double spacing. Yeah.
They're a double.
It's here.
I don't know. You found it. It was ready. It was just there.
it's really double spacing
Yeah, I don't know sir
If you ask AI or you Google it, you'll be able to do it.
All right. So what I want to do.
voice tree.
And I want to come up with an example.
which would look good for a demo, which I can work on.
And I'm thinking that a good example would be either clustering or subtree identification.
Um...
So I think let's talk about what subtree identification could look like.
Um, so what we would want is.
where we ideally, given...
like thousands of nodes with various subtrees and things can identify like the key groups within that.
Um, um,
And then...
color those groups.
Now,
We can't just send us out.
ones to a large language model.
Um, so I think we're going to have to do it a bit more smart. We're going to have to do like a hundred nodes at a time.
And...
Maybe what we do is we send.
each subtree.
to the large language model because our subtree user
up to like a 100, 200 nodes in size.
If necessary, we can split them.
a breaking point, but I don't think that'll be necessary yet because our subtrees are never that big.
And then so for a given sub.
We ask it.
to
identify the clear high-level groups within the subtree that itself
should become a
recursive tree.
So essentially, if there's a subtree,
of 100 nodes, and it's really talking about four highlights.
ideas or projects or whatever containers make sense within the context.
So that's important. We want it dynamic.
to have been the contact.
Um, um,
We...
So what am I talking about?
So what I mean by dynamic is depending on the content.
what the containers are that divide the subtree will be different. So sometimes it's objects.
Each subtree is going to be a different project.
And sometimes it's going to be.
a different
Sometimes each subtree is going to be a different, uh,
I don't know, like class of concept or something, theme perhaps.
Okay, now I think...
what I want.
is
So I want to come up with some terminology.
For what?
Hey.
So we want to differentiate.
We need terminology that differentiates between the subtrees that we pass as input.
So let's just call that the input tree.
So we take a lot of different trees.
We pass in, what do you call it? Like the islands.
or maybe
Perhaps.
D.
Um...
Isolated trees.
Um...
Oh, like a connected component. Okay, yeah, connected component is probably the best.
Um...
Okay, yeah, let's use forest.
which has a tree.
And the tree can have a subtree.
And so we want to clarify some terminology for what the subtree is.
Um...
So,
a subtree.
is
Um...
I actually don't know exactly.
how to define it.
It's...
going to be
So what we really want at a high level for the terminology or definition of a subtree is you have a tree, right?
and each node
has a left subtree and a right subtree.
No, it has.
um it can just have parent and children right because we're just general general trees so every subtree can have parents and children
So, and then what's a good way?
for identifying.
sub trees, within a tree.
like which nodes should be collapsed.
to become sub-trees.
Um...
Actually, we don't even want.
collapse a single existing node. In often cases, we want to create a new synthetic node.
to represent the subtree node.
And one idea I'm having is that that subtree node
have a small degree of connection.
So a subtree node, which has like 100 outgoing connections, 100 incoming.
That's not good as a subtree because it hasn't encapsulated an abstraction.
Thank you.
Does that make sense?
Or maybe it just means it's being used a lot.
There's definitely something there, though, to the...
out degree and how you want to minimize it.
Um, so.
Yeah.
Okay, so then for a proof of concept for this.
subtree identification. I'm thinking they'll all look good for a demo.
and I'm thinking
So I'm thinking...
that what would look good is what we can do is we can identify.
for a tree, identify the different subtrees.
And...
for each subtree.
Um...
Color the node.
Maybe add a tag.
Um,
Adding a tag would make sense if we're going to be using obsidian default graph.
Adding a color.
Would make sense if we want to use the juggle global graph.
I think it would be better to use the global graph for continuity.
What we could first do is that because sometimes our nodes already have colors for the different work that the agents have done, each agent gets a different color, so our first step
could be to remove all the existing colors.
in the folder.
and then do
So that can be a first preprocessing step is remove color from front YAML.
Um,
And then...
What we want is
Um,
We want...
Um.
to then get the LLM to divide the tree in.
subtrees
And return the nodes.
And we also want to have it to be allowed to return like.
Ah.
The...
nodes that aren't part of any sub-tree, like it's fine for a node to be basically irrelevant.
That's okay. We can have irrelevant nodes that just don't get a subtree.
Cool. Then we get the response back from the LLM saying,
Hey, here's the different subtrees you have and the names and IDs of the nodes.
in each subtree.
Um...
And then...
Um...
What we're going to do is we're going to process that response.
Um...
for, so we're gonna take,
The response.
from the LLM.
and we're going to go through
And
color the node.
So you color each.
subtree that we got back, color each subtree that we got back.
in a particular color.
I'm thinking...
Mmmmm.
Okay. And then what do we need to do? We need to do that for every subtree.
Um...
So we repeat that for each, sorry, so that whole algorithm was for a tree. The next step is repeating that same process for each tree in our forest.
Mm-hmm.
All right, let's try to spawn an agent on this now to work on it.
Um.
So let me just give you a few more high-level details.
So one thing for the subtree identification example is that we want...
The input.
to be
Mark
Okay, cool. So we've just added some more.
to subtree identification examples about the input folder.
Okay.
Okay, let's also create a won't do node.
with all the ideas that we're not going to work on as an orphan.
And let's add to that one do node.
Oh.
What are we not going to do? We're not going.
to add delete functionality for file.
Currently, you can hide files, but you can't actually delete them.
I'm not in the UI, at least.
Same for terminals, if you close a terminal, the terminal node doesn't actually get removed, you have to hide it manually.
which is pretty annoying.
Thank you.
Um, never won't do node.
is we could do like minor fix-ups to the eye in terms of positioning. Nodes are a little bit too close to each other.
once a lot of nudes start being created.
No.
I think I can just re-force a land.
to make that work.
Now, I am realizing that removing notes...
functionality is actually really important.
um because removing a node
Um,
means that you've pruned the context.
And that's really important.
You don't want to confuse the outlimbs.
So maybe that will become a will-do.
Okay, let's...
Wow.
Thank you.
Thank you.
Thank you.
So current LLMs have a performance problem with large context inputs and that's because for every request they append the, they resend the whole
Very inefficient.
and leads to context.
So Voice Tree solves that by converting the text into a graph
And then from that graph, so progressively converting your chat into a graph and then identifying relevant nodes and sending just the pruned tree back to the LLM, the pruned relevant nodes back to the LLM.
So we've our early results so we have a system work
And our early results are showing that on the GSM Infinite Benchmark, we're seeing significant...
Um,
a significant up to 70%
fewer tokens sent to the LLM and Accuracy improvements and it gets the right answer more often
Because there's just less context bloat, less to get confused about.
Cool, so I'll show you now.
So this has been running live, right now I've been running VoiceTreat live on my voice. So you can see here it's transcribing my text and then...
uh, providing, um, tree update events.
Um...
So, we can see here the tree.
And now, so I can also now spawn an agent.
Oh yes, what I wanted to say as well was...
We then had a breakthrough where we realized, hey, why even deal with linear text at all and instead send everything interface
with the AI, do all your interactions with the AI through this tree as the interface, almost like a shared memory. So I'll show you what I mean.
So, right, the tree is running and now I can open an agent.
Um...
you see here it gets all the
relevant context from the graph.
and
I can say, create simple.
So let's download Mackie.
And I can tell the AI to now also add back to our tree, our shared memory.
um
Amazing. So now we have that document.
Um,
And...
Cool. Yeah
So today's large language...
have a problem with performance at large input size.
And partly that's because they reprocess the entire transcript.
for every request to mimic a sort of memory but attention to items and memory
scales quadratically because it's an all-to-all comparison so you get degrading performance
VoiceTree solves this by progressively converting a linear text into a graph.
we're optimizing the input itself.
We then send the LLM only the pruned relevant
So we're seeing really promising early results on the GSM Infinite benchmark, where we're seeing up to 70% fewer input tokens.
And a step change from models previously not being able to answer the question at all and just going in circles, wasting output tokens.
to getting the correct answer consistently.
So current large language model...
have a performance problem.
especially at long input context length.
And that's partly because they reprocess the entire transcript.
for every request, to mimic a sort of memory, but attention to items and memory scales quadratically because it's an all-to-all comparison.
So Voice Tree solves that problem.
by optimizing the input itself, progressively converting the linear raw text into a graph of those key concepts and relationships.
We then work with the LLM to identify the relevant nodes and prune the context only to what's useful.
question. Our early results
on the GSM Infinite Bench.
have been really promising. We're seeing up to 70% fewer input tokens.
and step function change in output where previously models that couldn't generate the correct answer and would just go in circles wasting out
can now consistently get the correct answer to hard questions.
So I'm running VoiceTree now live on my voice and so my I'm transcribing my voice to text and then you can see VoiceTree there doing graph
Why? Why am I doing that? So what we realized is, is that once you're using this system...
You don't actually
Need to deal with this.
format, linear text at all, and instead you can deal, you can interface with LM's directly through the graph.
Uh, humans too.
degrade, their context to degrade.
So, when that happens, our memory is now, our working memory is now shared. And that's really cool because if I now...
launch an agent here
I can tell it to do something.
it's going to get only the relevant content.
from the graph.
And when I get it to do something, it's going to add its response back to the graph itself.
There we go. So I've been using this for software engineering.
to build VoiceTree itself every single day. It is so, it's amazing. I'll show you an example now.
I'll show you a real feature I've built with VoiceTree now.
So today's large language models have a problem with performance at large input sizes. And partly that's because they reprocess the entire transcript history for every request to mimic a sort of memory. But attention to items in memory is scaled quadratically because it's an all-to-all comparison. So you get degrading performance. VoiceTree solves this by progressively converting linear text into a graph. We're optimizing the input itself. We then send the LM only the pruned relevant content.
So, we're seeing really promising early results on the GSM Infinite benchmark, where we're seeing up to 70% fewer input tokens, and a step change from models previously not being able to answer the question at all, and just going in circles, wasting output tokens, to getting the correct answer.
for every request um to mimic a sort of memory um but attention times memory scales quadratically because it's an all-to-all comparison so voice tree solves that problem by optimizing the input itself progressively converting the linear raw text into a graph of those key concepts
We then work with the LM to identify the relevant nodes and the prune, the context only.
question. Our early results on the GSM infinite benchmark have been really promising. We're seeing up to 70% fewer input tokens and step function change in output where previously models that couldn't generate the correct answer and would just go in circles wasting output tokens can now consistently get the correct answer to hard questions. So I'm running VoiceTree now live on my voice and so I'm transcribing my voice to text and then you can see VoiceTree there doing graph actions. Why? Why am I doing that? So what we realize is that once you're using the system you don't actually
Alright, testing one, two, three, testing one, two, three, look at me, look, look, look at me, testing one, two, three, look, look at me, I'm testing one, two, three, look, look at me.
All right, we're testing currently how well where the position of new nodes spawn in voice
All right, we're testing right now where our nodes are.
Alright, we're testing right now.
what the exact position will be of a new note.
All right, right now we're doing a completely live demo of VoiceTree.
Let's create a new node for that.
All right, right now we're doing a live demo of VoiceTree.
and we'd like to see where the new note appears.
So large language models currently have a performance problem.
where especially at long input contact sites.
they struggle.
So current large language
have a performance problem at long input lengths. And part of the reason for this is because they reprocess the entire chat history for every request. And then this is to mimic a sort of memory, but attention to items and memory scales quadratically because it's an all-to-all comparison.
So VoiceTree solves this by optimizing the input itself. So we progressively convert input into a graph containing the key concepts and relationships between them. We then work together with the LLM.
to identify what exactly is needed to answer.
a specific question.
So, our early results on the GSM infinite
are amazingly promising. We're seeing up to 70% fewer input tokens.
as well as a behavior where previous models couldn't even answer a hard question and would just go in circles wasting output.
We're seeing them now get the correct answer.
So I'm running VoiceTree right now live on my voice itself. So you can see this window here. It's transcribing my voice to text and then resulting in.
tree actions.
But, why am I doing that? Well, one of our key realizations...
is that humans as well suffer from...
and can get the same benefits from VoiceTree.
So then what we realized is why not in fact completely get rid of this linear text format and instead only interface with the AI through this tree
I'll show you what I mean by that.
So our memory is now shared
So if I open
an agent on one of these nodes.
it's going to get only the relevant input context from the tree.
And then, if I tell it to do something...
It's going to save its output back to the tree.
So current large language models have a problem where their performance degrades at long input context length.
And part of the reason for this is because they reprocess the entire chat history for every single request.
to mimic a sort of memory, but attention to items and memory is quadratic, it's an all-to-all comparison.
So, what VoiceTree does is we optimize this input itself, we progressively convert it into a graph with the key concepts and...
And then we work with the LLM to identify what nodes are relevant and return only the pruned relevant.
Our early results on the GSM Infinite Bank
are very promising.
we're seeing up to 70% fewer input tokens.
we're seeing previous models that couldn't answer certain questions now being able to consistently as opposed to going in circles wasting output tokens confused on the hard problem
So I'm running it right now live. I'm running the VoiceTree algorithm on my voice.
that's what you're seeing in this top right corner and it's creating this concept graph here of what I'm saying
And why am I doing that?
So
Mmm, so close, so close.
All right, we're doing a live demo of voice.
I would like that to appear as a new note.
a new node to represent the live demo.
So current large language models have a performance problem at long input context length And part of the reason for that is that they took mimic memory. They reprocess the entire transit transcript history for every single
And then on top of that, attention to items and memory scales quadratically.
So VoiceTree solves this performance.
by optimizing the input it
We progressively convert any raw text into a graph.
All right, I would like to draw a new...
to represent the live demo of
Please create it.
A live demo of VoiceTree.
All right, so current large-language model.
All right, so current large language models have a performance problem at long input.
And part of the reason for that is because they mimic memory by reprocessing the entire transfer.
for every request.
And on top of that, attention to items in memory scales quadratically because it's an all-to-all comparison.
So, VoiceTree solves this by optimizing the input itself. We progressively convert raw text into a graph containing the concepts and key relationships.
We then work with the LLM to identify what nodes are
and return just that pruned context back.
So we're seeing very promising results on the GSM Infinite Bench.
where we're seeing up to 70% fewer input tokens.
and behavior where models that previously couldn't answer certain hard questions now are able to, instead of just wasting output tokens on the wrong answer.
So, I'm running VoiceTree right now.
completely live. So it's converting my voice to text to then do a tree action.
such as that one there. But why am I doing that? Well, one of the key realizations we had was that,
the same context integration that lms have humans have as well and voice tree can provide those same performance benefits to humans um but then we don't stop there what we realize is well then we can do away with linear interface completely and instead interact with ai purely through this tree itself
And then our memory
shared. I'll show you what I mean by that.
I'll launch an agent here.
And you can see that it gets only the relevant context from the tree.
and inject it into it.
And then if I get to do something like draw a diagram explaining what this would look like
it's going to save its output back to the tree.
like it just did here.
So this is incredibly useful for software engineering and I'll show you some real examples of me using it to build features for VoiceTree using VoiceTree itself.
All right, so current large-language models have a performance problem at long input lengths. And part of the reason for that is because they mimic memory by reprocessing the entire transcript history for every request. On top of that, attention to items in memory scales quadratically because it's an all-door.
So, VoiceTree solves this by optimizing the input itself. We progressively convert raw text into a graph containing the concepts and key relationships within them. We then work with the LLM to identify what nodes are relevant.
So, we're seeing very promising results on the GSM Infinite benchmark, where we're seeing up to 70% fewer input tokens. And behavior where models that previously...
certain hard questions, now are able to, instead of just wasting output tokens on the wrong answer. So I'm running VoiceTree right now, completely live. So it's converting my voice to text to then do tree actions, such as that one there. But why am I doing that? Well, one of the key realizations we had was that the same context iteration that elements have, humans have as well. And VoiceTree can provide the same performance benefits.
But then we don't stop there. What we realize is, well, then we can do away with linear interface completely. And instead, interact with AI purely through this tree itself. And then our memory is now shared. I'll show you what I mean by that.
So, I'll launch an agent here.
And you can see that it gets only the relevant context from the tree injected into it. And then if I could do something like draw a diagram explaining what this would be.
It's going to save its output back to the...
like it just did here.
So this is incredibly useful for software engineering, and I'll show you some real examples of me using it to build features for VoiceTree using VoiceTree itself.
All right, so current large language models have a performance problem.
at long input length and part of the reason for that is because they mimic memory by repressing
Thank you.
contains the backups of all our Markdown volts of where our VoiceTree outputs are stored.
All right, good.
Let me show you the light.
It's good to go in.
Okay, let me show you.
Or we can walk through the...
How is the algorithm?
Do you have my Roger letter? Yeah.
You
You
Kndelri und Kndelro hat Gesundheit ebenso.
All right, Lance calling.
Hey bro, I just I just recorded a live demo and I'm about to show it to the parents
Why don't you watch at the same time?
while I show it to them, so I'll mute myself, I'm muting myself now.
All right. So, so, so, so, so, so should I show you the early final founder video as well?
Ilan, I'm showing them first the founder video and then immediately after the other video because I'm sure that's what YC will do as well.
Hey YC, I'm Manu. I have a passion
performance optimization really in all areas of my life but especially software hey yc i'm manu i have a passion for
Really, in all areas of my life, but especially software. I just started out with my first role in the doctor.
to now my work at Atlassian where last year I saved the
$3 million through database optimization.
I'm now applying that same mindset to AI. So I built VoiceTree, an algorithm which optimizes the inputs to LLMs by representing it as a tree.
nodes. This decreases token usage and increases accuracy. Great, it's amazing, but the real breakthrough has been that I now, using this technology myself, no longer use linear chat. All my interactions with AI are through this tree itself, where it acts as sort of like a shared memory. So I'll show you what I mean in the demo, but it's real, it's working, it's very exciting, and now I'd like your help.
For me to get this into the hand developers everywhere as quickly as possible. Let's do it
Okay, next.
They won't? Okay. Martin, what do you think?
You can hear it. She's moving
All right, so current large language models have a performance problem at long input.
And part of the reason for that is because they mimic memory by reprocessing the entire trance
for every request.
And on top of that, attention to items and memory scales quadratically because it's an all-to-all comparison.
So, Voice Tree solves this by optimizing the
We progressively convert raw text into a graph containing the concepts and key relationships.
We then work with the LLM to identify what nodes are relevant and return just that pruned context back to the LLM.
So, we're seeing very promising results on the GSM.
where we're seeing up to 70% fewer input.
and behaviour where it models
couldn't answer certain hard questions now are able
instead of just wasting output token
on the wrong answer.
So I'm running VoiceTree right now completely live. So it's converting my voice to text to then do tree actions such as that one there. But why am I doing that? Well, one of the key realizations we had was that...
the same context iteration that lms have humans have as well and voicetree can provide those same performance
to humans. But then we don't stop there. What we realize is, well, then we can do away with linear interface completely, and instead interact with AI purely through this tree itself.
And then our memory is
shared. I'll show you what I mean by that.
I'll launch an agent here.
And you can see that it gets only the relevant context from the tree.
and inject it into it.
And then if I get to do something like draw a diagram explaining what this would look like
it's going to save its output back to the tree.
like it just did here.
So, this is incredibly useful for software engineering. And I'll show you some real examples of me using it to build features for VoiceTree using VoiceTree itself. Yeah. I'm going to actually land clean. Hey, the parents just finished watching it. That was it? Yeah. So, there's not the real world use case there yet. How are you going to add that in because it's only a three month? Yeah. Well, I'm thinking I can cut this one down to two minutes. There's a bit of fat that I can trim. Um, this one I was trying to do without a script. Like, I was just using bullet points. But I realized that for the demo, it's actually better if it's perfectly scripted.
Yeah.
It's a good product. Yeah.
i think we can just own yeah yeah i can do that um a bit more polished okay yeah that's true um and what do you think about um so you don't want me to use
the
the the pencil to rub it out or you want me to do it better just give me I think
Okay, and do you think, um...
Um...
Should I get, like, mom?
control my computer so I can focus just on speaking.
it. Yeah. Can't you just speak afterwards? Record it after. That's what I would do.
Um...
Yeah, I could do that as well, but then getting like it's just an extra step of work that I need to Record on top of it and make sure it times up well And then it's not actually live which is
They might be able to figure it out. I don't know how, but little small clues, right? Yeah, yeah. Yeah, I realize that's bad because it looks like I'm not typing. So should I film from the side so you can see my side profile of me typing? Or should I just...
Yeah.
Yeah, you think from the side with my iPhone camera. Yeah
Yeah, the iPhone better cameras.
That's when I filmed the founder introduction.
Okay, nice.
So, today's large language models become worse as context length increases because to mimic memory, they reprocess the whole chat history for every request.
but full all-to-all attention.
So today's large language models become worse as context length increases because to mimic memory, they reprocess the entire chat history for every request.
but full all-to-all attention scales quadratically. So VoiceTree's solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Our initial tests on complex long context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70 percent and more importantly models are solving questions they just couldn't answer
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from context degradation.
So the really exciting breakthrough we've made is why not just remove this chat interface?
Completely.
Instead
Interact directly with the AI through a shared memory.
I'll show you what I mean.
What you're seeing is the voice tree algorithm running live on my voice, building this tree.
I'll now launch an agent.
And the agent is going to get injected into its context, only the relevant context.
the graph.
If I then get it to do a task, like draw me a diagram.
It's going to add it back to the tree.
Voila!
So today's large language models become worse as context.
Because to mimic memory, they reprocess the whole chat history for every request.
but full all-to-all attention.
So today's large language models become worse as context-
because Dominic memory, they reprocessed the entire chat history frame.
but full ultra-alt attention scales quadratically. So Voice Tree's solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model. Our initial tests on complex, long-context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions that is...
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from chronic degradation.
So the really exciting breakthrough we've made is, why not just remove this chat interface completely? And instead, interact directly with the AI through a shared memory. I'll show you what I mean. So what you're seeing is the voice tree algorithm running live on my voice, building this tree in real time. I'll now launch an agent.
And the agent is going to get injected into its context, only the relevant content from
If I then get to do a task, like draw me a diagram.
It's going to add it back to the tree.
Voil.
So today's large language models become worse as context length increases because to mimic memory they reprocess the entire chat history for every
But full, all to all attention.
So voice-tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to...
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions that just remove this chat and instead interact directly with the AI through a shared memory. I'll show you what I mean. So, what you're seeing is the VoiceTree algorithm running live on my voice, building this tree in real time. I'll now launch an agent.
And the agent is going to get injected into its context, only the relevant content from
If I then get to do a task, like draw me a diagram.
It's going to add it back to the tree.
Voila.
So today's large language models become worse as context.
because to mimic memory, they reprocess the entire chat history.
but full ultra-alt attention scales quadratically. So voice-tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to mod-
Our initial tests on complex, long context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly before.
Moreover, these benefits aren't just for AI, they're for us too.
We humans also suffer from...
So the really exciting breakthrough we made is.
So today's large language.
So today's large language models become worse as context limiting.
Because to mimic memory, they reprocess the whole chat history.
But full ultra-ol tension scales quadratically.
VoiceTree's solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model. Our initial tests on complex, long-context reasoning
So today's large language models become worse as they're...
because to mimic memory, they reprocess the entire chat history.
but full all-to-all attention scales quadruple.
Voice tree solution is to progressively convert the conversation into a tree and then only then send the relevant
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%. And more importantly, models are solving questions
Now, these benefits aren't just for the age.
They're for us to we humans also suffer from.
So then the really exciting breakthrough
Why not?
Just remove this chat history entirely.
and instead interact directly with the AI through a shared memory.
I'll show you what I mean.
So what you're seeing up top is the voice tree algorithm running live on my voice, building this tree in real time.
I'll now launch an
and
As I launch the agent, it's going to get injected into it, only the relevant context from the tree.
Then if I get it to do a task
its output is going to be saved back to the tree.
Hey Siri, where are you?
So today's Largelink
So today's large language models become worse as...
increases because to mimic memory, they reprocess the whole chat.
for every single request.
but full all-to-all attention scales quadratically. So voice tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from...
So Ben, they're really exciting.
Why don't we just remove this chat history entirely?
and interact with the A.I.
through directly with this shared memory.
I'll show you what I mean.
So right now, what you're seeing is VoiceTree, the VoiceTree algorithm running live on my voice, building this tree in real time.
I can now spawn an agent.
and it will get injected into it.
only the relevant parts of the text.
from the graph
Now when I get it to do something for example explain this with a diagram its output is also going to be saved back to the
Amazing.
So today's large language models become worse as context link increases because to mimic memory, they reprocess the whole chat history for every record.
But full all-to-all attention scales quadratically.
VoiceTree's Solution
is to progressively convert the conversation into a tree and then only send the relevant branches
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions
Moreover, these benefits aren't just for the
We humans, too, also suffer from...
So then the really exciting breakthrough.
Why not remove this chat interface entirely?
and interact directly
with the AI through a shared memory.
I'll show you what I mean.
Today's large language...
become worse as context length increases. Because to mimic memory, they reprocess the whole chat history.
But full all-to-all attention scales quadratically.
VoiceTree's solution is to progressively convert the conversation
to a tree, and then only send the relevant branches.
Our initial tests
On long context, reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%. And more importantly, models are solving questions
couldn't answer directly before. Moreover, these benefits aren't just for the
They're for us, too. We humans also suffer from context degradation. So then the really exciting breakthrough is, why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
So I can spawn an agent.
on any one of these nodes
Once I spawn the agent
It gets only the relevant content.
The graph.
If I get it to do something.
It's going to now also
add back its context.
back into the graph.
Today's large language models become worse.
increases because to mimic memory, Ray reprocessed the whole chat history.
But full all-to-all attention
VoiceTree's solution is to progressively convert the conversation into a tree.
And then only send the relevant branches to the model.
Our initial tests on complex long context reasoning
We're seeing input token reductions of up to 70%, and more importantly, models are solving problems they just couldn't answer correctly before.
Now, these benefits aren't just for the AI, they're for us too. We humans also suffer.
So then the really exciting breakthrough.
Why not just remove this chat interface entirely and interact with the AI directly through a shared memory?
I'll show you what I mean.
So right now,
What you're seeing is the VoiceTree algorithm running live online.
building this tree in real time.
If I now launch an agent.
It gets injected into it only the relevant content.
If I get it to do a task,
For example to explain this with a diagram
It's gonna save it back to the graph
Today's large language models become worse as context length increases because to mimic memory, they reprocess the whole chat history for every...
But fool, out of-
today's large language models become worse as context length increases because dominic memory they reprocess the whole chat history for every
but full of attention scales.
Wastry solution is to progressively convert the conversation into
and then only send the relevant branches to the model. Our initial tests on long context reasoning benchmarks are incredible.
We're seeing input token reductions up to 70%. And more importantly, models are solving questions.
Moreover, these benefits aren't just for the AI, they're for us too. We humans also suffer from...
So then the really exciting breakthrough is why not remove this chat interface entirely and instead interact directly with the AI through a shared memory. I'll show you what I mean.
So I can spawn an agent on any one of these nodes.
Once I spawn the agent, it gets only the relevant context from the graph.
If I get to do something, it's going to now also add back its context.
Back into the graph.
Today's large language models become worse as contact length increases because to mimic memory they reprocess the whole chat history for
but full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation
into a tree, and then only send the relevant
Our initial tests on complex long context reasoning benchmarks.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly before. Moreover, these benefits aren't just for the AI, they're for us too. We humans also...
So then the really exciting thing
why not just remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
What you're seeing is the voice tree algorithm running live on my voice, building this tree in real time.
I'll now launch an ad.
As I launched the agent,
it will get injected into it only the relevant content.
from the existing tree.
If I get it to do a task,
It's going to add back its result to the tree.
Today's large language...
become worse as contact length increases because to mimic memory, they reprocess the whole chat history for every request.
But full, all-to-all attention.
VoiceTree's solution to this is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Our initial tests on complex, long context reasoning
are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Now, these benefits aren't just for the AI, they're for us too. We humans also...
So then the really exciting breakthrough.
Why not just remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
So right now, VoiceTree has been running live on my voice.
building this tree in real time.
I'll now launch an agent.
As I launch it, it's going to get injected into itself, only the relevant content.
Then if I get it to do a task like draw a diagram
It's going to add that back into the tree.
saving its progress.
Today's large language models become worse as context.
Because Dominic memory, they reprocess the whole chat history for every time.
Full ultra-attention scale is quadratic.
VoiceTree's solution to this is to progressively convert the conversation into a tree and then only send the relevant branches to them.
Our initial tests on complex, long context-reasoning benchmarks are incredibly promising. We're seeing input token reductions of about 70%, and more importantly, models are solving questions they just couldn't answer correctly before. Now, these benefits aren't just for the AI, they're for us too. We humans also suffer from...
So then the really exciting breakthrough is why not just remove this chat interface entirely and instead interact directly with the AI through a shared memory. I'll show you what I mean. So right now, VoiceTree has been running live on my voice.
building the street in real time. I'll now launch an agent.
As I launch it, it's going to get injected into itself, only the relevant context.
Then, if I get to do a task, like draw a diagram, it's going to add that back into the tree.
Saving its progress.
Today's large language model.
You
...become worse as contact length increases. Because Dominic Memory, they reprocess the whole chat history for every...
Before all attention scales.
VoiceTree's solution to this is to progressively convert the conversation into a tree and then only send the relevant branches to the model.
Today's large language model
I'm worse.
length increases. Because to mimic memory they reprocess the whole chat history.
for every request but full all-to-all attention scales quadratically.
VoiceTree's solution is to progressively convert the conversation
into a tree and then only send the relevant branches.
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
We're seeing input token reductions.
of up to 70% and more importantly models are solving questions.
Now, these benefits aren't just for the AI. They're for us, too. Humans, too, suffer from...
So then, the really exciting breakthrough.
Why not just remove this chat interface entirely?
and interact directly with the AI through a shared memory. I'll show you what I mean.
What you're seeing is the voice tree algorithm running. Oh, I didn't do very well
I'll see you next time.
Today's large language
become worse as context length increases.
Today's large language models become worse as context length increases because to mimic memory, they reprocess the whole chat history
But full all-to-all attentions
VoiceTree's solution is to progressively convert the conversation into a
and then only send the relevant branch.
Our initial tests on complex long context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%.
And more importantly, models are sold.
but they just couldn't answer correctly.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from...
So then the really exciting.
why not just remove this chat interface entirely and Interact directly with the AI through a shared memory
I'll show you what I mean.
So
What you're seeing is the voice tree algorithm
running live on my voice.
building this tree in real time.
But it's a bit too far away, so yes.
Today's large language models become worse as context length increases. Because to mimic memory, they reprocess the whole chat history for every request.
But full, all-to-all attention scales quadruple.
VoiceTree's solution is to progressively convert the conversation into a tree.
and then only send the relevant brand.
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Now, these benefits aren't just for the agent.
They're for us too. Humans as well suffer from...
So then the real exciting
why not just remove this chat interface entirely?
and interact directly with the AI through a shared memory instead.
I'll show you what I mean.
Are you fucking kidding me?
Today's large language models become worse as context length increases because to mimic memory they reprocess the whole chat history.
But full all-to-all tension scales quadratically.
Voice Tree's solution is to progressively convert the conversation into a tree.
and then only send the relevant brand.
Our initial tests
Today's large language models become worse as...
Because to mimic memory, they reprocess the whole chat history.
and full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation.
tree, and then only send the relevant branches.
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from context degradation.
So then the really exciting.
why not just remove this chat interface in entirely, and instead, interface directly.
through a shared memory.
I'll show you what I mean.
What you're seeing is the voice tree algorithm running live on my voice
building this tree in real time.
I'll now launch an agent.
it's going to get injected into itself, only the relevant content.
If I get it to do a task.
it's going to add back its output back into the tree.
Today's large language models become worse as contact
Because mimic memory, they reprocess the whole chat history for everyone.
and full all-to-all attention scale.
Voice 3 solution is to progressively convert the conversation
and then only send the relevant branches to the
Our initial tests on complex long context reasoning
We're seeing input token reductions up to 70%, and more importantly, models are solving questions.
Now, these benefits aren't just for AI, they're for us too. Humans as well suffer from content.
So then the really exciting breakthrough is.
why not just remove this chat interface in entirely and instead interface directly through a shared memory i'll show you what i mean
What you're seeing is the voice tree algorithm running live on my voice, building this tree in real time.
I'll now launch an agent.
It's going to get injected into itself, only the relevant context.
If I get it to do a task.
as context length increases because to mimic memory they reprocess the whole chat history
and full all-to-all attentions.
Voice tree solution is to progressively convert the conversation into a tree and then only send the relevant
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising.
We're seeing input token reductions of up to 70%. And more importantly, models are solving questions they just couldn't answer.
Now these benefits aren't just for the AI, they're for us too. Humans as well suffer from
So then the really exciting breakthrough is
Why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean.
What you're seeing is the VoiceTree algorithm running live in my voice.
building this tree in real time.
I'll now launch an agent.
No context warming, warm-up, or re-prompting.
It's all
already injected into the tree.
Let's get it to draw us an architecture diagram.
its progress is being saved back to the tree.
Today's large language models become worse as context.
Because to mimic memory, they reprocess the whole chat history for everyone.
and full all-to-all attention scale.
Voice tree solution is to progressively convert the conversation into a tree and then only send the relevant branches to model
Our initial tests on complex, long-context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70.
And more importantly, models are solving questions they just couldn't answer correctly.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from...
So then the really exciting breakthrough is why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean what you're seeing is the voice tree algorithm running live in my voice
building this tree in real time.
I'll now launch an agent.
No context warming, warm up, or re-prompting.
It's all already injected into the tree.
Let's get it to draw us an architecture diagram.
Its progress is being saved back to the tree.
back into the tree
Today's large language models become worse at this con.
Because to mimic memory, they reprocess the whole chat history for everyone.
and full all-to-all attention scale.
Voice tree solution is to progressively convert the conversation into a tree and then only send the relevant branch
on complex, long context reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%
And more importantly, models are solving questions they just couldn't answer correctly.
now these benefits aren't just for the ai they're for us too humans as well suffer from
So then the really exciting breakthrough is, why not remove this chat interface entirely and instead interact directly with the AI through a shared memory?
I'll show you what I mean. What you're seeing is the voice tree algorithm running live in my voice.
building this tree in real time. I'll now launch an agent.
No context warming warm-up or re-prompting.
it's all already injected into the tree.
Let's get it to draw us an architect.
Its progress is being saved back to the tree.
memory they reprocessed the whole chat history.
But full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation into a tree, and then only send the relevant branches to the model.
Our initial tests on complex, long context reasoning
are incredibly promising.
We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly.
Now, these benefits aren't just for the AI, they're for us too. Humans as well suffer from...
So then the really exciting breakthrough is
why not just remove this chat interface entirely and instead interact directly
with the AI through a shared memory.
I'll show you what I mean.
So, right now...
Voice tree algorithm running live on my voice, building this tree in real time.
I'm worse as.
increases because to mimic memory they reprocess the whole chat history
But full all-to-all attention.
VoiceTree's solution is to progressively convert the conversation in your voice.
and then only send the relevant branch.
Our initial tests on complex, long context reasoning benchmarks are incredibly promising.
we're seeing input token reductions of up to 70%. And more importantly, models are solving
now these benefits aren't just for the ai they're for us too humans as well suffer from
So then the really exciting breakthrough is
Why not just remove this chat interface entirely and interact directly with the AI?
Through a shared memory
I'll show you what I mean.
What you're seeing here is the voice tree algorithm running live on my
Building this tree in real
I'll now launch an agent.
So no context warmup or reprompting needed, it's already injected into the
and all progress is being saved back to the tree as well.
Today's large language models become worse as context.
because the mimic memory they reprocess
but full all-to-all attention scale
Voice tree solution is aggressively convert the conversation.
and then only send the relevant branch.
Our initial tests on complex, long-context-reasoning benchmarks are incredibly promising. We're seeing input token reductions of up to 70%, and more importantly, models are solving questions they just couldn't answer correctly.
Now, these benefits aren't just for the AI. They're for us too. Humans as well suffer from contact.
So then the really exciting breakthrough is, why not just remove this chat interface entirely and interact directly with the AI through a shared memory? I'll show you what I mean.
What you're seeing here is the VoiceTree algorithm running live on my voice.
building this tree in real time.
I'll now launch an agent.
Oh, so no context, it's already indicated the tree and all progress is being saved back to the tree.
Okay.
Thank you.
You

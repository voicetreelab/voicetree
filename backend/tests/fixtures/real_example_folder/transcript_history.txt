hHmm, alright, cool. So, today I'm working on something, <end> where <end> Okay, so, the thing that I'm working on is, um, okay, let's see what it says. Is it resumability? Read history markdown vectors. Um, so this is when you open voice tree, um, <end> it is <end> So, I think this is <end> So, one thing I want to do is, um, first of all, extract this into a new file. Um, <end> Okay, so, we have load directory.py, um, and we want that to do three things. <end> Um, <end> So, first thing is, um, <end> uh, load markdown files. <end> Um, two, we need to load the vector files. <end> Uh, the ChromaDB database. <end> Um, and then three, we need to load the, uh, transcript history. <end> Um, <end> And then, <end> four, <end> um, so, to do, to do task three. <end> Um, <end> we need a save the transcript history. <end> To the markdown file, uh, markdown, to the folder. <end> To the same folder with, <end> files. <end> Um, <end> cool. <end> So, step one, let's do the extraction. So, we want to extract, um, we want to extract what's currently in server.py. Um, for loading existing markdown. Um, into load dur.py. <end> Okay, um, with loading vector files, I want to know, I actually don't know the current state of, um, the code. I don't know if we currently load, um, vectors. It's not in the server.py. Um, but I think it still might be at the vector level. Um, so let's, uh, let's look if it's in some of the vector code. Um, and see what the current state is if we actually load it or not. <end> Um, cool. <end> Okay, so with saving transcript history, um, let's, identify the code that does that. Um, and make it save transcript history, um, to the, uh, whatever the markdown vault folder is. Um, and to make sure to use the same, uh, like, environment variable or whatever that defines it. Like, don't duplicate that. Um, and save it there instead of where it's, wherever it's currently saved. <end> Okay, and then for loading transcript history, the second part of loading transcript history is going to be able to be, um, making sure that our code can, uh, as input, take the history state. Um, so I think currently, um, in, uh, chunk processor, um, at some point it has, like, a, a buffer manager. Um, or a, it's even tree action cider workflow or buffer manager. Um, <end> Yeah, so it's actually in tree action decider, w uh, workflow. <end> Um, <end> So, we'll probably want to, instead, have that in, um, <end> I'm thinking now chunk processor.py, possibly, is an unnecessary layer of indirection. <end> Um, <end> So, let's just see what, um, an LLM thinks about whatever chunk processor and tree action decider workflow, um, if they're both sort of unnecessary. And we should just merge everything into, um, uh, tree action decider workflow. <end> Um, but that might not actually be the most important thing. Um, okay, so probably more important is to just, um, inject, uh, be able to start the chunk processor. Um, with an existing, um, <end> history. <end> Uh, which is a, an existing, um, <end> history manager. <end> Um, <end> Okay, so let's do, <end> Ah, so with, um, evaluating the vector loading investigation, um, <end> I'm thinking, wait, so investigate vector loading status. Um, I'm thinking that we actually don't need the load underscore dur.py. But instead, this should be in, um, <end> probably in chunk processor.py. <end> Um, <end> and <end> that, when it creates a, um, <end> Yeah, so chunk processor, uh, shouldn't b <end> Hmm. <end> So I guess the question is, do we make chunk processor, uh, load the markdown? <end> Files. <end> Or, do we make, um, server.py load them? <end> So I'm just seeing a problem right now with load existing tree from markdown. In load underscore dur.py, which is that it doesn't actually return anything. <end> Um, <end> so I'm a little bit confused. <end> How it works. <end> Or if it makes the decision tree. <end> Yeah, okay. <end> Yeah, I think, okay, so I think what we should do is, we shouldn't have, load underscore dur.py. Um, and when we make, the decision tree, um, so, line 39 in server.py, um, whatever the output dur is, it should just load, um, <end> Whatever is in that directory. <end> By default. <end> Yeah, okay, so I'm just reviewing Zoe's work, and, um, I think she's overcomplicated it because we don't need all these files to be changed. Um, we don't need chunk processor.py and tree action decider workflow and history manager. Um, we should, ideally, just have it saved to history manager. Um, and then, on, in it, of history manager, um, in initiation, I, and IT, <end> uh, so history manager in it, in it, um, we should, uh, call, um, load subdirectory. <end> Um, if, uh, load, load file, uh, if it's been given a output directory in the constructor. <end> Okay, now I want to do an architecture review where we made a decision, um, so I should add, probably to my decision log, um, <end> So, what we're saying is, the decision is we're not going to have a centralized auto load module. Um, which loads everything we need. Instead, um, each module itself, each module itself such as the, um, markdown tree module, um, vector module, history module, um, will each load themselves. Um, their own state. <end> Um, so centralized versus decentralized, um, state management. Um, and I'm not sure if this is a good decision or not, so let's ask AI. <end> Um, and then the next thing we want to do is make an overall integration test. Um, to check that if we, um, start up with a given folder, that has a, uh, ChromaDB. Um, it has a, uh, transcript history. Um, and it has a, uh, <end> ChromaDB, transcript history, and markdown files. Um, that it can load all three correctly. Um, and here I'm seeing one, um, con to having it decentralized, which is it makes it a little bit harder to test. Um, but it shouldn't be too hard. Um, <end> We just check that, uh, each thing that we want, each module that we care about has the state that it should have. Um, but maybe that's actually really hard about dependency injection. Um, <end> I'm not really sure what, what is optimal here. <end> Um, to be honest. <end> Um, yeah, I wonder what Bob's opinion is on, um, how we could then test this. Um, so, yeah, how, uh, what's the best way to do it in, like, an integration test of all this? Um, and does the decentralized module loading, um, make it significantly harder, or it's actually not too bad? <end> Okay, I'll, I'll get, um, <end> I'll get, uh, Bob in a minute to, um, propose an integration test, but I think before we do that, we need to do a git diff. <end> Step one, git diff. Um, and do a quick review. Um, and then we also need to, um, <end> Uh, then run it manually, uh, and see, uh, if the files or the transcript history files at least created. <end> Uh, <end> Um, we're making a decision that, um, history manager saves, uh, <end> uh, only process text. So, completed text. Um, when maybe history should just save, um, all text. <end> Um, oh, but we had a bug at some point, right, where it's like, if the history has text that wasn't processed, um, <end> Yeah, I think we could divorce it because it doesn't actually need it, but, um, is it the right thing to do? <end> Uh, <end> Okay, let's just save it to comment. Um, <end> Okay, now we just need to tell the agent. So, okay, that it needs to actually, um, save this, text. <end> Uh, <end> So, <end> Okay, so it actually, I realized, um, that identifying and modifying code for transcript saving, um, Zoe overcomplicated it. It should just, um, on, uh, append, just dump to the, uh, text file, whatever it is appended. Um, and, uh, <end> on init, initiation, it should just load whatever is in the file. Um, <end> and so, that means we're automatically, uh, writing every time, uh, append gets called. Um, and automatically reading on startup. <end>
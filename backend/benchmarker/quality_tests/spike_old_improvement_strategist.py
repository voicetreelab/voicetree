import os
import sys
import json
import logging
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add parent directories to path to import project modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../../..')))
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../..')))

import google.generativeai as genai
from google.generativeai import GenerativeModel
from backend import settings
import PackageProjectForLLM

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Configure Gemini API
genai.configure(api_key=settings.GOOGLE_API_KEY)

# Constants
CONTEXT_FILE = "latest_run_context.json"

def get_strategist_prompt(transcript_content, packaged_output, quality_report, trace_log):
    """Constructs the prompt for the improvement strategist LLM."""
    
    prompt = f"""
You are an expert at debugging and improving multi-agent LLM systems.
Your name is "The Strategist".
I have a "voice-to-tree" system that has underperformed on a task.
I will provide you with all the necessary context:
1. The original transcript that was processed.
2. The final (flawed) markdown tree output generated by the system.
3. A "Quality Report" written by another AI expert that evaluates the flawed output.
4. A detailed "Trace Log" of the internal LLM calls made by the system's agentic workflow to produce the output.

Your goal is to perform a root cause analysis and provide a concrete, actionable recommendation for improvement.

Your analysis should:
1.  Start by summarizing the primary weakness identified in the Quality Report (e.g., "Poor Coherence," "Node Fragmentation").
2.  Scour the Trace Log to find the specific agent step(s) and LLM call(s) that most likely caused this weakness.
3.  Explain the connection clearly. For example: "The Quality Report noted illogical parent-child links. The Trace Log shows that the 'node_linker' agent, using the 'link_nodes.txt' prompt, incorrectly linked Node A to Node B. The prompt for this agent seems to be missing instructions on how to handle cases where two topics are discussed sequentially but are not causally related, which was the situation for nodes A and B."
4.  Provide a specific, actionable recommendation. Do not just say "improve the prompt." Instead, suggest a concrete change. For example: "Recommendation: Modify the `prompts/link_nodes.txt` prompt. Add the following instruction: 'If two nodes are not causally related, prioritize creating them as siblings under the same parent rather than forcing a direct parent-child relationship.'"

Here is the information for your analysis:

--- START: Original Transcript ---
{transcript_content}
--- END: Original Transcript ---

--- START: Flawed Markdown Output ---
{packaged_output}
--- END: Flawed Markdown Output ---

--- START: Quality Report ---
{quality_report}
--- END: Quality Report ---

--- START: Trace Log ---
{trace_log}
--- END: Trace Log ---

Now, please provide your root cause analysis and recommendation.
"""
    return prompt

def main():
    """Main function to run the improvement strategist."""
    logging.info("Starting Improvement Strategist...")

    # 1. Load the context from the last benchmark run
    if not os.path.exists(CONTEXT_FILE):
        logging.error(f"Context file not found: {CONTEXT_FILE}. Please run quality_LLM_benchmarker.py first.")
        return

    logging.info(f"Loading run context from {CONTEXT_FILE}")
    with open(CONTEXT_FILE, 'r') as f:
        context = json.load(f)

    # 2. Read all the diagnostic files
    try:
        logging.info(f"Reading transcript file: {context['transcript_file']}")
        with open(context['transcript_file'], 'r') as f:
            transcript_content = f.read()

        logging.info(f"Reading quality log: {context['quality_log_file']}")
        with open(context['quality_log_file'], 'r') as f:
            quality_report = f.read()

        logging.info(f"Reading workflow trace log: {context['workflow_io_log']}")
        with open(context['workflow_io_log'], 'r') as f:
            trace_log = f.read()
            if not trace_log.strip():
                logging.warning("The trace log is empty. The strategist may not have enough information.")

        logging.info(f"Packaging output from directory: {context['output_dir']}")
        packaged_output = PackageProjectForLLM.package_project(context['output_dir'], ".md")
        if not packaged_output.strip():
            logging.warning("The packaged output is empty. This likely indicates a total failure in generation.")
            packaged_output = "The directory was empty. The system failed to produce any output."

    except FileNotFoundError as e:
        logging.error(f"Error reading file: {e}. Make sure all paths in {CONTEXT_FILE} are correct.")
        return
    except Exception as e:
        logging.error(f"An unexpected error occurred while reading files: {e}")
        return

    # 3. Construct the prompt
    prompt = get_strategist_prompt(transcript_content, packaged_output, quality_report, trace_log)
    logging.info("Strategist prompt constructed. Sending to LLM for analysis...")

    # 4. Call the LLM for analysis
    try:
        model = GenerativeModel('models/gemini-1.5-pro-latest')
        response = model.generate_content(prompt)
        recommendation = response.text.strip()

        # 5. Print the recommendation
        print("\n" + "="*80)
        print("ðŸ’¡ Improvement Strategist Recommendation ðŸ’¡")
        print("="*80 + "\n")
        print(recommendation)
        print("\n" + "="*80)

    except Exception as e:
        logging.error(f"An error occurred while communicating with the LLM: {e}")

if __name__ == "__main__":
    main() 
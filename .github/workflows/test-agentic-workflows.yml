name: Test Agentic Workflows

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - '*.py'
      - '.github/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - '*.py'
      - '.github/**'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode to run'
        required: false
        default: 'ci'
        type: choice
        options:
        - ci
        - local
      include_benchmarks:
        description: 'Run performance benchmarks'
        required: false
        default: false
        type: boolean

jobs:
  # Job 1: Fast Tests (Unit + Integration without API)
  fast-tests:
    runs-on: ubuntu-latest
    name: "Fast Tests (Unit + Integration)"
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-xdist

    - name: Run unit tests
      run: |
        cd backend
        echo "ðŸ§ª Running unit tests (no API calls)..."
        python -m pytest tests/unit_tests/ \
          --disable-warnings \
          -v \
          --tb=short \
          --maxfail=5 \
          --timeout=60 \
          -n auto
    
    - name: Run integration tests (without API)
      run: |
        cd backend
        echo "ðŸ”— Running integration tests (mocked/offline)..."
        python -m pytest tests/integration_tests/ \
          -k "not api and not requires_api" \
          -v \
          --tb=short \
          --disable-warnings \
          --timeout=60 \
          --maxfail=3 \
          -n auto
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: fast-test-results
        path: |
          backend/tests/unit_tests/test-results/
          backend/tests/integration_tests/test-results/

  # Job 2: API Integration Tests
  api-tests:
    runs-on: ubuntu-latest
    name: "API Integration Tests"
    needs: fast-tests
    if: |
      github.ref == 'refs/heads/main' || 
      github.ref == 'refs/heads/develop' || 
      github.event_name == 'workflow_dispatch'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-xdist

    - name: Check API connectivity
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        if [[ -z "$GOOGLE_API_KEY" ]]; then
          echo "âš ï¸ GOOGLE_API_KEY not set - skipping API tests"
          echo "skip_api_tests=true" >> $GITHUB_ENV
          exit 0
        fi
        
        echo "ðŸ§ª Testing Gemini API connectivity..."
        cd backend
        python -c "
        import os
        import sys
        try:
            from agentic_workflows.infrastructure.llm_integration import GEMINI_AVAILABLE, call_llm
            print(f'Gemini available: {GEMINI_AVAILABLE}')
            
            if GEMINI_AVAILABLE:
                result = call_llm('Say hello', 'gemini-2.0-flash')
                print(f'âœ… API test successful: {len(result)} chars')
            else:
                print('âŒ API initialization failed')
                sys.exit(1)
        except Exception as e:
            print(f'âŒ API test failed: {e}')
            sys.exit(1)
        "

    - name: Run API integration tests
      if: env.skip_api_tests != 'true'
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        PYTEST_TEST_MODE: ${{ github.event.inputs.test_mode || 'ci' }}
      run: |
        cd backend
        echo "ðŸš¨ Running API integration tests..."
        
        # Core pipeline integration with real APIs
        python -m pytest pipeline_system_tests/test_full_system_integration.py \
          -v \
          --tb=short \
          --disable-warnings \
          --timeout=120 \
          --maxfail=3
        
        # Agentic workflow API tests
        python -m pytest tests/integration_tests/agentic_workflows/ \
          --test-mode=${{ github.event.inputs.test_mode || 'ci' }} \
          -v \
          --tb=short \
          --disable-warnings \
          --timeout=180 \
          --maxfail=3
      timeout-minutes: 15

    - name: Upload API test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: api-test-results
        path: |
          backend/pipeline_system_tests/test-results/
          backend/tests/integration_tests/agentic_workflows/test-results/

  # Job 3: Quality & Performance Benchmarks
  benchmarks:
    runs-on: ubuntu-latest
    name: "Quality & Performance Benchmarks"
    needs: api-tests
    if: |
      (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.include_benchmarks == 'true')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio

    - name: Run quality benchmarks
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        echo "ðŸ“Š Running quality benchmarks..."
        python backend/benchmarker/quality_tests/quality_LLM_benchmarker.py
      timeout-minutes: 10
    
    - name: Run performance benchmarks
      if: github.event_name == 'workflow_dispatch' && github.event.inputs.include_benchmarks == 'true'
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        PYTEST_TEST_MODE: ${{ github.event.inputs.test_mode || 'ci' }}
      run: |
        echo "âš¡ Running performance benchmarks..."
        cd backend/tests/integration_tests/agentic_workflows
        python -m pytest test_performance_benchmark.py \
          --test-mode=${{ github.event.inputs.test_mode || 'ci' }} \
          -v \
          --tb=short
      timeout-minutes: 10

    - name: Generate test documentation
      run: |
        cat > TESTING_GUIDE.md << 'EOF'
        # ðŸ§ª VoiceTree Testing Guide
        
        ## Test Architecture Summary
        
        VoiceTree uses a 3-tier testing approach:
        
        ### 1. Fast Tests (< 60s)
        - **Unit Tests**: Isolated component testing
        - **Integration Tests**: Cross-module testing without API calls
        - **Purpose**: Rapid development feedback
        
        ### 2. API Integration Tests (< 15min)
        - **Real API Integration**: Tests with Gemini API
        - **System Integration**: Full pipeline testing
        - **Purpose**: Validate real-world functionality
        
        ### 3. Quality & Performance Benchmarks (< 20min)
        - **Quality Scoring**: LLM output quality assessment
        - **Performance Metrics**: Speed and efficiency testing
        - **Purpose**: Ensure production readiness
        
        ## Local Development Commands
        
        ```bash
        # Fast development testing
        python dev-test.py --speed smoke     # < 10s
        python dev-test.py --speed fast      # < 30s
        python dev-test.py --changed         # Test only changed files
        
        # Comprehensive testing
        make test-unit                       # < 45s
        make test-local                      # Limited API calls
        make test-ci                         # Full API testing
        
        # Watch mode for continuous testing
        python dev-test.py --watch --speed smoke
        ```
        
        ## CI/CD Philosophy
        
        - **Fail Fast**: Unit tests run first for quick feedback
        - **API Gating**: Real API tests only on main/develop branches
        - **Cost Control**: Limited API calls in local mode
        - **Parallel Execution**: Maximum efficiency with job parallelization
        
        ## Audio Testing
        
        Audio processing tests are excluded from CI/CD due to:
        - Heavy dependencies (Whisper models)
        - Environmental instability in containers
        - Resource requirements
        
        Run audio tests locally:
        ```bash
        python process_transcription.py --audio path/to/audio.m4a
        ```
        EOF
        
        echo "ðŸ“ Testing documentation generated"
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          backend/benchmarker/quality_log.txt
          backend/benchmarker/latest_quality_log.txt
          backend/benchmarker/quality_tests/latest_run_context.json
          oldVaults/VoiceTreePOC/QualityTest/
          TESTING_GUIDE.md
    
    - name: Comment results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: 'âœ… VoiceTree testing completed!\n\n**Fast Tests**: Unit + Integration (no API)\n**API Tests**: Real API integration\n**Benchmarks**: Quality & performance metrics\n\nCheck the "Actions" tab for detailed results and artifacts.'
          }) 